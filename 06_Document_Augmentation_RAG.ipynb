{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Document Augmentation RAG with Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로.\n",
    "\n",
    "    Returns:\n",
    "        str: 추출된 전체 텍스트.\n",
    "    \"\"\"\n",
    "    # PDF 파일을 엽니다.\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 텍스트를 저장할 문자열 초기화\n",
    "\n",
    "    # 각 페이지를 순회하면서 텍스트를 추출합니다.\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 해당 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출된 텍스트를 누적\n",
    "\n",
    "    return all_text  # 전체 텍스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로, 지정된 수의 문자가 겹치도록 분할합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 분할할 원본 텍스트.\n",
    "        n (int): 각 청크의 문자 수.\n",
    "        overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 분할된 텍스트 청크 리스트.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    # (n - overlap)만큼 이동하며 텍스트를 분할\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])  # 현재 위치부터 n자까지 슬라이싱하여 추가\n",
    "\n",
    "    return chunks  # 청크 리스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Questions for Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(text_chunk, num_questions=5, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 청크로부터 관련 질문들을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        text_chunk (str): 질문을 생성할 대상 텍스트 청크.\n",
    "        num_questions (int): 생성할 질문의 개수.\n",
    "        model (str): 사용할 언어 모델.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 생성된 질문 리스트.\n",
    "    \"\"\"\n",
    "    # AI의 역할을 정의하는 시스템 프롬프트\n",
    "    system_prompt = (\n",
    "        \"당신은 텍스트로부터 관련 질문을 생성하는 전문가입니다. \"\n",
    "        \"제공된 텍스트를 바탕으로 그 내용에만 근거한 간결한 질문들을 생성하세요. \"\n",
    "        \"핵심 정보와 개념에 초점을 맞추세요.\"\n",
    "    )\n",
    "    \n",
    "    # 사용자 프롬프트: 텍스트와 함께 질문 생성 요청\n",
    "    user_prompt = f\"\"\"\n",
    "    다음 텍스트를 기반으로, 해당 텍스트만으로 답할 수 있는 서로 다른 질문 {num_questions}개를 생성하세요:\n",
    "\n",
    "    {text_chunk}\n",
    "    \n",
    "    응답은 번호가 매겨진 질문 리스트 형식으로만 작성하고, 그 외 부가 설명은 하지 마세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 모델 호출을 통해 질문 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 응답에서 질문 문자열 추출\n",
    "    questions_text = response.choices[0].message.content.strip()\n",
    "    questions = []\n",
    "\n",
    "    # 줄 단위로 질문을 추출하고 정리\n",
    "    for line in questions_text.split('\\n'):\n",
    "        # 번호 제거 및 양쪽 공백 제거\n",
    "        cleaned_line = re.sub(r'^\\d+\\.\\s*', '', line.strip())\n",
    "        if cleaned_line and cleaned_line.endswith('?'):\n",
    "            questions.append(cleaned_line)\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    지정된 모델을 사용하여 입력 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 임베딩을 생성할 입력 텍스트 또는 텍스트 리스트.\n",
    "        model (str): 사용할 임베딩 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "        dict: 생성된 임베딩 정보를 포함한 OpenAI API의 응답 객체.\n",
    "    \"\"\"\n",
    "    # 입력 텍스트에 대해 임베딩 생성 요청\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    # 응답 객체 반환\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용한 간단한 벡터 저장소 구현 클래스입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소를 초기화합니다.\n",
    "        \"\"\"\n",
    "        self.vectors = []    # 임베딩 벡터들을 저장\n",
    "        self.texts = []      # 원본 텍스트들을 저장\n",
    "        self.metadata = []   # 텍스트에 대한 메타데이터 저장\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트.\n",
    "            embedding (List[float]): 임베딩 벡터.\n",
    "            metadata (dict, optional): 추가 메타데이터 (기본값: None).\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))             # 벡터 추가\n",
    "        self.texts.append(text)                              # 텍스트 추가\n",
    "        self.metadata.append(metadata or {})                 # 메타데이터 추가\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        쿼리 임베딩과 가장 유사한 항목을 검색합니다.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 쿼리 벡터.\n",
    "            k (int): 반환할 결과 수 (기본값: 5).\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개의 유사 항목. 텍스트, 메타데이터, 유사도 포함.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding)\n",
    "        similarities = []\n",
    "\n",
    "        # 각 벡터와의 코사인 유사도 계산\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Documents with Question Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200, questions_per_chunk=5):\n",
    "    \"\"\"\n",
    "    문서를 처리하고, 각 청크에 대해 질문을 생성하여 벡터 저장소에 추가합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로.\n",
    "        chunk_size (int): 각 청크의 문자 수.\n",
    "        chunk_overlap (int): 청크 간 중첩 문자 수.\n",
    "        questions_per_chunk (int): 청크당 생성할 질문 수.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore]: 생성된 텍스트 청크 리스트와 벡터 저장소 객체.\n",
    "    \"\"\"\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    print(\"텍스트 청크 분할 중...\")\n",
    "    text_chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"총 {len(text_chunks)}개의 텍스트 청크가 생성되었습니다.\")\n",
    "    \n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    print(\"각 청크에 대해 임베딩 및 질문 생성 중...\")\n",
    "    for i, chunk in enumerate(tqdm(text_chunks, desc=\"청크 처리 중\")):\n",
    "        # 청크 임베딩 생성\n",
    "        chunk_embedding_response = create_embeddings(chunk)\n",
    "        chunk_embedding = chunk_embedding_response.data[0].embedding\n",
    "        \n",
    "        # 청크를 벡터 저장소에 추가\n",
    "        vector_store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=chunk_embedding,\n",
    "            metadata={\"type\": \"chunk\", \"index\": i}\n",
    "        )\n",
    "        \n",
    "        # 해당 청크 기반 질문 생성\n",
    "        questions = generate_questions(chunk, num_questions=questions_per_chunk)\n",
    "        \n",
    "        # 각 질문에 대한 임베딩 생성 후 저장소에 추가\n",
    "        for j, question in enumerate(questions):\n",
    "            question_embedding_response = create_embeddings(question)\n",
    "            question_embedding = question_embedding_response.data[0].embedding\n",
    "            \n",
    "            vector_store.add_item(\n",
    "                text=question,\n",
    "                embedding=question_embedding,\n",
    "                metadata={\n",
    "                    \"type\": \"question\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"original_chunk\": chunk\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    return text_chunks, vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Processing the Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF에서 텍스트 추출 중...\n",
      "텍스트 청크 분할 중...\n",
      "총 21개의 텍스트 청크가 생성되었습니다.\n",
      "각 청크에 대해 임베딩 및 질문 생성 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "청크 처리 중: 100%|██████████| 21/21 [01:25<00:00,  4.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "벡터 저장소에 저장된 항목 수: 84개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# PDF 파일 경로 정의\n",
    "pdf_path = \"dataset/AI_Understanding.pdf\"\n",
    "\n",
    "# 문서 처리: 텍스트 추출, 청크 분할, 질문 생성, 벡터 저장소 구축\n",
    "text_chunks, vector_store = process_document(\n",
    "    pdf_path, \n",
    "    chunk_size=1000,       # 각 청크는 1000자\n",
    "    chunk_overlap=200,     # 청크 간 200자 중첩\n",
    "    questions_per_chunk=3  # 청크당 질문 3개 생성\n",
    ")\n",
    "\n",
    "# 벡터 저장소에 저장된 항목 개수 출력\n",
    "print(f\"벡터 저장소에 저장된 항목 수: {len(vector_store.texts)}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    쿼리와 벡터 저장소를 이용한 의미 기반 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 검색 쿼리.\n",
    "        vector_store (SimpleVectorStore): 검색 대상 벡터 저장소.\n",
    "        k (int): 반환할 결과 개수.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 관련성 높은 상위 k개의 항목 (텍스트, 메타데이터, 유사도 포함).\n",
    "    \"\"\"\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding_response = create_embeddings(query)\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "    \n",
    "    # 벡터 저장소에서 유사한 항목 검색\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on the Augmented Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: '설명 가능한 AI(Explainable AI)'란 무엇이며, 왜 중요한가?\n",
      "\n",
      "Search Results:\n",
      "\n",
      "Relevant Document Chunks:\n",
      "\n",
      "Matched Questions:\n",
      "Question 1 (유사도: 0.6719):\n",
      "설명 가능한 AI(XAI)의 목표는 무엇인가요?\n",
      "연결된 청크 인덱스: 5\n",
      "-----------------------------\n",
      "Question 2 (유사도: 0.6670):\n",
      "AI의 투명성과 설명 가능성을 높이는 것이 왜 중요한가요?\n",
      "연결된 청크 인덱스: 4\n",
      "-----------------------------\n",
      "Question 3 (유사도: 0.6499):\n",
      "설명 가능한 AI(XAI)의 주요 목표는 무엇인가요?\n",
      "연결된 청크 인덱스: 14\n",
      "-----------------------------\n",
      "Question 4 (유사도: 0.5694):\n",
      "인공 지능(AI)의 정의는 무엇인가요?\n",
      "연결된 청크 인덱스: 0\n",
      "-----------------------------\n",
      "Question 5 (유사도: 0.5665):\n",
      "사용자에게 AI 시스템을 제어할 수 있는 권한을 부여하는 것이 왜 중요한가요?\n",
      "연결된 청크 인덱스: 18\n",
      "-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터셋을 JSON 파일에서 불러옵니다.\n",
    "with open('dataset/validation.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 첫 번째 쿼리를 추출합니다.\n",
    "query = data[0]['question']\n",
    "\n",
    "# 의미 기반 검색을 통해 관련 있는 콘텐츠를 찾습니다.\n",
    "search_results = semantic_search(query, vector_store, k=5)\n",
    "\n",
    "print(\"Query:\", query)\n",
    "print(\"\\nSearch Results:\")\n",
    "\n",
    "# 검색 결과를 타입에 따라 분류합니다.\n",
    "chunk_results = []\n",
    "question_results = []\n",
    "\n",
    "for result in search_results:\n",
    "    if result[\"metadata\"][\"type\"] == \"chunk\":\n",
    "        chunk_results.append(result)\n",
    "    else:\n",
    "        question_results.append(result)\n",
    "\n",
    "# 먼저 문서 청크 결과 출력\n",
    "print(\"\\nRelevant Document Chunks:\")\n",
    "for i, result in enumerate(chunk_results):\n",
    "    print(f\"Context {i + 1} (유사도: {result['similarity']:.4f}):\")\n",
    "    print(result[\"text\"][:300] + \"...\")\n",
    "    print(\"-----------------------------\")\n",
    "\n",
    "# 이어서 관련 질문 결과 출력\n",
    "print(\"\\nMatched Questions:\")\n",
    "for i, result in enumerate(question_results):\n",
    "    print(f\"Question {i + 1} (유사도: {result['similarity']:.4f}):\")\n",
    "    print(result[\"text\"])\n",
    "    chunk_idx = result[\"metadata\"].get(\"chunk_index\", \"N/A\")\n",
    "    print(f\"연결된 청크 인덱스: {chunk_idx}\")\n",
    "    print(\"-----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Context for Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(search_results):\n",
    "    \"\"\"\n",
    "    응답 생성을 위한 통합 컨텍스트를 구성합니다.\n",
    "\n",
    "    Args:\n",
    "        search_results (List[Dict]): 의미 기반 검색 결과.\n",
    "\n",
    "    Returns:\n",
    "        str: 결합된 전체 컨텍스트 문자열.\n",
    "    \"\"\"\n",
    "    # 이미 포함된 청크 인덱스를 추적하기 위한 집합\n",
    "    chunk_indices = set()\n",
    "    context_chunks = []\n",
    "    \n",
    "    # 우선적으로 직접적으로 매칭된 문서 청크를 추가\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"chunk\":\n",
    "            chunk_idx = result[\"metadata\"][\"index\"]\n",
    "            if chunk_idx not in chunk_indices:\n",
    "                chunk_indices.add(chunk_idx)\n",
    "                context_chunks.append(f\"Chunk {chunk_idx}:\\n{result['text']}\")\n",
    "    \n",
    "    # 질문이 참조하는 원본 청크도 추가 (중복 제외)\n",
    "    for result in search_results:\n",
    "        if result[\"metadata\"][\"type\"] == \"question\":\n",
    "            chunk_idx = result[\"metadata\"][\"chunk_index\"]\n",
    "            if chunk_idx not in chunk_indices:\n",
    "                chunk_indices.add(chunk_idx)\n",
    "                original_chunk = result[\"metadata\"][\"original_chunk\"]\n",
    "                question_text = result[\"text\"]\n",
    "                context_chunks.append(\n",
    "                    f\"Chunk {chunk_idx} (참조 질문: '{question_text}'):\\n{original_chunk}\"\n",
    "                )\n",
    "    \n",
    "    # 모든 청크를 하나의 문자열로 결합\n",
    "    full_context = \"\\n\\n\".join(context_chunks)\n",
    "    return full_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    쿼리와 컨텍스트를 기반으로 AI 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자의 질문.\n",
    "        context (str): 벡터 저장소에서 검색된 문맥 정보.\n",
    "        model (str): 사용할 언어 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "        str: 생성된 응답 텍스트.\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 반드시 주어진 문맥에 기반해 응답하도록 설정\n",
    "    system_prompt = (\n",
    "        \"당신은 주어진 컨텍스트에 기반하여 엄격하게 응답하는 AI 어시스턴트입니다. \"\n",
    "        \"제공된 문맥에서 직접적으로 답변을 도출할 수 없는 경우, 다음과 같이 응답하세요: \"\n",
    "        \"'I do not have enough information to answer that.'\"\n",
    "    )\n",
    "    \n",
    "    # 사용자 프롬프트: 질문과 함께 문맥을 제공\n",
    "    user_prompt = f\"\"\"\n",
    "        문맥:\n",
    "        {context}\n",
    "\n",
    "        질문: {query}\n",
    "\n",
    "        위에 제공된 문맥에만 근거하여 질문에 답하세요. 간결하고 정확하게 응답해 주세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    # AI 모델을 호출하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 생성된 응답만 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and Displaying the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "질문(Query): '설명 가능한 AI(Explainable AI)'란 무엇이며, 왜 중요한가?\n",
      "\n",
      "응답(Response):\n",
      "설명 가능한 AI(Explainable AI, XAI)는 AI 시스템을 더욱 투명하고 이해하기 쉽게 만드는 것을 목표로 합니다. XAI 기술은 AI 모델이 의사 결정을 내리는 방식에 대한 인사이트를 제공하여 신뢰와 책임감을 향상시키기 위해 개발되고 있습니다. 이는 AI 시스템에 대한 신뢰를 구축하는 데 필수적이며, 사용자가 AI의 공정성과 정확성을 평가할 수 있도록 돕습니다.\n"
     ]
    }
   ],
   "source": [
    "# 검색 결과로부터 응답 생성을 위한 컨텍스트를 준비합니다.\n",
    "context = prepare_context(search_results)\n",
    "\n",
    "# 쿼리와 문맥을 기반으로 AI 응답을 생성합니다.\n",
    "response_text = generate_response(query, context)\n",
    "\n",
    "# 쿼리와 응답 출력\n",
    "print(\"\\n질문(Query):\", query)\n",
    "print(\"\\n응답(Response):\")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(query, response, reference_answer, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    AI가 생성한 응답을 기준 정답과 비교하여 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문.\n",
    "        response (str): AI가 생성한 응답.\n",
    "        reference_answer (str): 기준이 되는 정답.\n",
    "        model (str): 평가에 사용할 언어 모델.\n",
    "\n",
    "    Returns:\n",
    "        str: 평가 결과 및 점수에 대한 설명.\n",
    "    \"\"\"\n",
    "    # 평가 시스템용 시스템 프롬프트 정의\n",
    "    evaluate_system_prompt = \"\"\"당신은 AI 응답을 평가하는 지능형 평가 시스템입니다.\n",
    "    \n",
    "    AI 어시스턴트의 응답을 기준 정답과 비교하여 다음 기준으로 평가하세요:\n",
    "    1. 사실성(Factual correctness) - 정확한 정보를 담고 있는가?\n",
    "    2. 완전성(Completeness) - 기준 정답의 핵심 내용을 충분히 포함하고 있는가?\n",
    "    3. 관련성(Relevance) - 질문에 직접적으로 응답하고 있는가?\n",
    "\n",
    "    아래 기준에 따라 0 ~ 1 사이의 점수를 부여하세요:\n",
    "    - 1.0: 내용과 의미가 완벽하게 일치함\n",
    "    - 0.8: 아주 좋음, 약간의 누락 또는 차이 있음\n",
    "    - 0.6: 좋음, 주요 내용을 담고 있으나 일부 세부 정보 부족\n",
    "    - 0.4: 부분적인 응답, 중요한 내용이 빠짐\n",
    "    - 0.2: 관련된 정보가 거의 없음\n",
    "    - 0.0: 틀리거나 전혀 관련 없는 응답\n",
    "\n",
    "    점수와 함께 평가 사유를 제시하세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 평가 요청용 프롬프트 구성\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    사용자 질문:\n",
    "    {query}\n",
    "\n",
    "    AI 응답:\n",
    "    {response}\n",
    "\n",
    "    기준 정답:\n",
    "    {reference_answer}\n",
    "\n",
    "    위 기준에 따라 AI 응답을 평가해 주세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 평가 모델 호출\n",
    "    eval_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": evaluate_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 평가 결과 반환\n",
    "    return eval_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:\n",
      "점수: 0.8\n",
      "\n",
      "평가 사유: AI 응답은 설명 가능한 AI(XAI)의 정의와 중요성을 잘 설명하고 있으며, 기준 정답의 핵심 내용을 대부분 포함하고 있습니다. 그러나 \"AI 모델이 의사 결정을 내리는 방식에 대한 인사이트를 제공\"이라는 표현은 다소 모호하게 느껴질 수 있으며, \"공정성과 정확성을 평가할 수 있도록 돕는다\"는 부분이 기준 정답에서 강조된 \"공정성 보장\"과는 약간의 차이가 있습니다. 따라서 약간의 누락이 있지만 전반적으로 매우 좋은 응답입니다.\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에서 기준 정답을 가져옵니다.\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# 생성된 응답을 기준 정답과 비교하여 평가합니다.\n",
    "evaluation = evaluate_response(query, response_text, reference_answer)\n",
    "\n",
    "# 평가 결과 출력\n",
    "print(\"\\nEvaluation:\")\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Chunking Text from a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 텍스트 청크 수: 21\n",
      "\n",
      "첫 번째 텍스트 청크:\n",
      "인공 지능 이해 \n",
      " \n",
      "1장: 인공 지능 소개 \n",
      "인공 지능(AI)은 디지털 컴퓨터 또는 컴퓨터로 제어되는 로봇이 지적인 존재와 일반적으로 \n",
      "관련된 작업을 수행할 수 있는 능력을 말합니다. 이 용어는 추론, 의미 발견, 일반화, 과거 \n",
      "경험으로부터의 학습 능력 등 인간의 특징적인 지적 프로세스가 부여된 시스템을 \n",
      "개발하는 프로젝트에 자주 적용됩니다. 지난 수십 년 동안 컴퓨팅 성능과 데이터 가용성의 \n",
      "발전으로 AI의 개발과 배포가 크게 가속화되었습니다. \n",
      "역사적 맥락 \n",
      "인공 지능에 대한 개념은 수세기 동안 존재해 왔으며 종종 신화와 소설에 묘사되기도 \n",
      "했습니다. 하지만 공식적인 AI 연구 분야는 20세기 중반에 시작되었습니다. 1956년 \n",
      "다트머스 워크숍은 AI의 발상지로 널리 알려져 있습니다. 초기 AI 연구는 문제 해결과 \n",
      "상징적 방법에 중점을 두었습니다. 1980년대에는 전문가 시스템이 등장했고, 1990년대와 \n",
      "2000년대에는 머신러닝과 신경망이 발전했습니다. 최근 딥러닝의 획기적인 발전은 이 \n",
      "분야에 혁신을 가져왔습니다. \n",
      "현대 관측 \n",
      "최신 AI 시스템은 일상 생활에서 점점 더 널리 보급되고 있습니다. Siri와 Alexa 같은 가상 \n",
      "비서부터 스트리밍 서비스 및 소셜 미디어의 추천 알고리즘에 이르기까지 AI는 우리의 \n",
      "생활, 업무, 상호 작용 방식에 영향을 미치고 있습니다. 자율 주행 자동차, 첨단 의료 진단, \n",
      "정교한 재무 모델링 도구의 개발은 AI가 광범위하고 성장하는 응용 분야를 보여줍니다. \n",
      "윤리적 영향, 편견, 일자리 대체에 대한 우려도 점점 더 커지고 있습니다. \n",
      " \n",
      "2장: 인공 지능의 핵심 개념 \n",
      "머신 러닝 \n",
      "머신러닝(ML)은 명시적으로 프로그래밍하지 않고도 시스템이 데이터로부터 학습할 수 \n",
      "있도록 하는 데 중점을 둔 AI의 하위 집합입니다. ML 알고리즘은 더 많은 데이터에 \n",
      "노출됨에 따라 패턴을 식별하고 예측하며 시간이 지남에 따라 성능을 개선합니다. \n",
      "지도 학습 \n",
      "지도 학습에서는 입력 데이터가 올바른 출력과 짝을 이루는 레이블이 지정된\n"
     ]
    }
   ],
   "source": [
    "# PDF 파일 경로를 정의합니다.\n",
    "pdf_path = \"dataset/AI_Understanding.pdf\"\n",
    "\n",
    "# PDF 파일에서 텍스트를 추출합니다.\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# 텍스트를 1000자 단위, 200자 중첩으로 청크 분할합니다.\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# 생성된 텍스트 청크 수 출력\n",
    "print(\"생성된 텍스트 청크 수:\", len(text_chunks))\n",
    "\n",
    "# 첫 번째 청크 출력\n",
    "print(\"\\n첫 번째 텍스트 청크:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    지정된 모델을 사용하여 입력 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 임베딩을 생성할 입력 텍스트 또는 텍스트 리스트.\n",
    "        model (str): 사용할 임베딩 모델.\n",
    "\n",
    "    Returns:\n",
    "        dict: 생성된 임베딩 정보를 포함한 응답 객체.\n",
    "    \"\"\"\n",
    "    # 지정된 모델로 임베딩 생성 요청\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    # 응답 객체 반환\n",
    "    return response\n",
    "\n",
    "# 텍스트 청크에 대해 임베딩을 생성합니다.\n",
    "response = create_embeddings(text_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    두 벡터 간의 코사인 유사도를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        vec1 (np.ndarray): 첫 번째 벡터.\n",
    "        vec2 (np.ndarray): 두 번째 벡터.\n",
    "\n",
    "    Returns:\n",
    "        float: 두 벡터 간의 코사인 유사도 값.\n",
    "    \"\"\"\n",
    "    # 두 벡터의 내적을 계산하고, 벡터의 크기(norm) 곱으로 나누어 코사인 유사도를 구함\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    주어진 쿼리와 임베딩을 기반으로 텍스트 청크에서 의미 기반 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 의미 검색에 사용할 쿼리.\n",
    "        text_chunks (List[str]): 검색 대상이 되는 텍스트 청크 리스트.\n",
    "        embeddings (List[dict]): 각 텍스트 청크에 대한 임베딩 리스트.\n",
    "        k (int): 반환할 관련 청크의 수 (기본값: 5).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 쿼리와 가장 관련성 높은 상위 k개의 텍스트 청크 리스트.\n",
    "    \"\"\"\n",
    "    # 쿼리에 대한 임베딩 생성\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # 유사도 점수를 저장할 리스트 초기화\n",
    "\n",
    "    # 각 텍스트 청크 임베딩과 쿼리 임베딩 간의 유사도 계산\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(\n",
    "            np.array(query_embedding),\n",
    "            np.array(chunk_embedding.embedding)\n",
    "        )\n",
    "        similarity_scores.append((i, similarity_score))  # (인덱스, 유사도) 저장\n",
    "\n",
    "    # 유사도 기준 내림차순 정렬\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 상위 k개의 인덱스를 추출\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "\n",
    "    # 해당 인덱스의 텍스트 청크들을 반환\n",
    "    return [text_chunks[index] for index in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on Extracted Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문(Query): '설명 가능한 AI(Explainable AI)'란 무엇이며, 왜 중요한가?\n",
      "\n",
      "컨텍스트 1:\n",
      "고 사회에 유익한 AI 시스템의 개발과 \n",
      "배포를 보장하기 위한 지침입니다. 주요 원칙에는 인권 존중, 개인정보 보호, 비차별, \n",
      "공익성이 포함됩니다. \n",
      "AI의 편향성 해결 \n",
      "AI 시스템은 데이터에 존재하는 편견을 유전하고 증폭시켜 불공정하거나 차별적인 결과를 \n",
      "초래할 수 있습니다. 편향성을 해결하려면 신중한 데이터 수집, 알고리즘 설계, 지속적인 \n",
      "모니터링 및 평가가 필요합니다. \n",
      "투명성 및 설명 가능성 \n",
      "투명성과 설명 가능성은 AI 시스템에 대한 신뢰를 구축하는 데 필수적입니다. 설명 가능한 \n",
      "AI(XAI) 기술은 AI의 결정을 더 이해하기 쉽게 만들어 사용자가 공정성과 정확성을 \n",
      "평가할 수 있도록 하는 것을 목표로 합니다. \n",
      "개인정보 및 데이터 보호 \n",
      "AI 시스템은 대량의 데이터에 의존하는 경우가 많기 때문에 개인정보 보호와 데이터 \n",
      "보호에 대한 우려가 제기됩니다. 책임감 있는 데이터 처리, 개인정보 보호 기술 구현, \n",
      "데이터 보호 규정 준수는 매우 중요합니다. \n",
      "책임과 의무 \n",
      "AI 시스템에 대한 책임과 의무를 확립하는 것은 잠재적인 피해를 해결하고 윤리적 행동을 \n",
      "보장하는 데 필수적입니다. 여기에는 AI 시스템의 개발자, 배포자, 사용자에 대한 역할과 \n",
      "책임을 정의하는 것이 포함됩니다. \n",
      " \n",
      "20장: AI에 대한 신뢰 구축 \n",
      "투명성 및 설명 가능성 \n",
      "투명성과 설명 가능성은 AI에 대한 신뢰를 구축하는 데 있어 핵심입니다. AI 시스템을 \n",
      "이해하기 쉽게 만들고 의사 결정 프로세스에 대한 인사이트를 제공하면 사용자가 AI의 \n",
      "신뢰성과 공정성을 평가하는 데 도움이 됩니다. \n",
      "견고성 및 신뢰성 \n",
      "AI 시스템의 견고성과 신뢰성을 확보하는 것은 신뢰를 구축하는 데 필수적입니다. \n",
      "여기에는 AI 모델 테스트 및 검증, 성능 모니터링, 잠재적인 취약점 해결이 포함됩니다. \n",
      "사용자 제어 및 대행사 \n",
      "사용자에게 AI 시스템을 제어할 수 있는 권한을 부여하고 AI와의 상호 작용에 대한 \n",
      "선택권을 제공하면 신뢰가 향상됩니다. 여기에는 사용자가 AI 설정을 사용자 지정하고, \n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "컨텍스트 2:\n",
      "동화하고 위협 탐지 정확도를 \n",
      "개선하며 전반적인 사이버 보안 태세를 강화할 수 있습니다. \n",
      " \n",
      "4장: AI의 윤리적, 사회적 의미 \n",
      "AI의 급속한 발전과 보급은 윤리적, 사회적으로 심각한 우려를 불러일으킵니다. 이러한 \n",
      "우려에는 다음이 포함됩니다: \n",
      "편견과 공정성 \n",
      "AI 시스템은 데이터에 존재하는 편견을 계승하고 증폭시켜 불공정하거나 차별적인 결과를 \n",
      "초래할 수 있습니다. AI 시스템의 공정성을 보장하고 편견을 완화하는 것은 매우 중요한 \n",
      "과제입니다. \n",
      "투명성 및 설명 가능성 \n",
      "많은 AI 시스템, 특히 딥러닝 모델은 '블랙박스'와 같아서 어떤 방식으로 의사 결정에 \n",
      "도달하는지 이해하기 어렵습니다. 투명성과 설명 가능성을 높이는 것은 신뢰와 책임감을 \n",
      "구축하는 데 매우 중요합니다. \n",
      "개인정보 보호 및 보안 \n",
      "AI 시스템은 대량의 데이터에 의존하는 경우가 많기 때문에 개인정보 보호 및 데이터 \n",
      "보안에 대한 우려가 제기됩니다. 민감한 정보를 보호하고 책임감 있는 데이터 처리를 \n",
      "보장하는 것은 필수적입니다. \n",
      "일자리 이동 \n",
      "AI의 자동화 기능으로 인해 특히 반복적이거나 일상적인 업무가 많은 산업에서 일자리 \n",
      "대체에 대한 우려가 제기되고 있습니다. AI 기반 자동화의 잠재적인 경제적, 사회적 영향을 \n",
      "해결하는 것이 핵심 과제입니다. \n",
      "자율성 및 제어 \n",
      "AI 시스템이 더욱 자율화됨에 따라 통제, 책임, 의도하지 않은 결과의 발생 가능성에 대한 \n",
      "의문이 제기되고 있습니다. AI 개발 및 배포를 위한 명확한 가이드라인과 윤리적 \n",
      "프레임워크를 수립하는 것이 중요합니다. \n",
      "AI의 무기화 \n",
      "자율 무기 시스템에 AI를 사용할 경우 심각한 윤리적, 보안적 우려가 제기될 수 있습니다. AI \n",
      "기반 무기와 관련된 위험을 해결하기 위해 국제적인 논의와 규제가 필요합니다. \n",
      " \n",
      "5장: 인공 지능의 미래 \n",
      "AI의 미래는 다양한 영역에서 지속적인 발전과 폭넓은 도입으로 특징지어질 것입니다. \n",
      "주요 트렌드와 개발 분야는 다음과 같습니다: \n",
      "설명 가능한 AI(XAI) \n",
      "설명 가능한 AI(XAI)는\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터를 JSON 파일에서 불러옵니다.\n",
    "with open('dataset/validation.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 첫 번째 쿼리를 추출합니다.\n",
    "query = data[0]['question']\n",
    "\n",
    "# 의미 기반 검색을 수행하여 쿼리와 가장 관련성 높은 상위 2개의 텍스트 청크를 찾습니다.\n",
    "top_chunks = semantic_search(query, text_chunks, response.data, k=2)\n",
    "\n",
    "# 쿼리를 출력합니다.\n",
    "print(\"질문(Query):\", query)\n",
    "\n",
    "# 상위 2개의 관련 텍스트 청크를 출력합니다.\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"\\n컨텍스트 {i + 1}:\\n{chunk}\\n{'-' * 37}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI 어시스턴트를 위한 시스템 프롬프트 정의\n",
    "system_prompt = (\n",
    "    \"당신은 주어진 컨텍스트에 기반하여 엄격하게 응답하는 AI 어시스턴트입니다. \"\n",
    "    \"제공된 문맥에서 직접적으로 답변을 도출할 수 없는 경우, 다음과 같이 응답하세요: \"\n",
    "    \"'I do not have enough information to answer that.'\"\n",
    ")\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    시스템 프롬프트와 사용자 메시지를 기반으로 AI 모델의 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        system_prompt (str): AI의 응답 방식을 정의하는 시스템 프롬프트.\n",
    "        user_message (str): 사용자 메시지 또는 질문.\n",
    "        model (str): 사용할 언어 모델.\n",
    "\n",
    "    Returns:\n",
    "        dict: AI 모델의 응답 객체.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# 검색된 상위 청크들을 바탕으로 사용자 프롬프트 생성\n",
    "user_prompt = \"\\n\".join([\n",
    "    f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\"\n",
    "    for i, chunk in enumerate(top_chunks)\n",
    "])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# AI 응답 생성\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "점수 1\n"
     ]
    }
   ],
   "source": [
    "# 평가 시스템을 위한 시스템 프롬프트 정의\n",
    "evaluate_system_prompt = (\n",
    "    \"당신은 AI 어시스턴트의 응답을 평가하는 지능형 평가 시스템입니다. \"\n",
    "    \"AI 응답이 기준 정답과 매우 유사하면 점수 1을 부여하세요. \"\n",
    "    \"응답이 부정확하거나 부적절하면 점수 0을 부여하세요. \"\n",
    "    \"부분적으로 일치하면 점수 0.5를 부여하세요.\"\n",
    ")\n",
    "\n",
    "# 사용자 쿼리, AI 응답, 기준 정답을 포함한 평가용 프롬프트 생성\n",
    "evaluation_prompt = (\n",
    "    f\"User Query: {query}\\n\"\n",
    "    f\"AI Response:\\n{ai_response.choices[0].message.content}\\n\"\n",
    "    f\"True Response: {data[0]['ideal_answer']}\\n\"\n",
    "    f\"{evaluate_system_prompt}\"\n",
    ")\n",
    "\n",
    "# 평가 수행\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# 평가 점수 출력\n",
    "print(evaluation_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
