{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Contextual Compression for Enhanced RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        str: PDF에서 추출된 전체 텍스트\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 전체 텍스트를 저장할 문자열 초기화\n",
    "\n",
    "    # 각 페이지를 순회하며 텍스트 추출\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]               # 해당 페이지 가져오기\n",
    "        text = page.get_text(\"text\")         # 텍스트 형식으로 내용 추출\n",
    "        all_text += text                     # 추출된 텍스트 누적\n",
    "\n",
    "    # 추출된 전체 텍스트 반환\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로 분할하되, 각 청크 간에 overlap만큼 겹치게 합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 분할할 텍스트\n",
    "        n (int): 각 청크의 문자 수 (기본값: 1000)\n",
    "        overlap (int): 청크 간 겹치는 문자 수 (기본값: 200)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 분할된 텍스트 청크 리스트\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크들을 저장할 리스트 초기화\n",
    "\n",
    "    # n - overlap 만큼 이동하면서 청크 생성\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # i에서 i + n까지의 텍스트 조각을 청크로 추가\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 생성된 청크 리스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 활용한 간단한 벡터 저장소 구현체입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []     # 임베딩 벡터 리스트\n",
    "        self.texts = []       # 원본 텍스트 리스트\n",
    "        self.metadata = []    # 각 텍스트의 메타데이터 리스트\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        단일 텍스트 항목을 벡터 저장소에 추가합니다.\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트\n",
    "            embedding (List[float]): 텍스트의 임베딩 벡터\n",
    "            metadata (dict, optional): 추가 메타데이터 (기본값: 빈 딕셔너리)\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # 임베딩을 NumPy 배열로 변환하여 저장\n",
    "        self.texts.append(text)                   # 원본 텍스트 저장\n",
    "        self.metadata.append(metadata or {})      # 메타데이터 저장 (None일 경우 빈 딕셔너리)\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        질의 임베딩과 가장 유사한 텍스트를 검색합니다.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 질의 임베딩 벡터\n",
    "            k (int): 반환할 상위 결과 개수\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 유사한 항목 리스트 (텍스트, 메타데이터, 유사도 포함)\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "\n",
    "        # 질의 벡터를 NumPy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "\n",
    "        # 각 벡터와의 코사인 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (\n",
    "                np.linalg.norm(query_vector) * np.linalg.norm(vector)\n",
    "            )\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 저장\n",
    "\n",
    "        # 유사도 기준 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 상위 k개 항목 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대해 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트(또는 텍스트 리스트)\n",
    "        model (str): 사용할 임베딩 모델 이름\n",
    "\n",
    "    Returns:\n",
    "        List[float] 또는 List[List[float]]: 생성된 임베딩 벡터 또는 벡터 리스트\n",
    "    \"\"\"\n",
    "    # 입력이 문자열 하나일 수도 있고, 문자열 리스트일 수도 있으므로 리스트 형태로 통일\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "\n",
    "    # 지정된 모델을 사용하여 임베딩 생성 요청\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "\n",
    "    # 입력이 단일 문자열이었을 경우, 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    # 여러 문자열일 경우, 모든 임베딩 리스트 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Our Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    RAG 처리를 위한 문서 전처리 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로.\n",
    "        chunk_size (int): 각 청크의 문자 수.\n",
    "        chunk_overlap (int): 청크 간 겹치는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "        SimpleVectorStore: 청크 및 임베딩이 저장된 벡터 저장소 객체.\n",
    "    \"\"\"\n",
    "    print(\"PDF에서 텍스트를 추출합니다...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"텍스트를 청크 단위로 분할합니다...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"{len(chunks)}개의 청크가 생성되었습니다.\")\n",
    "\n",
    "    print(\"청크 임베딩을 생성합니다...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "\n",
    "    print(\"벡터 저장소를 초기화합니다...\")\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "\n",
    "    print(f\"{len(chunks)}개의 청크가 벡터 저장소에 추가되었습니다.\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compress_chunk(chunk, query, compression_type=\"selective\", model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트 청크에서 질의에 관련된 정보만 추출하여 압축합니다.\n",
    "\n",
    "    Args:\n",
    "        chunk (str): 압축 대상 텍스트 청크\n",
    "        query (str): 사용자 질의\n",
    "        compression_type (str): 압축 방식 (\"selective\", \"summary\", \"extraction\")\n",
    "        model (str): 사용할 LLM 모델 이름\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, float]: 압축된 청크 문자열과 압축률(%) \n",
    "    \"\"\"\n",
    "    \n",
    "    # 압축 방식에 따라 시스템 프롬프트 설정\n",
    "    if compression_type == \"selective\":\n",
    "        # 관련 문장 또는 문단만 **선택적 추출**\n",
    "        system_prompt = \"\"\"귀하는 정보 필터링 전문가입니다.\n",
    "        귀하의 임무는 문서 청크를 분석하여 사용자의 쿼리와 직접적으로 관련된 문장이나 단락만\n",
    "        추출하는 것입니다. 관련 없는 콘텐츠는 모두 제거하세요.\n",
    "\n",
    "        출력물은 다음과 같아야 합니다:\n",
    "        1. 쿼리에 대한 답변에 도움이 되는 텍스트만 포함해야 합니다.\n",
    "        2. 관련 문장의 정확한 표현을 유지하세요(의역하지 마세요).\n",
    "        3. 텍스트의 원래 순서 유지\n",
    "        4. 중복되는 것처럼 보이더라도 모든 관련 콘텐츠를 포함하세요.\n",
    "        5. 쿼리와 관련이 없는 모든 텍스트 제외\n",
    "\n",
    "        추가 설명 없이 일반 텍스트로 응답 형식을 지정하세요.\"\"\"\n",
    "    \n",
    "    elif compression_type == \"summary\":\n",
    "        # 관련 내용을 **요약** 형태로 제공\n",
    "        system_prompt = \"\"\"귀하는 요약의 전문가입니다.\n",
    "        여러분의 임무는 제공된 청크의 간결한 요약을 작성하여 사용자의 쿼리와 관련된 정보에만 초점을 맞추는 것입니다.\n",
    "        정보에만 초점을 맞춘 간결한 요약을 작성하는 것입니다.\n",
    "\n",
    "        출력물은 다음과 같아야 합니다:\n",
    "        1. 쿼리 관련 정보에 대해 간략하지만 포괄적으로 작성해야 합니다.\n",
    "        2. 쿼리와 관련된 정보에만 집중해야 합니다.\n",
    "        3. 관련 없는 세부 정보는 생략\n",
    "        4. 중립적이고 사실적인 어조로 작성합니다.\n",
    "\n",
    "        추가 설명 없이 일반 텍스트로 응답 형식을 지정합니다.\"\"\"\n",
    "    \n",
    "    else:  # \"extraction\"\n",
    "        # 질의 관련 **문장만 원문 그대로 추출**\n",
    "        system_prompt = \"\"\"귀하는 정보 추출 전문가입니다.\n",
    "        귀하의 임무는 문서 청크에서 사용자의 쿼리에 대한 답변과 관련된 정보가 포함된 정확한 문장만\n",
    "        정확한 문장만 추출하는 것입니다.\n",
    "\n",
    "        출력은 다음과 같아야 합니다:\n",
    "        1. 원본 텍스트에서 관련 문장의 직접 인용문만 포함해야 합니다.\n",
    "        2. 원본 문구를 그대로 유지합니다(텍스트를 수정하지 않습니다).\n",
    "        3. 쿼리와 직접 관련된 문장만 포함하세요.\n",
    "        4. 추출된 문장을 개행으로 구분합니다.\n",
    "        5. 주석이나 추가 텍스트를 추가하지 마세요.\n",
    "\n",
    "        추가 설명 없이 일반 텍스트로 응답 형식을 지정합니다.\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 구성 (질의 + 문서 청크)\n",
    "    user_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Document Chunk:\n",
    "        {chunk}\n",
    "\n",
    "        이 쿼리에 대한 답변과 관련된 콘텐츠만 추출합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    # LLM을 호출하여 압축된 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 생성된 압축된 텍스트 추출\n",
    "    compressed_chunk = response.choices[0].message.content.strip()\n",
    "\n",
    "    # 압축률 계산 (압축 전과 후 길이 차이로 % 계산)\n",
    "    original_length = len(chunk)\n",
    "    compressed_length = len(compressed_chunk)\n",
    "    compression_ratio = (original_length - compressed_length) / original_length * 100\n",
    "\n",
    "    return compressed_chunk, compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Batch Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def batch_compress_chunks(chunks, query, compression_type=\"selective\", model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    여러 개의 텍스트 청크를 개별적으로 압축하여 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): 압축 대상이 되는 텍스트 청크 리스트.\n",
    "        query (str): 사용자 질의.\n",
    "        compression_type (str): 압축 방식 (\"selective\", \"summary\", \"extraction\").\n",
    "        model (str): 사용할 LLM 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[str, float]]: (압축된 텍스트, 개별 압축률)로 구성된 리스트.\n",
    "    \"\"\"\n",
    "    print(f\"{len(chunks)}개의 청크 압축을 시작합니다...\")\n",
    "    results = []\n",
    "    total_original_length = 0\n",
    "    total_compressed_length = 0\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"[{i + 1}/{len(chunks)}] 청크 압축 중...\")\n",
    "\n",
    "        # 개별 청크 압축 수행\n",
    "        compressed_chunk, compression_ratio = compress_chunk(\n",
    "            chunk, query, compression_type, model\n",
    "        )\n",
    "        results.append((compressed_chunk, compression_ratio))\n",
    "\n",
    "        total_original_length += len(chunk)\n",
    "        total_compressed_length += len(compressed_chunk)\n",
    "\n",
    "    # 전체 압축률 출력\n",
    "    overall_ratio = (\n",
    "        (total_original_length - total_compressed_length) / total_original_length * 100\n",
    "        if total_original_length > 0 else 0.0\n",
    "    )\n",
    "    print(f\"전체 압축률: {overall_ratio:.2f}%\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    질의(query)와 문맥(context)을 바탕으로 LLM 응답을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 압축된 청크에서 추출한 문맥 텍스트\n",
    "        model (str): 사용할 LLM 모델 이름\n",
    "        \n",
    "    Returns:\n",
    "        str: 생성된 응답 문자열\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트 정의: LLM의 역할과 응답 조건 지정\n",
    "    system_prompt = \"\"\"당신은 유용한 AI 비서입니다. 제공된 문맥에만 근거하여 사용자의 질문에 답변하세요.\n",
    "    문맥에서 답을 찾을 수 없는 경우 정보가 충분하지 않다고 말합니다.\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 구성: 문맥 + 질문\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        위의 문맥에만 근거하여 포괄적인 답변을 제공하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # LLM 호출을 통해 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0  # 응답 일관성을 위한 설정\n",
    "    )\n",
    "\n",
    "    # 생성된 응답 텍스트 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_compression(pdf_path, query, k=10, compression_type=\"selective\", model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    압축 기반 문맥 생성이 포함된 RAG 파이프라인을 실행합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로.\n",
    "        query (str): 사용자 질의.\n",
    "        k (int): 초기 검색할 청크 수.\n",
    "        compression_type (str): 압축 방식 (\"selective\", \"summary\", \"extraction\").\n",
    "        model (str): 사용할 LLM 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "        dict: 쿼리, 압축된 문맥, 응답, 압축률 등의 결과 딕셔너리.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== 문맥 압축 기반 RAG 실행 ===\")\n",
    "    print(f\"질문: {query}\")\n",
    "    print(f\"압축 방식: {compression_type}\")\n",
    "\n",
    "    # 1. 문서 전처리 → 벡터 저장소 생성\n",
    "    vector_store = process_document(pdf_path)\n",
    "\n",
    "    # 2. 쿼리 임베딩 생성\n",
    "    query_embedding = create_embeddings(query)\n",
    "\n",
    "    # 3. 유사도 기반 상위 k개 청크 검색\n",
    "    print(f\"상위 {k}개의 관련 청크 검색 중...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "\n",
    "    # 4. 검색된 청크에 대해 압축 수행\n",
    "    compressed_results = batch_compress_chunks(retrieved_chunks, query, compression_type, model)\n",
    "    compressed_chunks = [result[0] for result in compressed_results]\n",
    "    compression_ratios = [result[1] for result in compressed_results]\n",
    "\n",
    "    # 5. 압축된 청크가 모두 비어 있는 경우 예외 처리\n",
    "    filtered_chunks = [(chunk, ratio) for chunk, ratio in zip(compressed_chunks, compression_ratios) if chunk.strip()]\n",
    "\n",
    "    if not filtered_chunks:\n",
    "        print(\"⚠️ 모든 청크가 빈 문자열로 압축되었습니다. 원본 청크를 사용합니다.\")\n",
    "        filtered_chunks = [(chunk, 0.0) for chunk in retrieved_chunks]\n",
    "    else:\n",
    "        compressed_chunks, compression_ratios = zip(*filtered_chunks)\n",
    "\n",
    "    # 6. 압축된 청크들을 문맥으로 구성\n",
    "    context = \"\\n\\n---\\n\\n\".join(compressed_chunks)\n",
    "\n",
    "    # 7. 문맥을 기반으로 응답 생성\n",
    "    print(\"압축된 문맥을 기반으로 응답 생성 중...\")\n",
    "    response = generate_response(query, context, model)\n",
    "\n",
    "    # 8. 결과 반환\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"original_chunks\": retrieved_chunks,\n",
    "        \"compressed_chunks\": compressed_chunks,\n",
    "        \"compression_ratios\": compression_ratios,\n",
    "        \"context_length_reduction\": f\"{sum(compression_ratios)/len(compression_ratios):.2f}%\",\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== 최종 응답 ===\")\n",
    "    print(response)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing RAG With and Without Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standard_rag(pdf_path, query, k=10, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    LLM 기반 압축 없이 수행하는 표준 RAG 파이프라인입니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로.\n",
    "        query (str): 사용자 질의.\n",
    "        k (int): 검색할 상위 유사 청크 개수.\n",
    "        model (str): 응답 생성을 위한 LLM 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "        dict: 다음 정보를 포함한 결과 딕셔너리:\n",
    "            - query: 사용자 질의\n",
    "            - chunks: 검색된 청크 목록\n",
    "            - response: LLM이 생성한 응답\n",
    "    \"\"\"\n",
    "    print(\"\\n***표준 RAG 실행***\")\n",
    "    print(f\"질문: {query}\")\n",
    "\n",
    "    # 1. 문서 처리 및 벡터 저장소 생성\n",
    "    vector_store = process_document(pdf_path)\n",
    "\n",
    "    # 2. 쿼리 임베딩 생성\n",
    "    query_embedding = create_embeddings(query)\n",
    "\n",
    "    # 3. 상위 k개의 관련 청크 검색\n",
    "    print(f\"상위 {k}개의 청크 검색 중...\")\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "\n",
    "    # 4. 검색된 청크를 문맥 문자열로 조합\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_chunks)\n",
    "\n",
    "    # 5. 문맥 기반 LLM 응답 생성\n",
    "    print(\"응답 생성 중...\")\n",
    "    response = generate_response(query, context, model)\n",
    "\n",
    "    # 6. 결과 딕셔너리 구성 및 반환\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "    print(\"\\n***최종 응답***\")\n",
    "    print(response)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, responses, reference_answer):\n",
    "    \"\"\"\n",
    "    여러 RAG 응답을 기준 정답과 비교하여 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의.\n",
    "        responses (Dict[str, str]): 각 방식별 응답 딕셔너리. (예: {\"standard\": ..., \"compressed\": ...})\n",
    "        reference_answer (str): 기준 정답.\n",
    "\n",
    "    Returns:\n",
    "        str: 평가 결과 텍스트.\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 평가자 역할 정의\n",
    "    system_prompt = \"\"\"당신은 다양한 RAG 응답을 평가하는 공정한 평가자입니다.\n",
    "    각 응답을 기준 정답과 비교하여 정확성, 포괄성, 관련성, 간결성을 기준으로 평가하고\n",
    "    가장 우수한 응답부터 순위를 매기세요.\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 구성\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "\n",
    "    [Reference Answer]\n",
    "    {reference_answer}\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # 각 방식별 응답 추가\n",
    "    for method, response in responses.items():\n",
    "        user_prompt += f\"[{method.capitalize()} Response]\\n{response}\\n\\n\"\n",
    "\n",
    "    # 평가 기준 안내 추가\n",
    "    user_prompt += \"\"\"\n",
    "    각 응답을 다음 기준에 따라 평가하세요:\n",
    "    1. 정확성 (기준 정답과의 사실 일치 여부)\n",
    "    2. 포괄성 (질문에 대해 얼마나 완전하게 답했는지)\n",
    "    3. 간결성 (불필요한 정보 없이 핵심만 전달했는지)\n",
    "    4. 전반적인 품질\n",
    "\n",
    "    각 응답에 대한 분석을 제공한 후, 가장 우수한 응답부터 순위를 정하고 그 이유를 설명하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # 평가 요청 전송\n",
    "    evaluation_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return evaluation_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_compression(pdf_path, query, reference_answer=None, compression_types=[\"selective\", \"summary\", \"extraction\"]):\n",
    "    \"\"\"\n",
    "    다양한 문맥 압축 방식과 standard RAG를 비교 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로.\n",
    "        query (str): 사용자 질의.\n",
    "        reference_answer (str): 기준 정답 (있을 경우 평가 수행).\n",
    "        compression_types (List[str]): 평가 대상 압축 방식 목록.\n",
    "\n",
    "    Returns:\n",
    "        dict: 다음을 포함한 평가 결과 딕셔너리:\n",
    "            - query: 질의\n",
    "            - responses: 각 방식의 응답\n",
    "            - evaluation: LLM 기반 평가 결과 텍스트\n",
    "            - metrics: 압축률 및 문맥 길이 비교\n",
    "            - standard_result: 압축 미사용 결과\n",
    "            - compression_results: 압축 방식별 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n*** 문맥 압축 방식 평가 시작 ***\")\n",
    "    print(f\"질문: {query}\")\n",
    "\n",
    "    # 1. standard RAG 실행 (압축 없음)\n",
    "    standard_result = standard_rag(pdf_path, query)\n",
    "\n",
    "    # 2. 압축 방식별 RAG 결과 저장\n",
    "    compression_results = {}\n",
    "    for comp_type in compression_types:\n",
    "        print(f\"\\n[{comp_type.upper()} 압축 방식 평가 중...]\")\n",
    "        compression_results[comp_type] = rag_with_compression(\n",
    "            pdf_path=pdf_path,\n",
    "            query=query,\n",
    "            compression_type=comp_type\n",
    "        )\n",
    "\n",
    "    # 3. 방식별 응답 수집\n",
    "    responses = {\n",
    "        \"standard\": standard_result[\"response\"]\n",
    "    }\n",
    "    for comp_type in compression_types:\n",
    "        responses[comp_type] = compression_results[comp_type][\"response\"]\n",
    "\n",
    "    # 4. 평가 수행 (참조 정답이 있는 경우)\n",
    "    if reference_answer:\n",
    "        evaluation = evaluate_responses(query, responses, reference_answer)\n",
    "        print(\"\\n*** 평가 결과 ***\")\n",
    "        print(evaluation)\n",
    "    else:\n",
    "        evaluation = \"기준 정답이 제공되지 않아 자동 평가를 생략했습니다.\"\n",
    "\n",
    "    # 5. 압축 방식별 메트릭 계산\n",
    "    metrics = {}\n",
    "    for comp_type in compression_types:\n",
    "        avg_ratio = sum(compression_results[comp_type][\"compression_ratios\"]) / len(compression_results[comp_type][\"compression_ratios\"])\n",
    "        metrics[comp_type] = {\n",
    "            \"avg_compression_ratio\": f\"{avg_ratio:.2f}%\",\n",
    "            \"total_context_length\": len(\"\\n\\n\".join(compression_results[comp_type][\"compressed_chunks\"])),\n",
    "            \"original_context_length\": len(\"\\n\\n\".join(standard_result[\"chunks\"]))\n",
    "        }\n",
    "\n",
    "    # 6. 결과 정리 및 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"responses\": responses,\n",
    "        \"evaluation\": evaluation,\n",
    "        \"metrics\": metrics,\n",
    "        \"standard_result\": standard_result,\n",
    "        \"compression_results\": compression_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Our Complete System (Custom Query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** 문맥 압축 방식 평가 시작 ***\n",
      "질문: 의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 무엇인가요?\n",
      "\n",
      "***표준 RAG 실행***\n",
      "질문: 의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 무엇인가요?\n",
      "PDF에서 텍스트를 추출합니다...\n",
      "텍스트를 청크 단위로 분할합니다...\n",
      "21개의 청크가 생성되었습니다.\n",
      "청크 임베딩을 생성합니다...\n",
      "벡터 저장소를 초기화합니다...\n",
      "21개의 청크가 벡터 저장소에 추가되었습니다.\n",
      "상위 10개의 청크 검색 중...\n",
      "응답 생성 중...\n",
      "\n",
      "***최종 응답***\n",
      "AI를 의사 결정에 사용하는 것과 관련된 윤리적 우려는 다음과 같습니다:\n",
      "\n",
      "1. **편견과 공정성**: AI 시스템은 데이터에 존재하는 편견을 계승하고 증폭시켜 불공정하거나 차별적인 결과를 초래할 수 있습니다. 따라서 AI 시스템의 공정성을 보장하고 편견을 완화하는 것이 중요한 과제입니다.\n",
      "\n",
      "2. **투명성 및 설명 가능성**: 많은 AI 시스템, 특히 딥러닝 모델은 '블랙박스'와 같아서 의사 결정 과정이 이해하기 어렵습니다. 투명성과 설명 가능성을 높이는 것은 신뢰와 책임감을 구축하는 데 매우 중요합니다.\n",
      "\n",
      "3. **개인정보 보호 및 보안**: AI 시스템은 대량의 데이터에 의존하는 경우가 많기 때문에 개인정보 보호와 데이터 보안에 대한 우려가 제기됩니다. 민감한 정보를 보호하고 책임감 있는 데이터 처리를 보장하는 것이 필수적입니다.\n",
      "\n",
      "4. **자율성 및 제어**: AI 시스템이 더욱 자율화됨에 따라 통제, 책임, 의도하지 않은 결과의 발생 가능성에 대한 의문이 제기되고 있습니다. AI 개발 및 배포를 위한 명확한 가이드라인과 윤리적 프레임워크를 수립하는 것이 중요합니다.\n",
      "\n",
      "5. **AI의 무기화**: 자율 무기 시스템에 AI를 사용할 경우 심각한 윤리적, 보안적 우려가 제기될 수 있습니다. AI 기반 무기와 관련된 위험을 해결하기 위해 국제적인 논의와 규제가 필요합니다.\n",
      "\n",
      "이러한 윤리적 우려들은 AI의 의사 결정 과정에서 신뢰성과 책임성을 확보하기 위해 반드시 고려되어야 합니다.\n",
      "\n",
      "[SELECTIVE 압축 방식 평가 중...]\n",
      "\n",
      "=== 문맥 압축 기반 RAG 실행 ===\n",
      "질문: 의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 무엇인가요?\n",
      "압축 방식: selective\n",
      "PDF에서 텍스트를 추출합니다...\n",
      "텍스트를 청크 단위로 분할합니다...\n",
      "21개의 청크가 생성되었습니다.\n",
      "청크 임베딩을 생성합니다...\n",
      "벡터 저장소를 초기화합니다...\n",
      "21개의 청크가 벡터 저장소에 추가되었습니다.\n",
      "상위 10개의 관련 청크 검색 중...\n",
      "10개의 청크 압축을 시작합니다...\n",
      "[1/10] 청크 압축 중...\n",
      "[2/10] 청크 압축 중...\n",
      "[3/10] 청크 압축 중...\n",
      "[4/10] 청크 압축 중...\n",
      "[5/10] 청크 압축 중...\n",
      "[6/10] 청크 압축 중...\n",
      "[7/10] 청크 압축 중...\n",
      "[8/10] 청크 압축 중...\n",
      "[9/10] 청크 압축 중...\n",
      "[10/10] 청크 압축 중...\n",
      "전체 압축률: 68.27%\n",
      "압축된 문맥을 기반으로 응답 생성 중...\n",
      "\n",
      "=== 최종 응답 ===\n",
      "제공된 문맥에는 의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려에 대한 구체적인 내용이 포함되어 있지 않습니다. 따라서 정보가 충분하지 않습니다.\n",
      "\n",
      "[SUMMARY 압축 방식 평가 중...]\n",
      "\n",
      "=== 문맥 압축 기반 RAG 실행 ===\n",
      "질문: 의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 무엇인가요?\n",
      "압축 방식: summary\n",
      "PDF에서 텍스트를 추출합니다...\n",
      "텍스트를 청크 단위로 분할합니다...\n",
      "21개의 청크가 생성되었습니다.\n",
      "청크 임베딩을 생성합니다...\n",
      "벡터 저장소를 초기화합니다...\n",
      "21개의 청크가 벡터 저장소에 추가되었습니다.\n",
      "상위 10개의 관련 청크 검색 중...\n",
      "10개의 청크 압축을 시작합니다...\n",
      "[1/10] 청크 압축 중...\n",
      "[2/10] 청크 압축 중...\n",
      "[3/10] 청크 압축 중...\n",
      "[4/10] 청크 압축 중...\n",
      "[5/10] 청크 압축 중...\n",
      "[6/10] 청크 압축 중...\n",
      "[7/10] 청크 압축 중...\n",
      "[8/10] 청크 압축 중...\n",
      "[9/10] 청크 압축 중...\n",
      "[10/10] 청크 압축 중...\n",
      "전체 압축률: 73.79%\n",
      "압축된 문맥을 기반으로 응답 생성 중...\n",
      "\n",
      "=== 최종 응답 ===\n",
      "의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 다음과 같습니다:\n",
      "\n",
      "1. **편향성**: AI 시스템이 데이터에 존재하는 편견을 계승하고 증폭시켜 불공정하거나 차별적인 결과를 초래할 수 있습니다. 이를 해결하기 위해 신중한 데이터 수집과 알고리즘 설계가 필요합니다.\n",
      "\n",
      "2. **투명성 및 설명 가능성**: AI의 결정을 이해하기 쉽게 만들어 사용자가 공정성과 정확성을 평가할 수 있도록 하는 것이 중요합니다. 이는 AI에 대한 신뢰를 구축하는 데 필수적입니다.\n",
      "\n",
      "3. **개인정보 및 데이터 보호**: AI 시스템이 대량의 데이터에 의존함에 따라 개인정보 보호와 데이터 보호에 대한 우려가 있으며, 책임감 있는 데이터 처리와 규정 준수가 필요합니다.\n",
      "\n",
      "4. **책임과 의무**: AI 시스템의 개발자, 배포자, 사용자에 대한 역할과 책임을 정의하여 잠재적인 피해를 해결하고 윤리적 행동을 보장해야 합니다.\n",
      "\n",
      "5. **일자리 이동**: AI의 자동화로 인해 일자리 대체에 대한 우려가 있으며, 경제적, 사회적 영향을 해결하는 것이 중요합니다.\n",
      "\n",
      "6. **자율성 및 제어**: AI 시스템의 자율화가 진행됨에 따라 통제와 책임에 대한 의문이 제기되고, 명확한 윤리적 가이드라인이 필요합니다.\n",
      "\n",
      "7. **AI의 무기화**: 자율 무기 시스템에 AI를 사용하는 경우 심각한 윤리적 및 보안적 우려가 있으며, 국제적인 논의와 규제가 필요합니다.\n",
      "\n",
      "이러한 윤리적 우려를 해결하기 위해서는 윤리적 원칙 준수, 공정성 및 투명성을 증진하며 인권과 가치를 보호하는 것이 중요합니다.\n",
      "\n",
      "[EXTRACTION 압축 방식 평가 중...]\n",
      "\n",
      "=== 문맥 압축 기반 RAG 실행 ===\n",
      "질문: 의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 무엇인가요?\n",
      "압축 방식: extraction\n",
      "PDF에서 텍스트를 추출합니다...\n",
      "텍스트를 청크 단위로 분할합니다...\n",
      "21개의 청크가 생성되었습니다.\n",
      "청크 임베딩을 생성합니다...\n",
      "벡터 저장소를 초기화합니다...\n",
      "21개의 청크가 벡터 저장소에 추가되었습니다.\n",
      "상위 10개의 관련 청크 검색 중...\n",
      "10개의 청크 압축을 시작합니다...\n",
      "[1/10] 청크 압축 중...\n",
      "[2/10] 청크 압축 중...\n",
      "[3/10] 청크 압축 중...\n",
      "[4/10] 청크 압축 중...\n",
      "[5/10] 청크 압축 중...\n",
      "[6/10] 청크 압축 중...\n",
      "[7/10] 청크 압축 중...\n",
      "[8/10] 청크 압축 중...\n",
      "[9/10] 청크 압축 중...\n",
      "[10/10] 청크 압축 중...\n",
      "전체 압축률: 74.79%\n",
      "압축된 문맥을 기반으로 응답 생성 중...\n",
      "\n",
      "=== 최종 응답 ===\n",
      "제공된 문맥에는 의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려에 대한 구체적인 내용이 포함되어 있지 않습니다. 따라서 정보가 충분하지 않습니다.\n",
      "\n",
      "*** 평가 결과 ***\n",
      "### 응답 평가\n",
      "\n",
      "1. **[Reference Answer]**\n",
      "   - **정확성**: 10/10 - 기준 정답과 일치하며, AI 의사 결정의 윤리적 우려를 정확하게 설명하고 있습니다.\n",
      "   - **포괄성**: 10/10 - 다양한 윤리적 우려를 포괄적으로 다루고 있으며, 각 항목에 대한 설명이 충분합니다.\n",
      "   - **간결성**: 9/10 - 전반적으로 간결하지만, 일부 문장이 다소 길어질 수 있습니다.\n",
      "   - **전반적인 품질**: 10/10 - 명확하고 체계적인 구조로 잘 정리되어 있습니다.\n",
      "\n",
      "2. **[Standard Response]**\n",
      "   - **정확성**: 10/10 - 기준 정답과 일치하며, AI 의사 결정의 윤리적 우려를 잘 설명하고 있습니다.\n",
      "   - **포괄성**: 10/10 - 다양한 윤리적 우려를 포괄적으로 다루고 있으며, 각 항목에 대한 설명이 충분합니다.\n",
      "   - **간결성**: 8/10 - 정보가 풍부하지만, 일부 항목이 다소 길어질 수 있습니다.\n",
      "   - **전반적인 품질**: 9/10 - 잘 구성되어 있으나, 약간의 중복이 있을 수 있습니다.\n",
      "\n",
      "3. **[Summary Response]**\n",
      "   - **정확성**: 9/10 - 대부분의 윤리적 우려를 잘 설명하고 있으나, 일부 세부 사항이 누락되었습니다.\n",
      "   - **포괄성**: 8/10 - 여러 윤리적 우려를 다루고 있지만, Reference Answer와 Standard Response에 비해 덜 포괄적입니다.\n",
      "   - **간결성**: 9/10 - 비교적 간결하게 요약하였으나, 일부 항목이 더 간단히 설명될 수 있습니다.\n",
      "   - **전반적인 품질**: 8/10 - 전반적으로 잘 작성되었으나, 깊이가 부족합니다.\n",
      "\n",
      "4. **[Selective Response]**\n",
      "   - **정확성**: 2/10 - 질문에 대한 답변이 전혀 없으며, 정보가 부족하다고만 언급하고 있습니다.\n",
      "   - **포괄성**: 1/10 - 윤리적 우려에 대한 내용이 전혀 포함되어 있지 않습니다.\n",
      "   - **간결성**: 5/10 - 간결하지만, 정보가 전혀 없기 때문에 의미가 없습니다.\n",
      "   - **전반적인 품질**: 1/10 - 질문에 대한 답변이 없으므로 품질이 매우 낮습니다.\n",
      "\n",
      "5. **[Extraction Response]**\n",
      "   - **정확성**: 2/10 - 질문에 대한 답변이 전혀 없으며, 정보가 부족하다고만 언급하고 있습니다.\n",
      "   - **포괄성**: 1/10 - 윤리적 우려에 대한 내용이 전혀 포함되어 있지 않습니다.\n",
      "   - **간결성**: 5/10 - 간결하지만, 정보가 전혀 없기 때문에 의미가 없습니다.\n",
      "   - **전반적인 품질**: 1/10 - 질문에 대한 답변이 없으므로 품질이 매우 낮습니다.\n",
      "\n",
      "### 순위\n",
      "1. **Reference Answer** - 가장 포괄적이고 정확하며, 잘 구성된 응답입니다.\n",
      "2. **Standard Response** - Reference Answer와 유사하지만 약간의 중복이 있어 두 번째로 평가합니다.\n",
      "3. **Summary Response** - 윤리적 우려를 요약하였으나, 깊이가 부족하여 세 번째로 평가합니다.\n",
      "4. **Selective Response** - 정보가 전혀 없으므로 낮은 점수를 받았습니다.\n",
      "5. **Extraction Response** - Selective Response와 유사하게 정보가 전혀 없으므로 가장 낮은 점수를 받았습니다.\n"
     ]
    }
   ],
   "source": [
    "# AI 윤리에 관한 정보를 담고 있는 PDF 문서 경로\n",
    "pdf_path = \"dataset/AI_Understanding.pdf\" \n",
    "\n",
    "# 문서에서 관련 정보를 추출하기 위한 사용자 질의\n",
    "query = \"의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 무엇인가요?\"  \n",
    "\n",
    "# (선택사항) 평가에 사용할 참조 정답\n",
    "reference_answer = \"\"\"\n",
    "의사 결정에 AI를 사용하면 몇 가지 윤리적 문제가 제기됩니다.\n",
    "- 특히 채용, 대출, 법 집행과 같은 중요한 영역에서 AI 모델의 편향성은 불공정하거나 차별적인 결과를 초래할 수 있습니다.\n",
    "- AI 기반 의사 결정의 투명성과 설명 가능성이 부족하면 개인이 불공정한 결과에 이의를 제기하기 어렵습니다.\n",
    "- AI 시스템이 명시적인 동의 없이 방대한 양의 개인 데이터를 처리하기 때문에 개인정보 보호 위험이 발생합니다.\n",
    "- 자동화로 인한 일자리 대체 가능성은 사회적, 경제적 우려를 불러일으킵니다.\n",
    "- 또한 AI 의사결정은 소수의 대형 기술 기업에 권력이 집중되어 책임 문제가 발생할 수 있습니다.\n",
    "- AI 시스템의 공정성, 책임성, 투명성을 보장하는 것은 윤리적 배포를 위해 필수적입니다.\n",
    "\"\"\"\n",
    "\n",
    "# 다양한 압축 기법을 사용하여 평가 실행\n",
    "# 압축 방식:\n",
    "# - \"selective\": 중요 정보는 유지하고 덜 중요한 내용은 생략\n",
    "# - \"summary\": 질의에 관련된 내용을 간결하게 요약\n",
    "# - \"extraction\": 관련 문장을 문서에서 그대로 추출\n",
    "results = evaluate_compression(  \n",
    "    pdf_path=pdf_path,  \n",
    "    query=query,  \n",
    "    reference_answer=reference_answer,  \n",
    "    compression_types=[\"selective\", \"summary\", \"extraction\"]  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Compression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def visualize_compression_results(evaluation_results):\n",
    "    \"\"\"\n",
    "    다양한 압축 방식의 결과를 시각적으로 비교 출력합니다.\n",
    "\n",
    "    Args:\n",
    "        evaluation_results (Dict): evaluate_compression 함수의 실행 결과.\n",
    "    \"\"\"\n",
    "    # 질의 및 standard 방식의 청크 가져오기\n",
    "    query = evaluation_results[\"query\"]\n",
    "    standard_chunks = evaluation_results[\"standard_result\"][\"chunks\"]\n",
    "\n",
    "    print(f\"\\n질문:\\n{query}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # 비교용으로 standard 방식의 첫 번째 청크 선택\n",
    "    original_chunk = standard_chunks[0]\n",
    "\n",
    "    # 압축 방식별 비교 시각화\n",
    "    for comp_type in evaluation_results[\"compression_results\"].keys():\n",
    "        compressed_chunks = evaluation_results[\"compression_results\"][comp_type][\"compressed_chunks\"]\n",
    "        compression_ratios = evaluation_results[\"compression_results\"][comp_type][\"compression_ratios\"]\n",
    "\n",
    "        compressed_chunk = compressed_chunks[0]\n",
    "        compression_ratio = compression_ratios[0]\n",
    "\n",
    "        print(f\"\\n=== {comp_type.upper()} COMPRESSION EXAMPLE ===\\n\")\n",
    "\n",
    "        # 원본 청크 출력\n",
    "        print(\"원본 청크:\")\n",
    "        print(\"-\" * 40)\n",
    "        if len(original_chunk) > 800:\n",
    "            print(original_chunk[:800] + \"... [중략]\")\n",
    "        else:\n",
    "            print(original_chunk)\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"문자 수: {len(original_chunk)}\\n\")\n",
    "\n",
    "        # 압축 청크 출력\n",
    "        print(\"압축된 청크:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(compressed_chunk)\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"문자 수: {len(compressed_chunk)}\")\n",
    "        print(f\"해당 청크 압축률: {compression_ratio:.2f}%\\n\")\n",
    "\n",
    "        # 전체 평균 압축률 및 길이 감소 정보\n",
    "        avg_ratio = sum(compression_ratios) / len(compression_ratios)\n",
    "        print(f\"전체 평균 압축률: {avg_ratio:.2f}%\")\n",
    "        print(f\"총 문맥 길이 감소율: {evaluation_results['metrics'][comp_type]['avg_compression_ratio']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    # 압축 방식별 요약 통계 표 출력\n",
    "    print(\"\\n*** 압축 방식 요약 통계 ***\\n\")\n",
    "    print(f\"{'방식':<15} {'평균 압축률':<20} {'압축 후 길이':<18} {'원본 길이':<15}\")\n",
    "    print(\"-\" * 70)\n",
    "    for comp_type, metrics in evaluation_results[\"metrics\"].items():\n",
    "        print(f\"{comp_type:<15} {metrics['avg_compression_ratio']:<20} {metrics['total_context_length']:<18} {metrics['original_context_length']:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "질문:\n",
      "의사 결정에 AI를 사용하는 것과 관련된 윤리적 우려는 무엇인가요?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== SELECTIVE COMPRESSION EXAMPLE ===\n",
      "\n",
      "원본 청크:\n",
      "----------------------------------------\n",
      " 등에 \n",
      "사용됩니다. AI 알고리즘은 대규모 데이터 세트를 분석하여 패턴을 파악하고, 시장 동향을 \n",
      "예측하고, 금융 프로세스를 자동화할 수 있습니다. \n",
      "교통편 \n",
      "AI는 자율주행차, 교통 최적화 시스템, 물류 관리의 발전으로 교통 분야에 혁신을 일으키고 \n",
      "있습니다. 자율 주행 차량은 AI를 사용하여 주변 환경을 인식하고, 주행 결정을 내리고, \n",
      "안전하게 주행합니다. \n",
      "리테일 \n",
      "리테일 업계에서는 개인화된 추천, 재고 관리, 고객 서비스 챗봇, 공급망 최적화를 위해 \n",
      "AI를 사용합니다. AI 기반 시스템은 고객 데이터를 분석하여 수요를 예측하고, 제안을 \n",
      "개인화하며, 쇼핑 경험을 개선할 수 있습니다. \n",
      "제조 \n",
      "AI는 제조업에서 예측 유지보수, 품질 관리, 프로세스 최적화, 로봇 공학에 사용됩니다. AI \n",
      "기반 시스템은 장비를 모니터링하고, 이상 징후를 감지하고, 작업을 자동화하여 효율성을 \n",
      "높이고 비용을 절감할 수 있습니다. \n",
      "교육 \n",
      "AI는 개인화된 학습 플랫폼, 자동화된 채점 시스템, 가상 튜터를 통해 교육을 향상시키고 \n",
      "있습니다. AI 기반 도구는 학생 개개인의 필요에 맞게 조정하고 피드백을 제공하며 맞춤형 \n",
      "학습 환경을 조성할 수 있습니다. \n",
      "엔터테인먼트 \n",
      "엔터테인먼트 업계에서는 콘텐츠 추천, 게임 개발, 가상 현실 경험에 AI를 사용합니다. AI \n",
      "알고리즘은 사용자 선호도를 분석하여 영화, 음악, 게임을 추천함으로써 사용자 참여도를 \n",
      "높입니다. \n",
      "사이버 보안 \n",
      "AI는 사이버 보안에서 위협을 탐지 및 대응하고, 네트워크 트래픽을 분석하고, 취약점을 \n",
      "식별하는 데 사용됩니다. AI 기반 시스템은 보안 작업을 자... [중략]\n",
      "----------------------------------------\n",
      "문자 수: 1000\n",
      "\n",
      "압축된 청크:\n",
      "----------------------------------------\n",
      "AI의 급속한 발전과 보급은 윤리적, 사회적으로 심각한 우려를 불러일으킵니다. 이러한 우려에는 다음이 포함됩니다: \n",
      "편견과 공정성 \n",
      "AI 시스템은 데이터에 존재하는 편견을 계승하고 증폭시켜 불공정하거나 차별적인 결과를 초래\n",
      "----------------------------------------\n",
      "문자 수: 124\n",
      "해당 청크 압축률: 87.60%\n",
      "\n",
      "전체 평균 압축률: 68.27%\n",
      "총 문맥 길이 감소율: 68.27%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== SUMMARY COMPRESSION EXAMPLE ===\n",
      "\n",
      "원본 청크:\n",
      "----------------------------------------\n",
      " 등에 \n",
      "사용됩니다. AI 알고리즘은 대규모 데이터 세트를 분석하여 패턴을 파악하고, 시장 동향을 \n",
      "예측하고, 금융 프로세스를 자동화할 수 있습니다. \n",
      "교통편 \n",
      "AI는 자율주행차, 교통 최적화 시스템, 물류 관리의 발전으로 교통 분야에 혁신을 일으키고 \n",
      "있습니다. 자율 주행 차량은 AI를 사용하여 주변 환경을 인식하고, 주행 결정을 내리고, \n",
      "안전하게 주행합니다. \n",
      "리테일 \n",
      "리테일 업계에서는 개인화된 추천, 재고 관리, 고객 서비스 챗봇, 공급망 최적화를 위해 \n",
      "AI를 사용합니다. AI 기반 시스템은 고객 데이터를 분석하여 수요를 예측하고, 제안을 \n",
      "개인화하며, 쇼핑 경험을 개선할 수 있습니다. \n",
      "제조 \n",
      "AI는 제조업에서 예측 유지보수, 품질 관리, 프로세스 최적화, 로봇 공학에 사용됩니다. AI \n",
      "기반 시스템은 장비를 모니터링하고, 이상 징후를 감지하고, 작업을 자동화하여 효율성을 \n",
      "높이고 비용을 절감할 수 있습니다. \n",
      "교육 \n",
      "AI는 개인화된 학습 플랫폼, 자동화된 채점 시스템, 가상 튜터를 통해 교육을 향상시키고 \n",
      "있습니다. AI 기반 도구는 학생 개개인의 필요에 맞게 조정하고 피드백을 제공하며 맞춤형 \n",
      "학습 환경을 조성할 수 있습니다. \n",
      "엔터테인먼트 \n",
      "엔터테인먼트 업계에서는 콘텐츠 추천, 게임 개발, 가상 현실 경험에 AI를 사용합니다. AI \n",
      "알고리즘은 사용자 선호도를 분석하여 영화, 음악, 게임을 추천함으로써 사용자 참여도를 \n",
      "높입니다. \n",
      "사이버 보안 \n",
      "AI는 사이버 보안에서 위협을 탐지 및 대응하고, 네트워크 트래픽을 분석하고, 취약점을 \n",
      "식별하는 데 사용됩니다. AI 기반 시스템은 보안 작업을 자... [중략]\n",
      "----------------------------------------\n",
      "문자 수: 1000\n",
      "\n",
      "압축된 청크:\n",
      "----------------------------------------\n",
      "AI의 발전과 보급은 윤리적, 사회적 우려를 초래합니다. 주요 우려 사항 중 하나는 AI 시스템이 데이터에 존재하는 편견을 계승하고 증폭시켜 불공정하거나 차별적인 결과를 초래할 수 있다는 점입니다.\n",
      "----------------------------------------\n",
      "문자 수: 110\n",
      "해당 청크 압축률: 89.00%\n",
      "\n",
      "전체 평균 압축률: 73.79%\n",
      "총 문맥 길이 감소율: 73.79%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== EXTRACTION COMPRESSION EXAMPLE ===\n",
      "\n",
      "원본 청크:\n",
      "----------------------------------------\n",
      " 등에 \n",
      "사용됩니다. AI 알고리즘은 대규모 데이터 세트를 분석하여 패턴을 파악하고, 시장 동향을 \n",
      "예측하고, 금융 프로세스를 자동화할 수 있습니다. \n",
      "교통편 \n",
      "AI는 자율주행차, 교통 최적화 시스템, 물류 관리의 발전으로 교통 분야에 혁신을 일으키고 \n",
      "있습니다. 자율 주행 차량은 AI를 사용하여 주변 환경을 인식하고, 주행 결정을 내리고, \n",
      "안전하게 주행합니다. \n",
      "리테일 \n",
      "리테일 업계에서는 개인화된 추천, 재고 관리, 고객 서비스 챗봇, 공급망 최적화를 위해 \n",
      "AI를 사용합니다. AI 기반 시스템은 고객 데이터를 분석하여 수요를 예측하고, 제안을 \n",
      "개인화하며, 쇼핑 경험을 개선할 수 있습니다. \n",
      "제조 \n",
      "AI는 제조업에서 예측 유지보수, 품질 관리, 프로세스 최적화, 로봇 공학에 사용됩니다. AI \n",
      "기반 시스템은 장비를 모니터링하고, 이상 징후를 감지하고, 작업을 자동화하여 효율성을 \n",
      "높이고 비용을 절감할 수 있습니다. \n",
      "교육 \n",
      "AI는 개인화된 학습 플랫폼, 자동화된 채점 시스템, 가상 튜터를 통해 교육을 향상시키고 \n",
      "있습니다. AI 기반 도구는 학생 개개인의 필요에 맞게 조정하고 피드백을 제공하며 맞춤형 \n",
      "학습 환경을 조성할 수 있습니다. \n",
      "엔터테인먼트 \n",
      "엔터테인먼트 업계에서는 콘텐츠 추천, 게임 개발, 가상 현실 경험에 AI를 사용합니다. AI \n",
      "알고리즘은 사용자 선호도를 분석하여 영화, 음악, 게임을 추천함으로써 사용자 참여도를 \n",
      "높입니다. \n",
      "사이버 보안 \n",
      "AI는 사이버 보안에서 위협을 탐지 및 대응하고, 네트워크 트래픽을 분석하고, 취약점을 \n",
      "식별하는 데 사용됩니다. AI 기반 시스템은 보안 작업을 자... [중략]\n",
      "----------------------------------------\n",
      "문자 수: 1000\n",
      "\n",
      "압축된 청크:\n",
      "----------------------------------------\n",
      "AI의 급속한 발전과 보급은 윤리적, 사회적으로 심각한 우려를 불러일으킵니다. 이러한 우려에는 다음이 포함됩니다:  \n",
      "AI 시스템은 데이터에 존재하는 편견을 계승하고 증폭시켜 불공정하거나 차별적인 결과를 초래\n",
      "----------------------------------------\n",
      "문자 수: 116\n",
      "해당 청크 압축률: 88.40%\n",
      "\n",
      "전체 평균 압축률: 74.79%\n",
      "총 문맥 길이 감소율: 74.79%\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "*** 압축 방식 요약 통계 ***\n",
      "\n",
      "방식              평균 압축률               압축 후 길이            원본 길이          \n",
      "----------------------------------------------------------------------\n",
      "selective       68.27%               3191               10018          \n",
      "summary         73.79%               2639               10018          \n",
      "extraction      74.79%               2539               10018          \n"
     ]
    }
   ],
   "source": [
    "# 결과 보기\n",
    "visualize_compression_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
