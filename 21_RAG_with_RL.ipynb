{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Simple RAG with RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 디렉토리에서 문서들을 불러오는 함수\n",
    "def load_documents(directory_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    지정된 디렉토리에서 모든 텍스트 문서를 불러옵니다.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): 텍스트 파일들이 있는 디렉토리 경로\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 각 텍스트 파일의 내용을 문자열로 담은 리스트\n",
    "    \"\"\"\n",
    "    documents = []  # 문서 내용을 저장할 빈 리스트를 초기화합니다.\n",
    "    for filename in os.listdir(directory_path):  # 디렉토리 내 모든 파일을 반복합니다.\n",
    "        if filename.endswith(\".txt\"):  # 확장자가 .txt인 경우에만 처리합니다.\n",
    "            # UTF-8 인코딩으로 파일을 읽고 그 내용을 리스트에 추가합니다.\n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents  # 문서 내용이 담긴 리스트를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 문서들을 일정한 크기의 덩어리로 나누는 함수\n",
    "def split_into_chunks(documents: List[str], chunk_size: int = 30) -> List[str]:\n",
    "    \"\"\"\n",
    "    문서들을 지정된 단어 수만큼의 작은 덩어리로 나눕니다.\n",
    "\n",
    "    Args:\n",
    "        documents (List[str]): 나눌 대상이 되는 문서 문자열들의 리스트\n",
    "        chunk_size (int): 각 덩어리에 포함될 최대 단어 수 (기본값은 30)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 각 덩어리를 문자열로 담은 리스트. 각 문자열은 최대 `chunk_size`개의 단어를 포함합니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 덩어리를 저장할 빈 리스트를 초기화합니다.\n",
    "    for doc in documents:  # 각 문서를 반복하면서 처리합니다.\n",
    "        words = doc.split()  # 문서를 단어 단위로 나눕니다.\n",
    "        # 지정된 크기만큼 단어들을 묶어 덩어리를 만듭니다.\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i:i + chunk_size])  # 단어들을 공백으로 연결하여 하나의 문자열로 만듭니다.\n",
    "            chunks.append(chunk)  # 만든 덩어리를 리스트에 추가합니다.\n",
    "    return chunks  # 덩어리 리스트를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 텍스트를 전처리하는 함수 (예: 소문자화, 특수문자 제거 등)\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    입력된 텍스트를 소문자로 변환하고 특수문자를 제거하여 전처리합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 전처리할 입력 텍스트\n",
    "\n",
    "    Returns:\n",
    "        str: 영숫자(알파벳 및 숫자)와 공백만 남긴 전처리된 텍스트\n",
    "    \"\"\"\n",
    "    # 텍스트를 소문자로 변환합니다.\n",
    "    text = text.lower()\n",
    "    # 특수문자를 제거하고 영숫자와 공백만 남깁니다.\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모든 텍스트 덩어리에 전처리를 적용하는 함수\n",
    "def preprocess_chunks(chunks: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    모든 텍스트 덩어리에 전처리 작업을 적용합니다.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): 전처리할 텍스트 덩어리들의 리스트\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 전처리된 텍스트 덩어리들의 리스트\n",
    "    \"\"\"\n",
    "    # 리스트의 각 덩어리에 preprocess_text 함수를 적용합니다.\n",
    "    return [preprocess_text(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 텍스트 파일이 들어 있는 디렉토리 경로를 지정합니다.\n",
    "directory_path = \"dataset\"\n",
    "\n",
    "# 지정된 디렉토리에서 모든 텍스트 문서를 불러옵니다.\n",
    "documents = load_documents(directory_path)\n",
    "\n",
    "# 불러온 문서들을 작은 텍스트 덩어리로 나눕니다.\n",
    "chunks = split_into_chunks(documents)\n",
    "\n",
    "# 각 덩어리에 대해 전처리 작업을 수행합니다 (예: 소문자화, 특수문자 제거).\n",
    "preprocessed_chunks = preprocess_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: 양자 컴퓨팅 원리 발전 가능성 소개 양자 컴퓨팅은 전자 컴퓨터의 출현 이후 컴퓨팅 이론에서 ... \n",
      "Chunk 2: 현상을 활용하여 양자 비트qubit로 정보를 처리합니다 이러한 근본적인 차이로 인해 양자  ... \n"
     ]
    }
   ],
   "source": [
    "# 전처리된 덩어리 중 처음 2개를 출력합니다. 각 덩어리는 처음 200자까지만 표시됩니다.\n",
    "for i in range(2):\n",
    "    # 슬라이싱을 사용하여 출력 길이를 200자로 제한합니다.\n",
    "    print(f\"Chunk {i+1}: {preprocessed_chunks[i][:50]} ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 텍스트 덩어리 한 배치(batch)에 대해 임베딩을 생성하는 함수\n",
    "def generate_embeddings_batch(chunks_batch: List[str], model: str = \"text-embedding-3-small\") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    OpenAI 클라이언트를 사용하여 텍스트 덩어리 배치에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        chunks_batch (List[str]): 임베딩을 생성할 텍스트 덩어리 배치\n",
    "        model (str): 임베딩 생성을 위한 모델 이름 (기본값: \"BAAI/bge-en-icl\")\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: 각 텍스트 덩어리에 대한 임베딩 리스트 (float 값들의 리스트)\n",
    "    \"\"\"\n",
    "    # OpenAI 클라이언트를 사용하여 입력된 배치에 대한 임베딩을 생성합니다.\n",
    "    response = client.embeddings.create(\n",
    "        model=model,  # 사용할 임베딩 모델을 지정합니다.\n",
    "        input=chunks_batch  # 텍스트 덩어리 배치를 입력으로 제공합니다.\n",
    "    )\n",
    "    # 응답에서 임베딩을 추출하여 반환합니다.\n",
    "    embeddings = [item.embedding for item in response.data]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 모든 텍스트 덩어리에 대해 배치 단위로 임베딩을 생성하는 함수\n",
    "def generate_embeddings(chunks: List[str], batch_size: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    모든 텍스트 덩어리에 대해 배치 단위로 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): 임베딩을 생성할 텍스트 덩어리 리스트\n",
    "        batch_size (int): 한 배치당 처리할 텍스트 덩어리 수 (기본값: 10)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모든 덩어리에 대한 임베딩을 담고 있는 NumPy 배열\n",
    "    \"\"\"\n",
    "    all_embeddings = []  # 모든 임베딩을 저장할 빈 리스트를 초기화합니다.\n",
    "\n",
    "    # 지정된 배치 크기만큼 반복하면서 임베딩을 생성합니다.\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        # 현재 배치에 해당하는 덩어리들을 추출합니다.\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        # 해당 배치에 대해 임베딩을 생성합니다.\n",
    "        embeddings = generate_embeddings_batch(batch)\n",
    "        # 생성된 임베딩을 전체 리스트에 추가합니다.\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    # 전체 임베딩 리스트를 NumPy 배열로 변환하여 반환합니다.\n",
    "    return np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 임베딩을 파일로 저장하는 함수\n",
    "def save_embeddings(embeddings: np.ndarray, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    임베딩을 JSON 파일로 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): 저장할 임베딩이 담긴 NumPy 배열\n",
    "        output_file (str): 임베딩을 저장할 JSON 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 지정된 파일을 UTF-8 인코딩으로 쓰기 모드로 엽니다.\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        # NumPy 배열을 리스트로 변환한 후 JSON 형식으로 저장합니다.\n",
    "        json.dump(embeddings.tolist(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 임베딩 생성을 위해 텍스트 덩어리를 전처리합니다.\n",
    "preprocessed_chunks = preprocess_chunks(chunks)\n",
    "\n",
    "# 전처리된 덩어리들에 대해 임베딩을 생성합니다.\n",
    "embeddings = generate_embeddings(preprocessed_chunks)\n",
    "\n",
    "# 생성된 임베딩을 \"embeddings.json\"이라는 JSON 파일로 저장합니다.\n",
    "save_embeddings(embeddings, \"dataset/embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# 메모리 내 벡터 저장소를 딕셔너리 형태로 초기화합니다.\n",
    "# 키는 고유한 정수값이며, 값은 임베딩과 해당 텍스트 덩어리를 담는 딕셔너리입니다.\n",
    "vector_store: dict[int, dict[str, object]] = {}\n",
    "\n",
    "# 임베딩과 해당하는 텍스트 덩어리를 벡터 저장소에 추가하는 함수\n",
    "def add_to_vector_store(embeddings: np.ndarray, chunks: List[str]) -> None:\n",
    "    \"\"\"\n",
    "    임베딩과 해당하는 텍스트 덩어리를 벡터 저장소에 추가합니다.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): 추가할 임베딩이 담긴 NumPy 배열\n",
    "        chunks (List[str]): 임베딩에 대응되는 텍스트 덩어리 리스트\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 임베딩과 텍스트 덩어리를 동시에 반복하며 저장소에 추가합니다.\n",
    "    for embedding, chunk in zip(embeddings, chunks):\n",
    "        # 저장소의 현재 길이를 고유 키로 사용하여 항목을 추가합니다.\n",
    "        vector_store[len(vector_store)] = {\n",
    "            \"embedding\": embedding,\n",
    "            \"chunk\": chunk\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 두 벡터 간의 코사인 유사도를 계산하는 함수\n",
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    두 벡터 간의 코사인 유사도를 계산합니다.\n",
    "\n",
    "    Args:\n",
    "        vec1 (np.ndarray): 첫 번째 벡터\n",
    "        vec2 (np.ndarray): 두 번째 벡터\n",
    "\n",
    "    Returns:\n",
    "        float: 두 벡터 사이의 코사인 유사도 (-1에서 1 사이의 값)\n",
    "    \"\"\"\n",
    "    # 두 벡터의 내적을 계산합니다.\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    # 첫 번째 벡터의 크기(노름)를 계산합니다.\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    # 두 번째 벡터의 크기(노름)를 계산합니다.\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    # 내적을 두 벡터의 노름 곱으로 나누어 코사인 유사도를 반환합니다.\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# 벡터 저장소에서 코사인 유사도를 기반으로 유사한 텍스트 덩어리를 검색하는 함수\n",
    "def similarity_search(query_embedding: np.ndarray, top_k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    벡터 저장소에서 유사도 검색을 수행하고, 가장 유사한 텍스트 덩어리 top_k개를 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        query_embedding (np.ndarray): 검색어에 대한 임베딩 벡터\n",
    "        top_k (int): 반환할 가장 유사한 텍스트 덩어리의 개수 (기본값: 5)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 유사도가 높은 순서대로 정렬된 텍스트 덩어리 리스트 (top_k개)\n",
    "    \"\"\"\n",
    "    similarities = []  # 유사도 점수와 해당 키를 저장할 리스트를 초기화합니다.\n",
    "\n",
    "    # 벡터 저장소의 모든 항목을 반복하면서 유사도를 계산합니다.\n",
    "    for key, value in vector_store.items():\n",
    "        # 검색 임베딩과 저장된 임베딩 간의 코사인 유사도를 계산합니다.\n",
    "        similarity = cosine_similarity(query_embedding, value[\"embedding\"])\n",
    "        # 키와 유사도를 튜플로 리스트에 추가합니다.\n",
    "        similarities.append((key, similarity))\n",
    "\n",
    "    # 유사도 점수를 기준으로 내림차순 정렬합니다.\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 유사도가 높은 순서대로 top_k개의 텍스트 덩어리를 반환합니다.\n",
    "    return [vector_store[key][\"chunk\"] for key, _ in similarities[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 주어진 질의(query)에 대해 관련 있는 문서 덩어리를 검색하는 함수\n",
    "def retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> List[str]:\n",
    "    \"\"\"\n",
    "    주어진 질의 텍스트에 대해 가장 관련성 높은 문서 덩어리들을 검색합니다.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): 관련 문서를 찾기 위한 질의 텍스트\n",
    "        top_k (int): 반환할 관련 텍스트 덩어리 개수 (기본값: 5)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 관련도 순으로 정렬된 상위 top_k개의 텍스트 덩어리 리스트\n",
    "    \"\"\"\n",
    "    # 질의 텍스트에 대한 임베딩을 생성합니다.\n",
    "    query_embedding = generate_embeddings([query_text])[0]\n",
    "    \n",
    "    # 생성된 임베딩을 기반으로 유사한 문서 덩어리를 검색합니다.\n",
    "    relevant_chunks = similarity_search(query_embedding, top_k=top_k)\n",
    "    \n",
    "    # 관련 텍스트 덩어리들을 반환합니다.\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: 양자 컴퓨팅 원리 발전 가능성 소개 양자 컴퓨팅은 전자 컴퓨터의 출현 이후 컴퓨팅 이론에서 ... \n",
      "Chunk 2: 현상을 활용하여 양자 비트qubit로 정보를 처리합니다 이러한 근본적인 차이로 인해 양자  ... \n",
      "Chunk 3: 양자 컴퓨터는 여러 가능성을 동시에 처리할 수 있어 계산상의 이점을 얻을 수 있습니다 얽힘 ... \n",
      "Chunk 4: 컴퓨팅은 혁신적인 계산 패러다임이자 우리 시대의 가장 야심 찬 공학 프로젝트 중 하나입니다 ... \n",
      "Chunk 5: 기초 중첩 양자 컴퓨팅의 핵심은 중첩 원리에 있습니다 기존의 비트bit는 0 또는 1 중  ... \n"
     ]
    }
   ],
   "source": [
    "# 생성된 임베딩과 해당 전처리된 텍스트 덩어리를 벡터 저장소에 추가합니다.\n",
    "add_to_vector_store(embeddings, preprocessed_chunks)\n",
    "\n",
    "# 관련 문서 덩어리를 검색할 질의 텍스트를 정의합니다.\n",
    "query_text = \"양자 컴퓨팅(Quantum Computing)이란 무엇인가요?\"\n",
    "\n",
    "# 질의 텍스트에 대해 가장 관련 있는 텍스트 덩어리들을 벡터 저장소에서 검색합니다.\n",
    "relevant_chunks = retrieve_relevant_chunks(query_text)\n",
    "\n",
    "# 검색된 관련 텍스트 덩어리들을 출력합니다 (각 덩어리의 앞 50자만 표시).\n",
    "for idx, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"Chunk {idx + 1}: {chunk[:50]} ... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 질의에 대한 문맥 정보를 포함한 프롬프트를 생성하는 함수\n",
    "def construct_prompt(query: str, context_chunks: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    검색된 문맥 정보와 함께 질의에 대한 프롬프트를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 프롬프트를 구성할 질의 텍스트\n",
    "        context_chunks (List[str]): 프롬프트에 포함할 관련 문맥 덩어리 리스트\n",
    "\n",
    "    Returns:\n",
    "        str: LLM에 입력할 최종 프롬프트 문자열\n",
    "    \"\"\"\n",
    "    # 문맥 덩어리들을 개행 문자로 구분하여 하나의 문자열로 합칩니다.\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    \n",
    "    # LLM의 응답 방향을 안내하는 시스템 메시지를 정의합니다.\n",
    "    system_message = (\n",
    "        \"당신은 유용한 AI 어시스턴트입니다. 반드시 제공된 문맥(context) 정보만 활용하여 질문에 답하십시오. \"\n",
    "        \"문맥에 필요한 정보가 없을 경우 '이 질문에 답하기에 충분한 정보가 없습니다.'라고 답하십시오.\"\n",
    "    )\n",
    "    \n",
    "    # 시스템 메시지, 문맥, 질의를 조합하여 최종 프롬프트를 구성합니다.\n",
    "    prompt = f\"System: {system_message}\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 프롬프트를 기반으로 OpenAI 호환 채팅 모델의 응답을 생성하는 함수\n",
    "def generate_response(\n",
    "    prompt: str,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    max_tokens: int = 512,\n",
    "    temperature: float = 1,\n",
    "    top_p: float = 0.9,\n",
    "    #top_k: int = 50\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    구성된 프롬프트를 기반으로 OpenAI 채팅 모델에서 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): 채팅 모델에 전달할 입력 프롬프트\n",
    "        model (str): 사용할 언어 모델 이름 (기본값: \"google/gemma-2-2b-it\")\n",
    "        max_tokens (int): 생성할 응답의 최대 토큰 수 (기본값: 512)\n",
    "        temperature (float): 샘플링 온도, 값이 높을수록 다양성이 증가함 (기본값: 1)\n",
    "        top_p (float): 누적 확률 기반 샘플링의 확률 한계 (기본값: 0.9)\n",
    "        top_k (int): 확률이 높은 상위 K개의 토큰만 고려 (기본값: 50)\n",
    "\n",
    "    Returns:\n",
    "        str: 언어 모델이 생성한 응답 문자열\n",
    "    \"\"\"\n",
    "    # 클라이언트를 사용하여 채팅 응답을 생성합니다.\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,  # 사용할 모델 지정\n",
    "        max_tokens=max_tokens,  # 응답에서 생성할 최대 토큰 수\n",
    "        temperature=temperature,  # 다양성을 위한 샘플링 온도\n",
    "        top_p=top_p,  # 누적 확률 상한 (nucleus sampling)\n",
    "        extra_body={  # 요청 본문에 추가할 항목\n",
    "            #\"top_k\": top_k  # 상위 확률 토큰 개수 제한\n",
    "        },\n",
    "        messages=[  # 채팅 히스토리 형태의 입력 메시지 목록\n",
    "            {\n",
    "                \"role\": \"user\",  # 사용자 역할로 메시지를 전달\n",
    "                \"content\": [  # 메시지 콘텐츠 (멀티모달도 가능)\n",
    "                    {\n",
    "                        \"type\": \"text\",  # 텍스트 유형 메시지\n",
    "                        \"text\": prompt  # 실제 전달할 프롬프트 텍스트\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # 첫 번째 생성 응답의 텍스트만 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 기본 RAG(Retrieval-Augmented Generation) 파이프라인을 수행하는 함수\n",
    "def basic_rag_pipeline(query: str) -> str:\n",
    "    \"\"\"\n",
    "    기본 RAG(Retrieval-Augmented Generation) 파이프라인을 수행합니다:\n",
    "    관련 문서 검색 → 프롬프트 구성 → LLM 응답 생성\n",
    "\n",
    "    Args:\n",
    "        query (str): 응답을 생성할 사용자 질의\n",
    "\n",
    "    Returns:\n",
    "        str: 검색된 문맥을 기반으로 LLM이 생성한 응답\n",
    "    \"\"\"\n",
    "    # 1단계: 주어진 질의에 대해 가장 관련 있는 텍스트 덩어리들을 검색합니다.\n",
    "    relevant_chunks: List[str] = retrieve_relevant_chunks(query)\n",
    "    \n",
    "    # 2단계: 검색된 문맥과 질의를 바탕으로 프롬프트를 구성합니다.\n",
    "    prompt: str = construct_prompt(query, relevant_chunks)\n",
    "    \n",
    "    # 3단계: 구성된 프롬프트를 기반으로 LLM의 응답을 생성합니다.\n",
    "    response: str = generate_response(prompt)\n",
    "    \n",
    "    # 생성된 응답을 반환합니다.\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the basic RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 질의: 큐비트가 중첩 상태에 있을 때의 수학적 표현은 무엇인가?\n",
      "\n",
      "기대 정답: |ψ⟩ = α|0⟩ + β|1⟩으로, α와 β는 복소수이며 |α|² + |β|² = 1을 만족하여, 각각 상태 |0⟩ 또는 |1⟩로 측정될 확률 진폭을 나타낸다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터 파일을 읽기 모드로 열고, 내용을 딕셔너리로 불러옵니다.\n",
    "with open('dataset/validation_rl.json', 'r') as file:\n",
    "    validation_data = json.load(file)\n",
    "\n",
    "# 기본 RAG 파이프라인을 테스트할 샘플 질의를 추출합니다.\n",
    "sample_query = validation_data['basic_factual_questions'][0]['question']  # 질의 텍스트 추출\n",
    "expected_answer = validation_data['basic_factual_questions'][0]['answer']  # 정답(기대 응답) 추출\n",
    "\n",
    "# 샘플 질의와 기대 정답을 출력합니다.\n",
    "print(f\"샘플 질의: {sample_query}\\n\")\n",
    "print(f\"기대 정답: {expected_answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG(Retrieval-Augmented Generation) 파이프라인을 실행합니다...\n",
      "질의: 큐비트가 중첩 상태에 있을 때의 수학적 표현은 무엇인가?\n",
      "\n",
      "AI 응답:\n",
      "------------------------------\n",
      "큐비트가 중첩 상태에 있을 때의 수학적 표현은 다음과 같습니다: ψ = α|0⟩ + β|1⟩, 여기서 α와 β는 복소수로 각각 상태 0 또는 1의 큐비트를 측정할 때의 확률 진폭을 나타내며, α² + β² = 1을 만족합니다.\n",
      "--------------------------------------------------\n",
      "기대 정답:\n",
      "------------------------------\n",
      "|ψ⟩ = α|0⟩ + β|1⟩으로, α와 β는 복소수이며 |α|² + |β|² = 1을 만족하여, 각각 상태 |0⟩ 또는 |1⟩로 측정될 확률 진폭을 나타낸다.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RAG 파이프라인 실행 시작 메시지를 출력합니다.\n",
    "print(\"RAG(Retrieval-Augmented Generation) 파이프라인을 실행합니다...\")\n",
    "print(f\"질의: {sample_query}\\n\")\n",
    "\n",
    "# RAG 파이프라인을 실행하고 모델 응답을 받아옵니다.\n",
    "response = basic_rag_pipeline(sample_query)\n",
    "\n",
    "# 모델 응답을 보기 좋게 출력합니다.\n",
    "print(\"AI 응답:\")\n",
    "print(\"-\" * 30)\n",
    "print(response.strip())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 비교를 위해 정답(ground truth)을 출력합니다.\n",
    "print(\"기대 정답:\")\n",
    "print(\"-\" * 30)\n",
    "print(expected_answer)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple RAG pipeline doesn't seem to perform well in its current state. The generated response is not only irrelevant to the ground truth but also misses critical information.\n",
    "\n",
    "But don't worry! In the upcoming steps, we will implement a Reinforcement Learning-based RAG pipeline to address these shortcomings. This will help us improve the retrieval and generation process, making the responses more accurate and contextually relevant.\n",
    "\n",
    "Stay tuned as we take our RAG pipeline to the next level!\n",
    "\n",
    "단순한 RAG 파이프라인은 현재 상태에서는 제대로 작동하지 않는 것 같습니다. 생성된 응답은 실체적 진실과 관련이 없을 뿐만 아니라 중요한 정보를 누락하기도 합니다.\n",
    "\n",
    "하지만 걱정하지 마세요! 다음 단계에서는 이러한 단점을 해결하기 위해 강화 학습 기반 RAG 파이프라인을 구현할 예정입니다. 이를 통해 검색 및 생성 프로세스를 개선하여 보다 정확하고 맥락에 맞는 응답을 제공할 수 있을 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "# Reinforcement Learning for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 강화 학습을 위한 state 표현을 정의하는 함수\n",
    "def define_state(\n",
    "    query: str, \n",
    "    context_chunks: List[str], \n",
    "    rewritten_query: str = None, \n",
    "    previous_responses: List[str] = None, \n",
    "    previous_rewards: List[float] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    강화 학습 에이전트를 위한 state 표현을 정의합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자의 원래 질의\n",
    "        context_chunks (List[str]): Knowledge base에서 검색된 context 덩어리\n",
    "        rewritten_query (str, optional): 원래 질의를 재구성한 쿼리 (선택 사항)\n",
    "        previous_responses (List[str], optional): 이전에 생성된 응답들의 리스트\n",
    "        previous_rewards (List[float], optional): 이전 행동에 대해 받은 reward들의 리스트\n",
    "\n",
    "    Returns:\n",
    "        dict: 현재 state를 나타내는 딕셔너리 (강화 학습 에이전트의 입력으로 사용됨)\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"original_query\": query,  # 사용자가 입력한 원래 쿼리\n",
    "        \"current_query\": rewritten_query if rewritten_query else query,  # 현재 사용 중인 쿼리 (재작성 가능)\n",
    "        \"context\": context_chunks,  # Knowledge base에서 검색된 문맥 정보\n",
    "        \"previous_responses\": previous_responses if previous_responses else [],  # 이전에 생성된 응답 히스토리\n",
    "        \"previous_rewards\": previous_rewards if previous_rewards else []  # 각 응답에 대해 받은 보상 히스토리\n",
    "    }\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# 강화 학습 에이전트가 선택할 수 있는 행동(action) 공간을 정의하는 함수\n",
    "def define_action_space() -> List[str]:\n",
    "    \"\"\"\n",
    "    강화 학습 에이전트가 수행할 수 있는 가능한 행동들을 정의합니다.\n",
    "\n",
    "    포함되는 행동은 다음과 같습니다:\n",
    "    - rewrite_query: 검색 성능 향상을 위해 원래 쿼리를 재작성\n",
    "    - expand_context: 추가적인 문맥(context) 정보를 검색\n",
    "    - filter_context: 관련 없는 문맥 정보를 제거\n",
    "    - generate_response: 현재 쿼리와 문맥을 기반으로 응답 생성\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 사용 가능한 행동들의 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    # 에이전트가 수행할 수 있는 행동들을 정의합니다.\n",
    "    actions = [\"rewrite_query\", \"expand_context\", \"filter_context\", \"generate_response\"]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 응답 품질을 기반으로 보상(reward)을 계산하는 함수\n",
    "def calculate_reward(response: str, ground_truth: str) -> float:\n",
    "    \"\"\"\n",
    "    생성된 응답과 정답(ground truth)을 비교하여 보상 값을 계산합니다.\n",
    "\n",
    "    응답과 정답의 임베딩 간 코사인 유사도를 사용하여\n",
    "    응답이 얼마나 정확한지를 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        response (str): RAG 파이프라인에서 생성된 응답\n",
    "        ground_truth (str): 기대되는 정답 (정답 기준)\n",
    "\n",
    "    Returns:\n",
    "        float: -1과 1 사이의 보상 값. 값이 클수록 정답과 유사한 응답임을 의미합니다.\n",
    "    \"\"\"\n",
    "    # 응답과 정답 각각에 대해 임베딩을 생성합니다.\n",
    "    response_embedding = generate_embeddings([response])[0]\n",
    "    ground_truth_embedding = generate_embeddings([ground_truth])[0]\n",
    "    \n",
    "    # 두 임베딩 간의 코사인 유사도를 계산하여 보상으로 사용합니다.\n",
    "    similarity = cosine_similarity(response_embedding, ground_truth_embedding)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Function Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 더 나은 문서 검색을 위해 쿼리를 재작성하는 함수\n",
    "def rewrite_query(\n",
    "    query: str, \n",
    "    context_chunks: List[str], \n",
    "    model: str = \"gpt-4o-mini\", \n",
    "    max_tokens: int = 100, \n",
    "    temperature: float = 0.3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    더 나은 문서 검색을 위해 LLM을 사용하여 쿼리를 재작성합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자의 원래 질의 텍스트\n",
    "        context_chunks (List[str]): 현재까지 검색된 문맥 덩어리 리스트\n",
    "        model (str): 쿼리 재작성을 위한 LLM 모델\n",
    "        max_tokens (int): 재작성된 쿼리의 최대 토큰 수 (기본값: 100)\n",
    "        temperature (float): 응답 다양성을 위한 샘플링 온도 (기본값: 0.3)\n",
    "\n",
    "    Returns:\n",
    "        str: 문서 검색 최적화를 위해 재작성된 쿼리\n",
    "    \"\"\"\n",
    "    # LLM에 전달할 쿼리 재작성 프롬프트 구성\n",
    "    rewrite_prompt = f\"\"\"\n",
    "    당신은 쿼리 최적화 어시스턴트입니다. 주어진 질의를 더 효과적으로 재작성하여\n",
    "    관련 있는 정보를 더 잘 검색할 수 있도록 도와주는 역할을 합니다.\n",
    "    이 쿼리는 문서 검색에 사용될 예정입니다.\n",
    "\n",
    "    원래 질의: {query}\n",
    "\n",
    "    지금까지 검색된 문맥 기반:\n",
    "    {' '.join(context_chunks[:2]) if context_chunks else '아직 문맥이 제공되지 않았습니다.'}\n",
    "\n",
    "    더 구체적이고 정확한 정보를 검색할 수 있도록 질의를 재작성하세요.\n",
    "    재작성된 질의:\n",
    "    \"\"\"\n",
    "\n",
    "    # LLM을 호출하여 재작성된 쿼리를 생성합니다.\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,  # 사용할 모델 지정\n",
    "        max_tokens=max_tokens,  # 최대 토큰 수 제한\n",
    "        temperature=temperature,  # 다양성을 위한 샘플링 온도\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": rewrite_prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 생성된 응답에서 재작성된 쿼리를 추출하여 반환\n",
    "    rewritten_query = response.choices[0].message.content.strip()\n",
    "    return rewritten_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 추가적인 문맥 덩어리를 검색하여 context를 확장하는 함수\n",
    "def expand_context(query: str, current_chunks: List[str], top_k: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    문맥(context)을 확장하기 위해 추가적인 텍스트 덩어리(chunks)를 검색합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 문맥을 확장하고자 하는 질의 텍스트\n",
    "        current_chunks (List[str]): 현재까지 확보된 문맥 덩어리 리스트\n",
    "        top_k (int): 새롭게 추가할 문맥 덩어리의 최대 개수 (기본값: 3)\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 기존 문맥에 새로 추가된 문맥을 포함한 확장된 context 리스트\n",
    "    \"\"\"\n",
    "    # 현재 확보된 문맥 수보다 많은 수를 요청하여 추가 문맥을 검색합니다.\n",
    "    additional_chunks = retrieve_relevant_chunks(query, top_k=top_k + len(current_chunks))\n",
    "    \n",
    "    # 이미 포함된 문맥은 제외하고 새로운 문맥만 추려냅니다.\n",
    "    new_chunks = []\n",
    "    for chunk in additional_chunks:\n",
    "        if chunk not in current_chunks:\n",
    "            new_chunks.append(chunk)\n",
    "    \n",
    "    # 새로운 문맥 중 최대 top_k개만 추가하여 context를 확장합니다.\n",
    "    expanded_context = current_chunks + new_chunks[:top_k]\n",
    "    return expanded_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 가장 관련성 높은 문맥 덩어리만 남기도록 context를 필터링하는 함수\n",
    "def filter_context(query: str, context_chunks: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    context에서 가장 관련성 높은 덩어리들만 필터링하여 유지합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 관련성을 계산할 기준이 되는 질의 텍스트\n",
    "        context_chunks (List[str]): 필터링할 문맥 덩어리 리스트\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 관련성 높은 문맥만 남긴 필터링된 context 리스트\n",
    "    \"\"\"\n",
    "    if not context_chunks:\n",
    "        return []\n",
    "        \n",
    "    # 질의 및 각 문맥 덩어리에 대해 임베딩을 생성합니다.\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    chunk_embeddings = [generate_embeddings([chunk])[0] for chunk in context_chunks]\n",
    "    \n",
    "    # 각 문맥 덩어리에 대해 질의와의 유사도 점수(관련성)를 계산합니다.\n",
    "    relevance_scores = []\n",
    "    for chunk_embedding in chunk_embeddings:\n",
    "        score = cosine_similarity(query_embedding, chunk_embedding)\n",
    "        relevance_scores.append(score)\n",
    "    \n",
    "    # 유사도 점수를 기준으로 문맥을 내림차순 정렬합니다.\n",
    "    sorted_chunks = [x for _, x in sorted(zip(relevance_scores, context_chunks), reverse=True)]\n",
    "    \n",
    "    # 최대 5개의 가장 관련성 높은 문맥만 유지합니다 (필요 시 더 적게 유지 가능)\n",
    "    filtered_chunks = sorted_chunks[:min(5, len(sorted_chunks))]\n",
    "    \n",
    "    return filtered_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 현재 상태에 따라 행동을 선택하는 정책 네트워크 함수 (epsilon-greedy 전략 사용)\n",
    "def policy_network(\n",
    "    state: dict, \n",
    "    action_space: List[str], \n",
    "    epsilon: float = 0.2\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    현재 state를 기반으로 epsilon-greedy 전략을 사용하여 행동(action)을 선택하는 정책 네트워크를 정의합니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 환경의 상태. 쿼리, 문맥, 응답 히스토리, 보상 등을 포함합니다.\n",
    "        action_space (List[str]): 에이전트가 선택할 수 있는 행동 목록\n",
    "        epsilon (float): 무작위 탐색(exploration)을 수행할 확률 (기본값: 0.2)\n",
    "\n",
    "    Returns:\n",
    "        str: 선택된 행동 (action_space 내의 항목 중 하나)\n",
    "    \"\"\"\n",
    "    # epsilon-greedy 전략: 무작위 탐색 vs. 현재 상태 기반 결정\n",
    "    if np.random.random() < epsilon:\n",
    "        # 탐색: 무작위로 행동을 선택\n",
    "        action = np.random.choice(action_space)\n",
    "    else:\n",
    "        # 활용: 현재 상태를 기반으로 간단한 휴리스틱에 따라 최적의 행동 선택\n",
    "\n",
    "        # 이전 응답이 전혀 없다면, 쿼리를 먼저 재작성\n",
    "        if len(state[\"previous_responses\"]) == 0:\n",
    "            action = \"rewrite_query\"\n",
    "        # 응답은 있었지만 보상이 낮다면 문맥을 확장\n",
    "        elif state[\"previous_rewards\"] and max(state[\"previous_rewards\"]) < 0.7:\n",
    "            action = \"expand_context\"\n",
    "        # 문맥이 너무 많다면 필터링 시도\n",
    "        elif len(state[\"context\"]) > 5:\n",
    "            action = \"filter_context\"\n",
    "        # 그 외의 경우에는 응답 생성\n",
    "        else:\n",
    "            action = \"generate_response\"\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single RL Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 한 번의 강화 학습 단계를 수행하는 함수\n",
    "def rl_step(\n",
    "    state: dict, \n",
    "    action_space: List[str], \n",
    "    ground_truth: str\n",
    ") -> tuple[dict, str, float, str]:\n",
    "    \"\"\"\n",
    "    한 번의 강화 학습 단계를 수행합니다: 행동 선택, 실행, 보상 계산까지 포함됩니다.\n",
    "\n",
    "    Args:\n",
    "        state (dict): 현재 환경의 상태 (쿼리, 문맥, 응답 기록, 보상 등 포함)\n",
    "        action_space (List[str]): 에이전트가 선택할 수 있는 행동 목록\n",
    "        ground_truth (str): 정답 응답 (보상 계산을 위한 기준)\n",
    "\n",
    "    Returns:\n",
    "        tuple: 다음 항목으로 구성된 튜플을 반환합니다:\n",
    "            - state (dict): 행동 실행 후 업데이트된 상태\n",
    "            - action (str): 선택된 행동\n",
    "            - reward (float): 해당 행동으로 얻은 보상 값\n",
    "            - response (str): 생성된 응답 (해당되는 경우)\n",
    "    \"\"\"\n",
    "    # 정책 네트워크를 통해 행동 선택\n",
    "    action: str = policy_network(state, action_space)\n",
    "    response: str = None  # 생성된 응답 초기화\n",
    "    reward: float = 0  # 보상 초기화\n",
    "\n",
    "    # 선택된 행동을 실행합니다.\n",
    "    if action == \"rewrite_query\":\n",
    "        # 쿼리를 재작성하여 검색 품질 개선\n",
    "        rewritten_query: str = rewrite_query(state[\"original_query\"], state[\"context\"])\n",
    "        state[\"current_query\"] = rewritten_query  # 상태 내 현재 쿼리 업데이트\n",
    "        # 재작성된 쿼리로 새 context 검색\n",
    "        new_context: List[str] = retrieve_relevant_chunks(rewritten_query)\n",
    "        state[\"context\"] = new_context  # context 업데이트\n",
    "\n",
    "    elif action == \"expand_context\":\n",
    "        # context 확장 (추가 문맥 검색)\n",
    "        expanded_context: List[str] = expand_context(state[\"current_query\"], state[\"context\"])\n",
    "        state[\"context\"] = expanded_context  # context 업데이트\n",
    "\n",
    "    elif action == \"filter_context\":\n",
    "        # 관련 없는 문맥을 제거하여 필터링\n",
    "        filtered_context: List[str] = filter_context(state[\"current_query\"], state[\"context\"])\n",
    "        state[\"context\"] = filtered_context  # context 업데이트\n",
    "\n",
    "    elif action == \"generate_response\":\n",
    "        # 현재 쿼리와 문맥으로 프롬프트 구성\n",
    "        prompt: str = construct_prompt(state[\"current_query\"], state[\"context\"])\n",
    "        # LLM을 통해 응답 생성\n",
    "        response: str = generate_response(prompt)\n",
    "        # 생성된 응답과 정답 간 유사도를 기반으로 보상 계산\n",
    "        reward: float = calculate_reward(response, ground_truth)\n",
    "        # 상태에 응답과 보상 기록 추가\n",
    "        state[\"previous_responses\"].append(response)\n",
    "        state[\"previous_rewards\"].append(reward)\n",
    "\n",
    "    # 최종적으로 업데이트된 상태, 선택된 행동, 보상, 응답 반환\n",
    "    return state, action, reward, response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters and Policy Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Union\n",
    "\n",
    "# 학습 파라미터를 초기화하는 함수\n",
    "def initialize_training_params() -> Dict[str, Union[float, int]]:\n",
    "    \"\"\"\n",
    "    학습률, 에피소드 수, 할인 계수 등 강화 학습에 사용할 기본 파라미터들을 초기화합니다.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Union[float, int]]: 초기화된 학습 파라미터들을 담은 딕셔너리\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": 0.01,    # 정책 업데이트를 위한 학습률\n",
    "        \"num_episodes\": 100,      # 전체 학습 에피소드 수\n",
    "        \"discount_factor\": 0.99   # 미래 보상에 대한 할인 계수 (γ)\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 보상 값을 기반으로 정책(policy)을 업데이트하는 함수\n",
    "def update_policy(\n",
    "    policy: Dict[str, Dict[str, Union[float, str]]], \n",
    "    state: Dict[str, object], \n",
    "    action: str, \n",
    "    reward: float, \n",
    "    learning_rate: float\n",
    ") -> Dict[str, Dict[str, Union[float, str]]]:\n",
    "    \"\"\"\n",
    "    에이전트가 받은 보상 값을 바탕으로 정책을 업데이트합니다.\n",
    "\n",
    "    Args:\n",
    "        policy (Dict[str, Dict[str, Union[float, str]]]): 현재의 정책 정보\n",
    "        state (Dict[str, object]): 현재 환경 상태\n",
    "        action (str): 에이전트가 선택한 행동\n",
    "        reward (float): 해당 행동으로부터 받은 보상\n",
    "        learning_rate (float): 정책 업데이트를 위한 학습률\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Union[float, str]]]: 업데이트된 정책 딕셔너리\n",
    "    \"\"\"\n",
    "    # 예시: 매우 단순한 정책 업데이트 (실제 강화 학습 알고리즘으로 대체 가능)\n",
    "    policy[state[\"query\"]] = {\n",
    "        \"action\": action,  # 해당 쿼리에서 수행한 행동 저장\n",
    "        \"reward\": reward   # 해당 행동에 대한 보상 저장\n",
    "    }\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "# 학습 진행 상황을 추적하는 함수\n",
    "def track_progress(\n",
    "    episode: int, \n",
    "    reward: float, \n",
    "    rewards_history: List[float]\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    각 에피소드마다 받은 보상을 저장하여 학습 진행 상황을 추적합니다.\n",
    "\n",
    "    Args:\n",
    "        episode (int): 현재 에피소드 번호\n",
    "        reward (float): 현재 에피소드에서 받은 보상\n",
    "        rewards_history (List[float]): 에피소드별 보상을 저장하는 리스트\n",
    "\n",
    "    Returns:\n",
    "        List[float]: 업데이트된 보상 기록 리스트\n",
    "    \"\"\"\n",
    "    # 현재 보상을 보상 기록 리스트에 추가합니다.\n",
    "    rewards_history.append(reward)\n",
    "    \n",
    "    # 10 에피소드마다 진행 상황을 출력합니다.\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"에피소드 {episode}: 보상 = {reward}\")\n",
    "    \n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RL 기반 RAG 학습 루프를 실행하는 함수\n",
    "def training_loop(\n",
    "    query_text: str, \n",
    "    ground_truth: str, \n",
    "    params: Optional[Dict[str, Union[float, int]]] = None\n",
    ") -> Tuple[Dict[str, Dict[str, Union[float, str]]], List[float], List[List[str]], Optional[str]]:\n",
    "    \"\"\"\n",
    "    RL이 적용된 RAG 파이프라인에 대해 학습 루프를 실행합니다.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): RAG 파이프라인에 입력할 질의 텍스트\n",
    "        ground_truth (str): 해당 질의에 대한 정답 (보상 계산에 사용)\n",
    "        params (Optional[Dict[str, Union[float, int]]]): 학습률, 에피소드 수, 할인 계수 등 학습 파라미터\n",
    "            (None인 경우 기본값을 사용하여 초기화됨)\n",
    "\n",
    "    Returns:\n",
    "        Tuple:\n",
    "            - policy (Dict[str, Dict[str, Union[float, str]]]): 학습 이후의 정책 딕셔너리\n",
    "            - rewards_history (List[float]): 각 에피소드에서 받은 보상 기록\n",
    "            - actions_history (List[List[str]]): 각 에피소드에서 수행한 행동 기록\n",
    "            - best_response (Optional[str]): 학습 중 생성된 최고의 응답\n",
    "    \"\"\"\n",
    "    # 학습 파라미터가 제공되지 않았다면 기본값으로 초기화\n",
    "    if params is None:\n",
    "        params = initialize_training_params()\n",
    "    \n",
    "    # 학습 진행을 위한 변수 초기화\n",
    "    rewards_history: List[float] = []  # 에피소드별 보상 기록 리스트\n",
    "    actions_history: List[List[str]] = []  # 에피소드별 행동 리스트\n",
    "    policy: Dict[str, Dict[str, Union[float, str]]] = {}  # 행동 및 보상을 저장할 정책\n",
    "    action_space: List[str] = define_action_space()  # 사용 가능한 행동 공간 정의\n",
    "    best_response: Optional[str] = None  # 가장 우수한 응답 저장\n",
    "    best_reward: float = -1  # 최고 보상 초기화 (낮은 값으로 시작)\n",
    "\n",
    "    # 기준 비교용: 기본 RAG 파이프라인의 성능 평가\n",
    "    simple_response: str = basic_rag_pipeline(query_text)\n",
    "    simple_reward: float = calculate_reward(simple_response, ground_truth)\n",
    "    print(f\"기본 RAG 보상: {simple_reward:.4f}\")\n",
    "\n",
    "    # 학습 루프 시작\n",
    "    for episode in range(params[\"num_episodes\"]):\n",
    "        # 같은 질의를 사용하여 환경 초기화\n",
    "        context_chunks: List[str] = retrieve_relevant_chunks(query_text)\n",
    "        state: Dict[str, object] = define_state(query_text, context_chunks)\n",
    "        episode_reward: float = 0  # 이번 에피소드의 총 보상\n",
    "        episode_actions: List[str] = []  # 이번 에피소드에서 수행한 행동들\n",
    "\n",
    "        # 무한 루프 방지를 위한 최대 스텝 제한\n",
    "        for step in range(10):\n",
    "            # 한 번의 RL 스텝 수행\n",
    "            state, action, reward, response = rl_step(state, action_space, ground_truth)\n",
    "            episode_actions.append(action)  # 수행한 행동 기록\n",
    "\n",
    "            # 응답이 생성되었으면 에피소드 종료\n",
    "            if response:\n",
    "                episode_reward = reward  # 에피소드 보상 기록\n",
    "\n",
    "                # 가장 높은 보상 및 응답 갱신\n",
    "                if reward > best_reward:\n",
    "                    best_reward = reward\n",
    "                    best_response = response\n",
    "                \n",
    "                break  # 응답이 생성되면 루프 종료\n",
    "\n",
    "        # 학습 이력 업데이트\n",
    "        rewards_history.append(episode_reward)\n",
    "        actions_history.append(episode_actions)\n",
    "\n",
    "        # 매 5 에피소드마다 진행 상황 출력\n",
    "        if episode % 5 == 0:\n",
    "            print(f\"에피소드 {episode}: 보상 = {episode_reward:.4f}, 행동 = {episode_actions}\")\n",
    "    \n",
    "    # 최종 비교 출력\n",
    "    improvement: float = best_reward - simple_reward\n",
    "    print(f\"\\n학습 완료:\")\n",
    "    print(f\"기본 RAG 보상: {simple_reward:.4f}\")\n",
    "    print(f\"강화 학습 기반 RAG 최고 보상: {best_reward:.4f}\")\n",
    "    print(f\"개선 정도: {improvement:.4f} ({improvement * 100:.2f}%)\")\n",
    "\n",
    "    return policy, rewards_history, actions_history, best_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 기본 RAG와 강화 학습 기반 RAG의 성능을 비교하는 함수\n",
    "def compare_rag_approaches(query_text: str, ground_truth: str) -> Tuple[str, str, float, float]:\n",
    "    \"\"\"\n",
    "    기본 RAG와 RL이 적용된 RAG의 출력과 성능을 비교합니다.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): RAG 파이프라인에 사용할 입력 질의\n",
    "        ground_truth (str): 해당 질의에 대한 기대 정답\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str, float, float]: 다음 항목들을 포함하는 튜플 반환:\n",
    "            - simple_response (str): 기본 RAG가 생성한 응답\n",
    "            - best_rl_response (str): 강화 학습 기반 RAG가 생성한 최적 응답\n",
    "            - simple_similarity (float): 기본 RAG 응답과 정답 간 유사도\n",
    "            - rl_similarity (float): 강화 학습 RAG 응답과 정답 간 유사도\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"질의(Query): {query_text}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1단계: 기본 RAG 파이프라인 실행\n",
    "    simple_response: str = basic_rag_pipeline(query_text)\n",
    "    simple_similarity: float = calculate_reward(simple_response, ground_truth)\n",
    "    \n",
    "    print(\"\\n기본 RAG 응답:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(simple_response)\n",
    "    print(f\"정답과의 유사도: {simple_similarity:.4f}\")\n",
    "    \n",
    "    # 2단계: 강화 학습 기반 RAG 모델 학습\n",
    "    print(\"\\n강화 학습 기반 RAG 모델 학습 중...\")\n",
    "    params: Dict[str, Union[float, int]] = initialize_training_params()\n",
    "    params[\"num_episodes\"] = 5  # 시연을 위한 에피소드 수 축소\n",
    "\n",
    "    _, rewards_history, actions_history, best_rl_response = training_loop(\n",
    "        query_text, ground_truth, params\n",
    "    )\n",
    "    \n",
    "    # 학습 중 응답이 생성되지 않았다면 수동으로 생성\n",
    "    if best_rl_response is None:\n",
    "        context_chunks: List[str] = retrieve_relevant_chunks(query_text)\n",
    "        prompt: str = construct_prompt(query_text, context_chunks)\n",
    "        best_rl_response: str = generate_response(prompt)\n",
    "    \n",
    "    rl_similarity: float = calculate_reward(best_rl_response, ground_truth)\n",
    "    \n",
    "    print(\"\\n강화 학습 기반 RAG 응답:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(best_rl_response)\n",
    "    print(f\"정답과의 유사도: {rl_similarity:.4f}\")\n",
    "    \n",
    "    # 3단계: 결과 비교 및 평가\n",
    "    improvement: float = rl_similarity - simple_similarity\n",
    "    \n",
    "    print(\"\\n평가 결과:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"기본 RAG 유사도: {simple_similarity:.4f}\")\n",
    "    print(f\"강화 학습 RAG 유사도: {rl_similarity:.4f}\")\n",
    "    print(f\"개선율: {improvement * 100:.2f}%\")\n",
    "    \n",
    "    # 4단계: 보상 히스토리 시각화 (matplotlib이 설치되어 있을 경우)\n",
    "    if len(rewards_history) > 1:\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(rewards_history)\n",
    "            plt.title('강화 학습 동안의 보상 변화 추이')\n",
    "            plt.xlabel('에피소드')\n",
    "            plt.ylabel('보상')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"matplotlib을 사용할 수 없어 보상을 시각화하지 못했습니다.\")\n",
    "    \n",
    "    return simple_response, best_rl_response, simple_similarity, rl_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 검색된 문맥 덩어리들이 정답 문맥과 얼마나 관련 있는지 평가하는 함수\n",
    "def evaluate_relevance(retrieved_chunks: List[str], ground_truth_chunks: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    검색된 문맥 덩어리들을 정답 문맥 덩어리들과 비교하여 관련성을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        retrieved_chunks (List[str]): 시스템이 검색한 문맥 텍스트 덩어리 리스트\n",
    "        ground_truth_chunks (List[str]): 기준이 되는 정답 문맥 텍스트 덩어리 리스트\n",
    "\n",
    "    Returns:\n",
    "        float: 검색된 문맥과 정답 문맥 간의 평균 관련성 점수 (코사인 유사도 기반)\n",
    "    \"\"\"\n",
    "    relevance_scores: List[float] = []  # 관련성 점수를 저장할 리스트 초기화\n",
    "\n",
    "    # 검색된 덩어리와 정답 덩어리를 한 쌍씩 비교\n",
    "    for retrieved, ground_truth in zip(retrieved_chunks, ground_truth_chunks):\n",
    "        # 각 덩어리의 임베딩을 생성하고, 코사인 유사도를 계산\n",
    "        relevance: float = cosine_similarity(\n",
    "            generate_embeddings([retrieved])[0],\n",
    "            generate_embeddings([ground_truth])[0]\n",
    "        )\n",
    "        # 계산된 관련성 점수를 리스트에 추가\n",
    "        relevance_scores.append(relevance)\n",
    "\n",
    "    # 관련성 점수들의 평균값을 반환\n",
    "    return np.mean(relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 생성된 응답의 정확도를 평가하는 함수\n",
    "def evaluate_accuracy(responses: List[str], ground_truth_responses: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    생성된 응답들을 정답 응답들과 비교하여 정확도를 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        responses (List[str]): 평가할 생성된 응답 리스트\n",
    "        ground_truth_responses (List[str]): 비교 대상이 되는 정답 응답 리스트\n",
    "\n",
    "    Returns:\n",
    "        float: 생성된 응답과 정답 응답 간 임베딩 유사도(코사인 유사도)의 평균값을 반환\n",
    "    \"\"\"\n",
    "    accuracy_scores: List[float] = []  # 정확도 점수를 저장할 리스트 초기화\n",
    "\n",
    "    # 생성된 응답과 정답 응답을 한 쌍씩 반복하며 비교\n",
    "    for response, ground_truth in zip(responses, ground_truth_responses):\n",
    "        # 두 응답의 임베딩을 생성하고, 코사인 유사도를 계산\n",
    "        accuracy: float = cosine_similarity(\n",
    "            generate_embeddings([response])[0],\n",
    "            generate_embeddings([ground_truth])[0]\n",
    "        )\n",
    "        # 계산된 유사도 점수를 리스트에 추가\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "    # 평균 정확도 점수를 반환\n",
    "    return np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "# 생성된 응답의 품질을 평가하는 함수\n",
    "def evaluate_response_quality(responses: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    휴리스틱 또는 외부 모델을 사용하여 응답의 품질을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        responses (List[str]): 평가할 생성된 응답 리스트\n",
    "\n",
    "    Returns:\n",
    "        float: 응답들의 평균 품질 점수 (0 ~ 1 사이의 값)\n",
    "    \"\"\"\n",
    "    quality_scores: List[float] = []  # 각 응답의 품질 점수를 저장할 리스트 초기화\n",
    "\n",
    "    for response in responses:\n",
    "        # 예시 휴리스틱: 응답 길이를 기반으로 품질 점수를 계산\n",
    "        # 최대 100단어 기준으로 정규화하며, 1.0을 초과하지 않도록 제한\n",
    "        quality: float = len(response.split()) / 100\n",
    "        quality_scores.append(min(quality, 1.0))  # 최대 1.0으로 제한하여 저장\n",
    "\n",
    "    # 모든 응답에 대한 평균 품질 점수를 반환\n",
    "    return np.mean(quality_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAG 파이프라인의 성능을 평가하는 함수\n",
    "def evaluate_rag_performance(\n",
    "    queries: List[str], \n",
    "    ground_truth_chunks: List[str], \n",
    "    ground_truth_responses: List[str]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    관련성, 정확도, 응답 품질 지표를 사용하여 RAG 파이프라인의 성능을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        queries (List[str]): 평가할 질의 목록\n",
    "        ground_truth_chunks (List[str]): 각 질의에 대응되는 정답 문맥 덩어리 목록\n",
    "        ground_truth_responses (List[str]): 각 질의에 대응되는 정답 응답 목록\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: 관련성, 정확도, 품질의 평균 점수를 포함한 딕셔너리\n",
    "    \"\"\"\n",
    "    # 각 지표별 점수를 저장할 리스트 초기화\n",
    "    relevance_scores: List[float] = []\n",
    "    accuracy_scores: List[float] = []\n",
    "    quality_scores: List[float] = []\n",
    "\n",
    "    # 각 질의와 해당 정답 데이터를 기준으로 반복 평가\n",
    "    for query, ground_truth_chunk, ground_truth_response in zip(queries, ground_truth_chunks, ground_truth_responses):\n",
    "        # 질의에 대해 관련 문맥 덩어리 검색\n",
    "        retrieved_chunks: List[str] = retrieve_relevant_chunks(query)\n",
    "        \n",
    "        # 검색된 문맥과 정답 문맥 간의 관련성 평가\n",
    "        relevance: float = evaluate_relevance(retrieved_chunks, [ground_truth_chunk])\n",
    "        relevance_scores.append(relevance)\n",
    "\n",
    "        # 기본 RAG 파이프라인을 사용해 응답 생성\n",
    "        response: str = basic_rag_pipeline(query)\n",
    "        \n",
    "        # 생성된 응답과 정답 응답 간의 정확도 평가\n",
    "        accuracy: float = evaluate_accuracy([response], [ground_truth_response])\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "        # 생성된 응답의 품질 평가 (예: 길이 기반 휴리스틱)\n",
    "        quality: float = evaluate_response_quality([response])\n",
    "        quality_scores.append(quality)\n",
    "\n",
    "    # 각 지표의 평균값 계산\n",
    "    avg_relevance: float = np.mean(relevance_scores)\n",
    "    avg_accuracy: float = np.mean(accuracy_scores)\n",
    "    avg_quality: float = np.mean(quality_scores)\n",
    "\n",
    "    # 평균 점수들을 딕셔너리 형태로 반환\n",
    "    return {\n",
    "        \"average_relevance\": avg_relevance,\n",
    "        \"average_accuracy\": avg_accuracy,\n",
    "        \"average_quality\": avg_quality\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating (RL vs Simple) RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG(Retrieval-Augmented Generation) 파이프라인을 실행합니다...\n",
      "입력 질의: 큐비트가 중첩 상태에 있을 때의 수학적 표현은 무엇인가?\n",
      "\n",
      "AI 응답:\n",
      "------------------------------\n",
      "큐비트가 중첩 상태에 있을 때의 수학적 표현은 다음과 같습니다: ψ = α|0⟩ + β|1⟩, 여기서 α와 β는 각각 상태 0 또는 1의 큐비트를 측정할 때의 확률 진폭을 나타내며, α² + β² = 1을 만족하는 복소수입니다.\n",
      "------------------------------\n",
      "정답(기대 응답):\n",
      "------------------------------\n",
      "|ψ⟩ = α|0⟩ + β|1⟩으로, α와 β는 복소수이며 |α|² + |β|² = 1을 만족하여, 각각 상태 |0⟩ 또는 |1⟩로 측정될 확률 진폭을 나타낸다.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# RAG 파이프라인 실행 시작 알림 출력\n",
    "print(\"RAG(Retrieval-Augmented Generation) 파이프라인을 실행합니다...\")\n",
    "print(f\"입력 질의: {sample_query}\\n\")\n",
    "\n",
    "# RAG 파이프라인 실행 및 응답 생성\n",
    "response = basic_rag_pipeline(sample_query)\n",
    "\n",
    "# 생성된 응답 출력 (포맷팅 포함)\n",
    "print(\"AI 응답:\")\n",
    "print(\"-\" * 30)\n",
    "print(response.strip())\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 정답(ground truth) 출력\n",
    "print(\"정답(기대 응답):\")\n",
    "print(\"-\" * 30)\n",
    "print(expected_answer)\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "질의(Query): 큐비트가 중첩 상태에 있을 때의 수학적 표현은 무엇인가?\n",
      "================================================================================\n",
      "\n",
      "기본 RAG 응답:\n",
      "----------------------------------------\n",
      "큐비트가 중첩 상태에 있을 때의 수학적 표현은 ψ = α|0⟩ + β|1⟩ 입니다. 여기서 α와 β는 각각 상태 0 또는 1의 큐비트를 측정할 때의 확률 진폭을 나타내며, α² + β² = 1을 만족하는 복소수입니다.\n",
      "정답과의 유사도: 0.7544\n",
      "\n",
      "강화 학습 기반 RAG 모델 학습 중...\n",
      "기본 RAG 보상: 0.7558\n",
      "에피소드 0: 보상 = 0.6768, 행동 = ['rewrite_query', 'rewrite_query', 'rewrite_query', 'filter_context', 'generate_response']\n",
      "\n",
      "학습 완료:\n",
      "기본 RAG 보상: 0.7558\n",
      "강화 학습 기반 RAG 최고 보상: 0.7126\n",
      "개선 정도: -0.0432 (-4.32%)\n",
      "\n",
      "강화 학습 기반 RAG 응답:\n",
      "----------------------------------------\n",
      "큐비트의 중첩 상태는 다음과 같이 수학적으로 표현됩니다: \n",
      "\n",
      "ψ = α|0⟩ + β|1⟩ \n",
      "\n",
      "여기서 α와 β는 복소수로 각각 상태 0 또는 1의 큐비트를 측정할 때의 확률 진폭을 나타내며, 이들은 다음 조건을 만족해야 합니다: \n",
      "\n",
      "|α|² + |β|² = 1 \n",
      "\n",
      "이러한 중첩 상태는 큐비트가 측정될 때까지 0과 1이 동시에 존재하는 상태를 나타냅니다.\n",
      "정답과의 유사도: 0.7124\n",
      "\n",
      "평가 결과:\n",
      "----------------------------------------\n",
      "기본 RAG 유사도: 0.7544\n",
      "강화 학습 RAG 유사도: 0.7124\n",
      "개선율: -4.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 48372 (\\N{HANGUL SYLLABLE BO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 49345 (\\N{HANGUL SYLLABLE SANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 44053 (\\N{HANGUL SYLLABLE GANG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54868 (\\N{HANGUL SYLLABLE HWA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54617 (\\N{HANGUL SYLLABLE HAG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 49845 (\\N{HANGUL SYLLABLE SEUB}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 46041 (\\N{HANGUL SYLLABLE DONG}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 50504 (\\N{HANGUL SYLLABLE AN}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51032 (\\N{HANGUL SYLLABLE YI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 48320 (\\N{HANGUL SYLLABLE BYEON}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 52628 (\\N{HANGUL SYLLABLE CU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 50640 (\\N{HANGUL SYLLABLE E}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 54588 (\\N{HANGUL SYLLABLE PI}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 49548 (\\N{HANGUL SYLLABLE SO}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/home/kubwa/anaconda3/envs/lecture/lib/python3.11/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 46300 (\\N{HANGUL SYLLABLE DEU}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 샘플 질의 및 정답을 사용하여 기본 RAG 파이프라인과 RL 기반 RAG 파이프라인의 성능을 비교합니다.\n",
    "# 함수는 다음 값을 반환합니다:\n",
    "# - simple_response: 기본 RAG 파이프라인이 생성한 응답\n",
    "# - rl_response: RL 기반 RAG 파이프라인이 생성한 최상의 응답\n",
    "# - simple_sim: 기본 RAG 응답과 정답 간의 유사도 점수\n",
    "# - rl_sim: RL 기반 RAG 응답과 정답 간의 유사도 점수\n",
    "simple_response, rl_response, simple_sim, rl_sim = compare_rag_approaches(sample_query, expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "결과가 'rl_rag_results.json' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 나중에 비교할 수 있도록 결과를 저장합니다\n",
    "results = {\n",
    "    \"query\": query_text,  # 입력 질의\n",
    "    \"ground_truth\": expected_answer,  # 해당 질의의 기대 정답\n",
    "    \"simple_rag\": {\n",
    "        \"response\": simple_response,  # 기본 RAG 파이프라인이 생성한 응답\n",
    "        \"similarity\": float(simple_sim)  # 정답과의 유사도 점수\n",
    "    },\n",
    "    \"rl_rag\": {\n",
    "        \"response\": rl_response,  # RL 기반 RAG 파이프라인이 생성한 응답\n",
    "        \"similarity\": float(rl_sim)  # 정답과의 유사도 점수\n",
    "    },\n",
    "    \"improvement\": float(rl_sim - simple_sim)  # RL 기반 RAG로 인한 유사도 향상 정도\n",
    "}\n",
    "\n",
    "# 결과를 JSON 파일로 저장\n",
    "with open('dataset/rl_rag_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)  # 읽기 쉽게 들여쓰기하여 저장\n",
    "\n",
    "# 저장 완료 메시지 출력\n",
    "print(\"\\n결과가 'rl_rag_results.json' 파일에 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
