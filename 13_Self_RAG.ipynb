{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Self-RAG: A Dynamic Approach to RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "model = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일로부터 텍스트를 추출하는 함수\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        str: 추출된 전체 텍스트\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 전체 텍스트를 저장할 문자열 초기화\n",
    "\n",
    "    # PDF의 각 페이지를 순회하며 텍스트 추출\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 현재 페이지 가져오기\n",
    "        text = page.get_text(\"text\")  # 해당 페이지에서 텍스트 추출\n",
    "        all_text += text  # 추출한 텍스트를 누적\n",
    "\n",
    "    # 최종적으로 전체 텍스트 반환\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 일정 길이로 나누되, 일부 겹치는 부분을 포함하여 청크로 분할하는 함수\n",
    "\n",
    "    Args:\n",
    "        text (str): 분할할 원본 텍스트\n",
    "        n (int): 각 청크의 문자 수\n",
    "        overlap (int): 청크 간 겹치는 문자 수\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 분할된 텍스트 청크 리스트\n",
    "    \"\"\"\n",
    "    chunks = []  # 분할된 청크들을 저장할 리스트 초기화\n",
    "\n",
    "    # 시작 인덱스를 (n - overlap) 간격으로 이동하면서 청크 생성\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunk = text[i:i + n]  # i부터 i+n까지의 텍스트를 하나의 청크로 추출\n",
    "        chunks.append(chunk)  # 추출한 청크를 리스트에 추가\n",
    "\n",
    "    # 생성된 청크 리스트 반환\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 활용한 간단한 벡터 저장소 클래스\n",
    "    텍스트, 임베딩, 메타데이터를 함께 관리하고 유사도 기반 검색을 지원\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []   # 임베딩 벡터들을 저장하는 리스트\n",
    "        self.texts = []     # 원본 텍스트를 저장하는 리스트\n",
    "        self.metadata = []  # 각 텍스트에 대한 메타데이터를 저장하는 리스트\n",
    "\n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 새로운 항목을 추가\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트\n",
    "            embedding (List[float]): 해당 텍스트의 임베딩 벡터\n",
    "            metadata (dict, optional): 텍스트와 관련된 부가 정보\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))        # 임베딩을 NumPy 배열로 변환하여 저장\n",
    "        self.texts.append(text)                         # 원본 텍스트 저장\n",
    "        self.metadata.append(metadata or {})            # 메타데이터가 없으면 빈 딕셔너리 저장\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        주어진 질의 임베딩과 가장 유사한 항목 k개를 검색\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 질의 임베딩 벡터\n",
    "            k (int): 반환할 유사 항목 수\n",
    "            filter_func (callable, optional): 메타데이터 필터링 함수\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 유사도가 높은 상위 k개의 항목 리스트 (텍스트, 메타데이터, 유사도 포함)\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "\n",
    "        # 질의 임베딩을 NumPy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "\n",
    "        # 코사인 유사도를 기반으로 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 필터 함수가 있는 경우, 조건을 만족하지 않으면 건너뜀\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "\n",
    "            # 코사인 유사도 계산\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 저장\n",
    "\n",
    "        # 유사도 기준으로 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 상위 k개의 결과를 구성하여 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],          # 관련 텍스트\n",
    "                \"metadata\": self.metadata[idx],   # 관련 메타데이터\n",
    "                \"similarity\": score               # 유사도 점수\n",
    "            })\n",
    "\n",
    "        return results  # 유사 항목 리스트 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대해 임베딩을 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "        text (str 또는 List[str]): 임베딩을 생성할 입력 텍스트 또는 텍스트 리스트\n",
    "        model (str): 임베딩 생성을 위한 모델 이름\n",
    "\n",
    "    Returns:\n",
    "        List[float] 또는 List[List[float]]: 생성된 임베딩 벡터 또는 벡터 리스트\n",
    "    \"\"\"\n",
    "    # 입력이 문자열이면 리스트로 변환하여 처리\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "\n",
    "    # 지정한 모델을 사용하여 임베딩 생성\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "\n",
    "    # 단일 문자열 입력인 경우 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    # 리스트 입력인 경우 모든 임베딩 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Self-RAG을 위한 문서 처리 함수\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        chunk_size (int): 각 청크의 문자 수\n",
    "        chunk_overlap (int): 청크 간 겹치는 문자 수\n",
    "\n",
    "    Returns:\n",
    "        SimpleVectorStore: 문서 청크와 임베딩을 포함하는 벡터 저장소\n",
    "    \"\"\"\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 추출된 텍스트를 일정 길이로 분할\n",
    "    print(\"텍스트 청크 분할 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"{len(chunks)}개의 텍스트 청크 생성 완료\")\n",
    "\n",
    "    # 각 청크에 대해 임베딩 생성\n",
    "    print(\"청크 임베딩 생성 중...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "\n",
    "    # 벡터 저장소 초기화\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    # 각 청크와 임베딩, 메타데이터를 저장소에 추가\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "\n",
    "    print(f\"{len(chunks)}개의 청크가 벡터 저장소에 추가됨\")\n",
    "\n",
    "    # 벡터 저장소 반환\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-RAG Components\n",
    "### 1. Retrieval Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_if_retrieval_needed(query):\n",
    "    \"\"\"\n",
    "    주어진 질의에 대해 외부 정보 검색이 필요한지 여부를 판단하는 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "\n",
    "    Returns:\n",
    "        bool: 검색이 필요한 경우 True, 그렇지 않으면 False\n",
    "    \"\"\"\n",
    "    # AI에게 검색 필요 여부를 판별하는 기준을 알려주는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 주어진 질의에 대해 검색이 필요한지를 판별하는 AI 어시스턴트입니다.\n",
    "    사실 기반 질문, 특정 정보 요청, 사건, 인물, 개념에 대한 질문이라면 \"Yes\"라고 답하세요.\n",
    "    의견, 가정적 시나리오, 일반 상식에 해당하는 질문은 \"No\"라고 답하세요.\n",
    "    반드시 \"Yes\" 또는 \"No\"로만 답변하세요.\"\"\"\n",
    "\n",
    "    # 사용자 질의를 포함한 프롬프트 구성\n",
    "    user_prompt = f\"Query: {query}\\n\\nIs retrieval necessary to answer this query accurately?\"\n",
    "\n",
    "    # AI 모델 호출을 통해 검색 필요 여부 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 응답 내용에서 \"yes\" 또는 \"no\" 추출 후 소문자로 변환\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    # \"yes\"가 포함되어 있으면 True 반환, 아니면 False\n",
    "    return \"yes\" in answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Relevance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(query, context):\n",
    "    \"\"\"\n",
    "    주어진 문서 내용이 사용자 질의와 관련이 있는지를 평가하는 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str): 문서 또는 텍스트 콘텐츠\n",
    "\n",
    "    Returns:\n",
    "        str: 'relevant' 또는 'irrelevant' 중 하나\n",
    "    \"\"\"\n",
    "    # AI에게 문서 관련성 판단 기준을 안내하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 문서가 특정 질의와 관련이 있는지를 판단하는 AI 어시스턴트입니다.\n",
    "    문서가 질의에 답하는 데 도움이 되는 정보를 포함하고 있는지를 기준으로 판단하세요.\n",
    "    반드시 \"Relevant\" 또는 \"Irrelevant\" 중 하나로만 답변하세요.\"\"\"\n",
    "\n",
    "    # 너무 긴 문맥은 잘라서 처리 (토큰 초과 방지)\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [truncated]\"\n",
    "\n",
    "    # 사용자 프롬프트 구성: 질의와 문서 내용 포함\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "    Document content:\n",
    "    {context}\n",
    "\n",
    "    이 문서가 쿼리와 관련이 있나요? \"관련 있음\" 또는 \"관련 없음\"으로만 답변하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # AI 모델 호출을 통해 관련성 판단 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 응답을 소문자로 변환하여 반환\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Support Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_support(response, context):\n",
    "    \"\"\"\n",
    "    응답이 문맥에 의해 얼마나 잘 뒷받침되는지를 평가하는 함수\n",
    "\n",
    "    Args:\n",
    "        response (str): 생성된 응답\n",
    "        context (str): 응답의 근거가 되는 문서 또는 텍스트\n",
    "\n",
    "    Returns:\n",
    "        str: 'fully supported', 'partially supported', 'no support' 중 하나\n",
    "    \"\"\"\n",
    "    # 문맥이 응답을 뒷받침하는지를 판단하는 기준 안내용 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 주어진 문맥이 응답 내용을 얼마나 잘 뒷받침하는지를 평가하는 AI 어시스턴트입니다.\n",
    "    응답 내 주장, 사실, 정보가 문맥에 의해 근거를 갖는지를 판단하세요.\n",
    "    반드시 다음 중 하나로만 답변하세요:\n",
    "    - \"Fully supported\": 응답의 모든 정보가 문맥에 명확하게 근거함\n",
    "    - \"Partially supported\": 일부 정보는 문맥에 근거하지만 일부는 아님\n",
    "    - \"No support\": 문맥에 전혀 근거가 없거나 모순되는 정보가 포함됨\n",
    "    \"\"\"\n",
    "\n",
    "    # 문맥이 너무 길 경우 일부만 사용 (토큰 초과 방지)\n",
    "    max_context_length = 2000\n",
    "    if len(context) > max_context_length:\n",
    "        context = context[:max_context_length] + \"... [truncated]\"\n",
    "\n",
    "    # 사용자 프롬프트 구성: 문맥과 응답 내용을 포함\n",
    "    user_prompt = f\"\"\"Context:\n",
    "    {context}\n",
    "\n",
    "    Response:\n",
    "    {response}\n",
    "\n",
    "    이 응답이 문맥에 의해 얼마나 잘 뒷받침되는지 평가하세요. 반드시 \"Fully supported\", \"Partially supported\", 또는 \"No support\" 중 하나로만 답변하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # AI 모델 호출을 통해 평가 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 응답에서 평가 결과 추출 후 소문자로 변환\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    return answer  # 평가 결과 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Utility Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_utility(query, response):\n",
    "    \"\"\"\n",
    "    질의에 대한 응답의 유용성을 평가하는 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        response (str): 생성된 응답\n",
    "\n",
    "    Returns:\n",
    "        int: 유용성 평가 점수 (1점부터 5점까지)\n",
    "    \"\"\"\n",
    "    # AI에게 유용성 평가 기준을 안내하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 질의에 대한 응답의 유용성을 평가하는 AI 어시스턴트입니다.\n",
    "    응답이 질의를 얼마나 잘 해결하는지, 정보의 완전성, 정확성, 실용성을 고려하세요.\n",
    "    다음 기준에 따라 1점에서 5점 사이로 평가하세요:\n",
    "    - 1: 전혀 유용하지 않음\n",
    "    - 2: 거의 유용하지 않음\n",
    "    - 3: 보통 수준으로 유용함\n",
    "    - 4: 매우 유용함\n",
    "    - 5: 탁월하게 유용함\n",
    "    반드시 1~5 중 하나의 숫자만 답변하세요.\"\"\"\n",
    "\n",
    "    # 사용자 질의와 응답을 포함한 프롬프트 구성\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "    Response:\n",
    "    {response}\n",
    "\n",
    "    위 응답의 유용성을 1점에서 5점 사이로 평가하세요:\"\"\"\n",
    "\n",
    "    # AI 모델 호출을 통해 유용성 점수 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 생성된 응답에서 숫자 추출\n",
    "    rating = response.choices[0].message.content.strip()\n",
    "\n",
    "    # 응답 내에서 1~5 사이의 숫자만 추출\n",
    "    rating_match = re.search(r'[1-5]', rating)\n",
    "    if rating_match:\n",
    "        return int(rating_match.group())  # 숫자만 정수로 변환하여 반환\n",
    "\n",
    "    return 3  # 실패 시 중간값 3점 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context=None):\n",
    "    \"\"\"\n",
    "    질의와 선택적 문맥을 기반으로 응답을 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        context (str, optional): 참고할 문맥 텍스트 (선택)\n",
    "\n",
    "    Returns:\n",
    "        str: 생성된 응답 텍스트\n",
    "    \"\"\"\n",
    "    # AI에게 도움이 되는 응답을 생성하라고 안내하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 유용한 AI 어시스턴트입니다. 명확하고 정확하며 정보에 기반한 응답을 제공하세요.\"\"\"\n",
    "\n",
    "    # 문맥이 제공된 경우, 문맥을 포함하여 사용자 프롬프트 구성\n",
    "    if context:\n",
    "        user_prompt = f\"\"\"Context:\n",
    "        {context}\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        위 문맥을 기반으로 질의에 응답하세요.\"\"\"\n",
    "    else:\n",
    "        # 문맥이 없는 경우, 질의만 포함\n",
    "        user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "        최선을 다해 질의에 응답하세요.\"\"\"\n",
    "\n",
    "    # AI 모델을 호출하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "    # 응답 텍스트 추출 후 양쪽 공백 제거하여 반환\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Self-RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_rag(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    Self-RAG 전체 파이프라인을 수행하는 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 문서 청크를 담고 있는 벡터 저장소\n",
    "        top_k (int): 초기 검색 시 반환할 문서 수\n",
    "\n",
    "    Returns:\n",
    "        dict: 질의, 생성된 응답, 과정 중 수집된 평가 메트릭을 포함한 결과 딕셔너리\n",
    "    \"\"\"\n",
    "    print(f\"\\nSelf-RAG 시작: {query}\\n\")\n",
    "\n",
    "    # 1단계: 외부 검색이 필요한 질의인지 판단\n",
    "    print(\"1단계: 검색 필요 여부 판단 중...\")\n",
    "    retrieval_needed = determine_if_retrieval_needed(query)\n",
    "    print(f\"검색 필요 여부: {retrieval_needed}\")\n",
    "\n",
    "    # Self-RAG 과정 중 측정할 메트릭 초기화\n",
    "    metrics = {\n",
    "        \"retrieval_needed\": retrieval_needed,\n",
    "        \"documents_retrieved\": 0,\n",
    "        \"relevant_documents\": 0,\n",
    "        \"response_support_ratings\": [],\n",
    "        \"utility_ratings\": []\n",
    "    }\n",
    "\n",
    "    best_response = None\n",
    "    best_score = -1\n",
    "\n",
    "    if retrieval_needed:\n",
    "        # 2단계: 질의 임베딩을 기반으로 문서 검색\n",
    "        print(\"\\n2단계: 관련 문서 검색 중...\")\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "        metrics[\"documents_retrieved\"] = len(results)\n",
    "        print(f\"{len(results)}개의 문서 검색됨\")\n",
    "\n",
    "        # 3단계: 검색된 문서들의 관련성 평가\n",
    "        print(\"\\n3단계: 문서 관련성 평가 중...\")\n",
    "        relevant_contexts = []\n",
    "\n",
    "        for i, result in enumerate(results):\n",
    "            context = result[\"text\"]\n",
    "            relevance = evaluate_relevance(query, context)\n",
    "            print(f\"문서 {i+1} 관련성: {relevance}\")\n",
    "            if relevance == \"relevant\":\n",
    "                relevant_contexts.append(context)\n",
    "\n",
    "        metrics[\"relevant_documents\"] = len(relevant_contexts)\n",
    "        print(f\"관련 문서 수: {len(relevant_contexts)}\")\n",
    "\n",
    "        # 4단계: 관련 문서 각각에 대해 응답 생성 및 평가\n",
    "        if relevant_contexts:\n",
    "            print(\"\\n4단계: 관련 문서 기반 응답 생성 및 평가 중...\")\n",
    "            for i, context in enumerate(relevant_contexts):\n",
    "                print(f\"\\n문맥 {i+1}/{len(relevant_contexts)} 처리 중...\")\n",
    "\n",
    "                print(\"응답 생성 중...\")\n",
    "                response = generate_response(query, context)\n",
    "\n",
    "                print(\"응답의 문맥 기반 근거 평가 중...\")\n",
    "                support_rating = assess_support(response, context)\n",
    "                print(f\"근거 평가: {support_rating}\")\n",
    "                metrics[\"response_support_ratings\"].append(support_rating)\n",
    "\n",
    "                print(\"응답 유용성 평가 중...\")\n",
    "                utility_rating = rate_utility(query, response)\n",
    "                print(f\"유용성 점수: {utility_rating}/5\")\n",
    "                metrics[\"utility_ratings\"].append(utility_rating)\n",
    "\n",
    "                # 응답의 전반적 점수 계산\n",
    "                support_score = {\n",
    "                    \"fully supported\": 3,\n",
    "                    \"partially supported\": 1,\n",
    "                    \"no support\": 0\n",
    "                }.get(support_rating, 0)\n",
    "\n",
    "                overall_score = support_score * 5 + utility_rating\n",
    "                print(f\"전체 점수: {overall_score}\")\n",
    "\n",
    "                # 가장 높은 점수의 응답 저장\n",
    "                if overall_score > best_score:\n",
    "                    best_response = response\n",
    "                    best_score = overall_score\n",
    "                    print(\"새로운 최적 응답 업데이트됨\")\n",
    "\n",
    "        # 관련 문서가 없거나 응답 품질이 낮은 경우\n",
    "        if not relevant_contexts or best_score <= 0:\n",
    "            print(\"\\n적절한 문맥이 없거나 응답 품질이 낮아 문맥 없이 직접 생성 중...\")\n",
    "            best_response = generate_response(query)\n",
    "    else:\n",
    "        # 검색 없이 직접 응답 생성\n",
    "        print(\"\\n검색 없이 직접 응답 생성 중...\")\n",
    "        best_response = generate_response(query)\n",
    "\n",
    "    # 최종 메트릭 정리\n",
    "    metrics[\"best_score\"] = best_score\n",
    "    metrics[\"used_retrieval\"] = retrieval_needed and best_score > 0\n",
    "\n",
    "    print(\"\\nSelf-RAG 완료\")\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": best_response,\n",
    "        \"metrics\": metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete Self-RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_rag_example():\n",
    "    \"\"\"\n",
    "    Self-RAG 시스템의 전체 작동 예시를 보여주는 함수\n",
    "    \"\"\"\n",
    "    # 문서 전처리 수행\n",
    "    pdf_path = \"dataset/AI_Understanding.pdf\"  # 처리할 PDF 문서 경로\n",
    "    print(f\"문서 처리 중: {pdf_path}\")\n",
    "    vector_store = process_document(pdf_path)  # 문서를 벡터 저장소로 변환\n",
    "\n",
    "    # 예제 1: 검색이 필요할 가능성이 높은 질의\n",
    "    query1 = \"AI 개발의 주요 윤리적 문제는 무엇인가요?\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"예제 1: {query1}\")\n",
    "    result1 = self_rag(query1, vector_store)  # Self-RAG 실행\n",
    "    print(\"\\n최종 응답:\")\n",
    "    print(result1[\"response\"])  # 생성된 응답 출력\n",
    "    print(\"\\n메트릭:\")\n",
    "    print(json.dumps(result1[\"metrics\"], indent=2))  # 평가 지표 출력\n",
    "\n",
    "    # 예제 2: 검색 없이 직접 생성해도 되는 창작형 질의\n",
    "    query2 = \"인공지능에 대한 시 한 편을 쓸 수 있나요?\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"예제 2: {query2}\")\n",
    "    result2 = self_rag(query2, vector_store)\n",
    "    print(\"\\n최종 응답:\")\n",
    "    print(result2[\"response\"])\n",
    "    print(\"\\n메트릭:\")\n",
    "    print(json.dumps(result2[\"metrics\"], indent=2))\n",
    "\n",
    "    # 예제 3: 문서와 관련 있지만 추가 지식이 필요한 복합 질의\n",
    "    query3 = \"AI가 개발도상국의 경제 성장에 어떤 영향을 미칠까요?\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"예제 3: {query3}\")\n",
    "    result3 = self_rag(query3, vector_store)\n",
    "    print(\"\\n최종 응답:\")\n",
    "    print(result3[\"response\"])\n",
    "    print(\"\\n메트릭:\")\n",
    "    print(json.dumps(result3[\"metrics\"], indent=2))\n",
    "\n",
    "    # 결과 딕셔너리로 반환\n",
    "    return {\n",
    "        \"example1\": result1,\n",
    "        \"example2\": result2,\n",
    "        \"example3\": result3\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Self-RAG Against Traditional RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traditional_rag(query, vector_store, top_k=3):\n",
    "    \"\"\"\n",
    "    전통적인 RAG 방식으로 질의에 응답을 생성하는 함수 (비교용)\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        vector_store (SimpleVectorStore): 문서 청크를 포함한 벡터 저장소\n",
    "        top_k (int): 검색할 문서 수\n",
    "\n",
    "    Returns:\n",
    "        str: 생성된 응답 텍스트\n",
    "    \"\"\"\n",
    "    print(f\"\\n전통적인 RAG 실행 중: {query}\\n\")\n",
    "\n",
    "    # 질의 임베딩 생성 및 유사 문서 검색\n",
    "    print(\"문서 검색 중...\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "    results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "    print(f\"{len(results)}개의 문서 검색됨\")\n",
    "\n",
    "    # 검색된 문서들의 텍스트를 하나의 문맥으로 결합\n",
    "    contexts = [result[\"text\"] for result in results]\n",
    "    combined_context = \"\\n\\n\".join(contexts)\n",
    "\n",
    "    # 결합된 문맥을 기반으로 응답 생성\n",
    "    print(\"응답 생성 중...\")\n",
    "    response = generate_response(query, combined_context)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_approaches(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Self-RAG과 전통적인 RAG 방식을 비교 평가하는 함수\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): 문서 경로\n",
    "        test_queries (List[str]): 테스트 질의 리스트\n",
    "        reference_answers (List[str], optional): 평가용 참조 정답 리스트\n",
    "\n",
    "    Returns:\n",
    "        dict: 평가 결과 딕셔너리 (질의별 비교 결과 및 전체 분석 포함)\n",
    "    \"\"\"\n",
    "    print(\"RAG 방식 비교 평가 시작\")\n",
    "\n",
    "    # 문서를 처리하여 벡터 저장소 생성\n",
    "    vector_store = process_document(pdf_path)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n질의 {i+1} 처리 중: {query}\")\n",
    "\n",
    "        # Self-RAG 실행\n",
    "        self_rag_result = self_rag(query, vector_store)\n",
    "        self_rag_response = self_rag_result[\"response\"]\n",
    "\n",
    "        # 전통적인 RAG 실행\n",
    "        trad_rag_response = traditional_rag(query, vector_store)\n",
    "\n",
    "        # 참조 정답이 있다면 불러옴\n",
    "        reference = reference_answers[i] if reference_answers and i < len(reference_answers) else None\n",
    "\n",
    "        # 응답 비교 수행 (정량 또는 정성 평가)\n",
    "        comparison = compare_responses(\n",
    "            query,\n",
    "            self_rag_response,\n",
    "            trad_rag_response,\n",
    "            reference\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"self_rag_response\": self_rag_response,\n",
    "            \"traditional_rag_response\": trad_rag_response,\n",
    "            \"reference_answer\": reference,\n",
    "            \"comparison\": comparison,\n",
    "            \"self_rag_metrics\": self_rag_result[\"metrics\"]\n",
    "        })\n",
    "\n",
    "    # 전체 결과 분석 수행 (예: 점수 평균, 빈도, 요약 등)\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_responses(query, self_rag_response, trad_rag_response, reference=None):\n",
    "    \"\"\"\n",
    "    Self-RAG과 전통 RAG 응답을 비교 분석하는 함수\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질의\n",
    "        self_rag_response (str): Self-RAG 응답\n",
    "        trad_rag_response (str): 전통 RAG 응답\n",
    "        reference (str, optional): 참조 정답 (사실 검증용)\n",
    "\n",
    "    Returns:\n",
    "        str: 응답 비교 분석 결과\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 비교 기준과 역할 정의\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템 응답 평가 전문가입니다.\n",
    "    당신의 임무는 두 가지 RAG 접근 방식의 응답을 비교 분석하는 것입니다:\n",
    "\n",
    "    1. Self-RAG: 검색 필요 여부를 동적으로 판단하고, 관련성과 응답 품질을 평가함\n",
    "    2. 전통 RAG: 항상 문서를 검색하여 그 내용을 기반으로 응답을 생성함\n",
    "\n",
    "    다음 기준에 따라 응답을 비교하세요:\n",
    "    - 질의와의 관련성\n",
    "    - 사실적 정확성\n",
    "    - 정보의 완전성과 유익함\n",
    "    - 간결성 및 초점의 명확성\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 구성\n",
    "    user_prompt = f\"\"\"질의:\n",
    "    {query}\n",
    "\n",
    "    Self-RAG 응답:\n",
    "    {self_rag_response}\n",
    "\n",
    "    전통 RAG 응답:\n",
    "    {trad_rag_response}\n",
    "    \"\"\"\n",
    "\n",
    "    # 참조 정답이 있다면 포함\n",
    "    if reference:\n",
    "        user_prompt += f\"\"\"\n",
    "    참조 정답 (사실 검증용):\n",
    "    {reference}\n",
    "    \"\"\"\n",
    "\n",
    "    # 평가 요청 문구 추가\n",
    "    user_prompt += \"\"\"\n",
    "    위 두 응답을 비교하고 어떤 응답이 더 나은지 그 이유를 설명하세요.\n",
    "    정확성, 관련성, 정보의 완전성, 응답 품질을 중심으로 평가해주세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # LLM을 통해 비교 분석 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # 평가에 적합한 모델 사용\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 분석 결과 텍스트 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Self-RAG과 전통적인 RAG의 테스트 결과를 바탕으로 종합 분석을 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "        results (List[Dict]): evaluate_rag_approaches에서 생성된 비교 결과 리스트\n",
    "\n",
    "    Returns:\n",
    "        str: 전체 분석 결과 텍스트\n",
    "    \"\"\"\n",
    "    # LLM에게 비교 분석의 기준과 작성 방향을 안내하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"당신은 RAG 시스템 평가 전문가입니다. 여러 테스트 질의 결과를 바탕으로\n",
    "    Self-RAG과 전통적인 RAG을 비교 분석하세요.\n",
    "\n",
    "    다음 항목에 중점을 두어 분석을 작성하세요:\n",
    "    1. Self-RAG이 더 잘 작동하는 경우와 그 이유\n",
    "    2. 전통 RAG이 더 잘 작동하는 경우와 그 이유\n",
    "    3. Self-RAG의 동적 검색 판단이 미치는 영향\n",
    "    4. Self-RAG 내 관련성 및 근거 평가의 가치\n",
    "    5. 질의 유형에 따른 접근 방식 선택에 대한 권장사항\"\"\"\n",
    "\n",
    "    # 각 질의별 비교 결과 요약 텍스트 생성\n",
    "    comparisons_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        comparisons_summary += f\"질의 {i+1}: {result['query']}\\n\"\n",
    "        comparisons_summary += f\"Self-RAG 메트릭: 검색 필요 여부: {result['self_rag_metrics']['retrieval_needed']}, \"\n",
    "        comparisons_summary += f\"관련 문서 수: {result['self_rag_metrics']['relevant_documents']}/{result['self_rag_metrics']['documents_retrieved']}\\n\"\n",
    "        comparisons_summary += f\"비교 요약: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # 사용자 프롬프트 구성: 비교 요약 전체 포함\n",
    "    user_prompt = f\"\"\"다음은 총 {len(results)}개의 테스트 질의에 대한 Self-RAG vs 전통 RAG 비교 요약입니다:\n",
    "\n",
    "    {comparisons_summary}\n",
    "\n",
    "    이 요약을 기반으로 두 접근 방식에 대한 종합 분석을 작성해주세요.\"\"\"\n",
    "\n",
    "    # LLM 호출을 통해 분석 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # 생성된 분석 결과 텍스트 반환\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG 방식 비교 평가 시작\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트 청크 분할 중...\n",
      "21개의 텍스트 청크 생성 완료\n",
      "청크 임베딩 생성 중...\n",
      "21개의 청크가 벡터 저장소에 추가됨\n",
      "\n",
      "질의 1 처리 중: AI 개발에서 주요한 윤리적 문제는 무엇인가요?\n",
      "\n",
      "Self-RAG 시작: AI 개발에서 주요한 윤리적 문제는 무엇인가요?\n",
      "\n",
      "1단계: 검색 필요 여부 판단 중...\n",
      "검색 필요 여부: True\n",
      "\n",
      "2단계: 관련 문서 검색 중...\n",
      "3개의 문서 검색됨\n",
      "\n",
      "3단계: 문서 관련성 평가 중...\n",
      "문서 1 관련성: relevant\n",
      "문서 2 관련성: relevant\n",
      "문서 3 관련성: relevant\n",
      "관련 문서 수: 3\n",
      "\n",
      "4단계: 관련 문서 기반 응답 생성 및 평가 중...\n",
      "\n",
      "문맥 1/3 처리 중...\n",
      "응답 생성 중...\n",
      "응답의 문맥 기반 근거 평가 중...\n",
      "근거 평가: fully supported\n",
      "응답 유용성 평가 중...\n",
      "유용성 점수: 5/5\n",
      "전체 점수: 20\n",
      "새로운 최적 응답 업데이트됨\n",
      "\n",
      "문맥 2/3 처리 중...\n",
      "응답 생성 중...\n",
      "응답의 문맥 기반 근거 평가 중...\n",
      "근거 평가: fully supported\n",
      "응답 유용성 평가 중...\n",
      "유용성 점수: 5/5\n",
      "전체 점수: 20\n",
      "\n",
      "문맥 3/3 처리 중...\n",
      "응답 생성 중...\n",
      "응답의 문맥 기반 근거 평가 중...\n",
      "근거 평가: fully supported\n",
      "응답 유용성 평가 중...\n",
      "유용성 점수: 5/5\n",
      "전체 점수: 20\n",
      "\n",
      "Self-RAG 완료\n",
      "\n",
      "전통적인 RAG 실행 중: AI 개발에서 주요한 윤리적 문제는 무엇인가요?\n",
      "\n",
      "문서 검색 중...\n",
      "3개의 문서 검색됨\n",
      "응답 생성 중...\n",
      "\n",
      "***전체 비교 분석 결과***\n",
      "\n",
      "### Self-RAG vs 전통 RAG: 종합 분석\n",
      "\n",
      "#### 1. Self-RAG이 더 잘 작동하는 경우와 그 이유\n",
      "Self-RAG은 특정 질의에 대해 더 높은 관련성과 정확성을 제공하는 경우가 많습니다. 예를 들어, \"AI 개발에서 주요한 윤리적 문제는 무엇인가요?\"라는 질의에 대해 Self-RAG은 직접적인 답변을 제공하며, 각 문제에 대한 설명을 포함하여 사용자가 이해하기 쉽게 정보를 전달합니다. 이는 Self-RAG이 동적 검색을 통해 최신 정보와 관련된 문서를 실시간으로 검색하고, 이를 바탕으로 응답을 생성하기 때문입니다. 이러한 접근 방식은 사용자의 요구에 맞춘 맞춤형 정보를 제공할 수 있어, 질의에 대한 정확한 답변을 도출하는 데 유리합니다.\n",
      "\n",
      "#### 2. 전통 RAG이 더 잘 작동하는 경우와 그 이유\n",
      "전통 RAG은 특정한 정보가 명확하게 정의되어 있거나, 고정된 데이터베이스에서 정보를 검색해야 할 때 더 효과적일 수 있습니다. 예를 들어, 역사적 사실이나 정량적 데이터와 같은 명확한 답변이 필요한 경우, 전통 RAG은 신뢰할 수 있는 출처에서 정보를 추출하여 일관된 결과를 제공할 수 있습니다. 또한, 전통 RAG은 데이터의 일관성과 정확성을 보장하는 데 강점을 가지며, 특정 도메인에 대한 깊이 있는 지식을 요구하는 경우 유리합니다.\n",
      "\n",
      "#### 3. Self-RAG의 동적 검색 판단이 미치는 영향\n",
      "Self-RAG의 동적 검색 기능은 최신 정보와 관련된 문서를 실시간으로 검색할 수 있는 능력을 제공합니다. 이는 사용자가 요청하는 정보가 시시각각 변화하는 경우, 예를 들어 AI 윤리와 같은 빠르게 발전하는 분야에서 특히 중요합니다. 동적 검색은 사용자가 필요로 하는 최신 트렌드와 이슈를 반영할 수 있어, 보다 적절하고 시의적절한 정보를 제공하는 데 기여합니다.\n",
      "\n",
      "#### 4. Self-RAG 내 관련성 및 근거 평가의 가치\n",
      "Self-RAG은 관련성 및 근거 평가를 통해 제공하는 정보의 신뢰성을 높입니다. 사용자가 요청한 정보에 대해 관련된 문서를 검색하고, 그 문서의 내용을 바탕으로 응답을 생성함으로써, 정보의 출처와 근거를 명확히 할 수 있습니다. 이는 사용자가 제공된 정보의 신뢰성을 판단하는 데 도움을 주며, 정보의 질을 높이는 데 기여합니다.\n",
      "\n",
      "#### 5. 질의 유형에 따른 접근 방식 선택에 대한 권장사항\n",
      "질의 유형에 따라 Self-RAG과 전통 RAG의 접근 방식을 선택하는 것이 중요합니다. \n",
      "- **Self-RAG 추천**: 동적이고 변화하는 정보가 필요한 질의, 예를 들어 최신 기술 동향이나 사회적 이슈에 대한 질문에 적합합니다. \n",
      "- **전통 RAG 추천**: 고정된 정보나 역사적 사실, 정량적 데이터가 필요한 경우에 적합합니다. 이러한 경우, 전통 RAG의 일관성과 신뢰성을 활용하는 것이 바람직합니다.\n",
      "\n",
      "결론적으로, Self-RAG과 전통 RAG은 각각의 장단점이 있으며, 질의의 특성과 요구에 따라 적절한 접근 방식을 선택하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# AI 정보 문서 경로\n",
    "pdf_path = \"dataset/AI_Understanding.pdf\"\n",
    "\n",
    "# Self-RAG의 적응형 검색 전략을 테스트하기 위한 다양한 유형의 테스트 질의 정의\n",
    "test_queries = [\n",
    "    \"AI 개발에서 주요한 윤리적 문제는 무엇인가요?\",  # 문서 기반 정보 질의\n",
    "    # \"설명 가능한 AI는 어떻게 신뢰를 향상시키나요?\",  # 문서 기반 정보 질의\n",
    "    # \"인공지능에 관한 짧은 시를 써주세요\",             # 창의적 질의 (검색 불필요)\n",
    "    # \"초지능 AI는 인간의 소외를 초래할까요?\"           # 가설적 질의 (부분 검색 필요)\n",
    "]\n",
    "\n",
    "# 보다 객관적인 평가를 위한 참조 정답\n",
    "reference_answers = [\n",
    "    \"AI 개발에서 주요한 윤리적 문제는 편향과 공정성, 프라이버시, 투명성, 책임성, 안전성, 악용 가능성 등입니다.\",\n",
    "    # \"설명 가능한 AI는 의사결정 과정을 사용자에게 이해 가능하게 제공하여 공정성 검증, 편향 탐지, 시스템 신뢰 형성에 기여합니다.\",\n",
    "    # \"양질의 인공지능 시는 AI의 가능성과 한계, 인간과의 관계, 미래 사회, 인식과 지능에 대한 철학적 탐구 등을 창의적으로 표현해야 합니다.\",\n",
    "    # \"초지능 AI의 인간 소외에 대한 관점은 다양합니다. 일부는 경제적 대체나 통제 상실을 우려하며, 다른 일부는 보완적 역할과 인간 중심 설계로 여전히 인간이 중요하다고 봅니다. 대부분의 전문가는 안전하고 책임 있는 AI 설계가 핵심이라고 강조합니다.\"\n",
    "]\n",
    "\n",
    "# Self-RAG과 전통 RAG 접근법을 비교 평가 실행\n",
    "evaluation_results = evaluate_rag_approaches(\n",
    "    pdf_path=pdf_path,                  # AI 정보를 담고 있는 문서 경로\n",
    "    test_queries=test_queries,          # AI 관련 테스트 질의 리스트\n",
    "    reference_answers=reference_answers # 평가용 기준 정답\n",
    ")\n",
    "\n",
    "# 최종 종합 비교 분석 결과 출력\n",
    "print(\"\\n***전체 비교 분석 결과***\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
