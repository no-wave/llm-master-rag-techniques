{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Corrective RAG (CRAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import re\n",
    "from urllib.parse import quote_plus\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 전체 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "\n",
    "    Returns:\n",
    "        str: 추출된 전체 텍스트\n",
    "    \"\"\"\n",
    "    print(f\"{pdf_path}에서 텍스트 추출 중...\")\n",
    "\n",
    "    # PDF 파일 열기\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "\n",
    "    # 각 페이지를 순회하며 텍스트를 이어 붙임\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]\n",
    "        text += page.get_text()  # 현재 페이지에서 텍스트 추출 및 누적\n",
    "\n",
    "    return text  # 전체 텍스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    효율적인 검색과 처리를 위한 텍스트 청크 분할 함수입니다.\n",
    "\n",
    "    이 함수는 입력 텍스트를 지정된 길이의 청크로 나누되,\n",
    "    각 청크 간에 일정한 문자 수만큼 중첩(overlap)을 유지합니다.\n",
    "    RAG 시스템에서는 문맥을 유지한 정확한 검색을 위해 청크 분할이 매우 중요합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 분할할 원본 텍스트\n",
    "        chunk_size (int): 각 청크의 최대 문자 수\n",
    "        overlap (int): 청크 간 중첩될 문자 수 (문맥 연속성 확보용)\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 청크 목록. 각 청크는 다음 필드를 포함합니다:\n",
    "            - text: 실제 청크 텍스트\n",
    "            - metadata: 위치 정보와 출처 타입이 포함된 딕셔너리\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    # 슬라이딩 윈도우 방식으로 청크 생성\n",
    "    # (chunk_size - overlap) 단위로 이동하며 중첩을 유지\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunk_text = text[i:i + chunk_size]  # 현재 구간의 텍스트 추출\n",
    "\n",
    "        if chunk_text:  # 빈 청크는 제외\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,\n",
    "                \"metadata\": {\n",
    "                    \"start_pos\": i,  # 원문에서의 시작 위치\n",
    "                    \"end_pos\": i + len(chunk_text),  # 종료 위치\n",
    "                    \"source_type\": \"document\"  # 문서 기반 청크임을 명시\n",
    "                }\n",
    "            })\n",
    "\n",
    "    print(f\"총 {len(chunks)}개의 텍스트 청크가 생성되었습니다.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 활용한 간단한 벡터 저장소 구현 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 임베딩 벡터, 텍스트, 메타데이터를 저장할 리스트 초기화\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 하나의 항목을 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            text (str): 원본 텍스트\n",
    "            embedding (List[float]): 임베딩 벡터\n",
    "            metadata (Dict, optional): 부가 메타데이터 (선택)\n",
    "        \"\"\"\n",
    "        # 각각의 리스트에 항목 추가\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        여러 개의 항목을 한 번에 저장소에 추가합니다.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): 텍스트 및 메타데이터를 포함한 항목 리스트\n",
    "            embeddings (List[List[float]]): 임베딩 벡터 리스트\n",
    "        \"\"\"\n",
    "        # 각 항목과 임베딩을 쌍으로 반복하여 추가\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                text=item[\"text\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        주어진 쿼리 임베딩과 가장 유사한 항목들을 검색합니다.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): 쿼리 임베딩 벡터\n",
    "            k (int): 반환할 상위 유사 항목 수\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: 유사도가 높은 상위 k개 항목\n",
    "        \"\"\"\n",
    "        # 저장소가 비어 있을 경우 빈 리스트 반환\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 쿼리 벡터를 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 계산하여 저장\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도 기준으로 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 항목 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    OpenAI 임베딩 모델을 사용하여 텍스트의 벡터 임베딩을 생성합니다.\n",
    "\n",
    "    임베딩은 텍스트의 의미적 유사성을 수치화한 벡터 표현으로,\n",
    "    RAG 시스템에서는 질문과 문서 청크 간의 유사도를 비교하는 데 핵심적으로 활용됩니다.\n",
    "\n",
    "    Args:\n",
    "        texts (str 또는 List[str]): 임베딩할 텍스트. 문자열 하나 또는 문자열 리스트 가능\n",
    "        model (str): 사용할 임베딩 모델 이름. 기본값은 \"text-embedding-3-small\"\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]] 또는 List[float]:\n",
    "            - 입력이 리스트일 경우: 임베딩 벡터 리스트 반환\n",
    "            - 입력이 단일 문자열일 경우: 하나의 임베딩 벡터 반환\n",
    "    \"\"\"\n",
    "    # 단일 문자열 입력도 리스트로 처리\n",
    "    input_texts = texts if isinstance(texts, list) else [texts]\n",
    "\n",
    "    # 배치 단위로 처리하여 API 제한(속도/페이로드) 회피\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "\n",
    "    # 배치 단위로 임베딩 생성\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        batch = input_texts[i:i + batch_size]\n",
    "\n",
    "        # 현재 배치에 대해 OpenAI 임베딩 API 호출\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "\n",
    "        # 응답에서 임베딩 벡터 추출\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "\n",
    "    # 입력이 문자열 하나였다면 첫 번째 벡터만 반환\n",
    "    if isinstance(texts, str):\n",
    "        return all_embeddings[0]\n",
    "\n",
    "    # 리스트 입력이라면 전체 임베딩 리스트 반환\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    PDF 문서를 벡터 저장소(Vector Store)로 처리합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로\n",
    "        chunk_size (int): 각 청크의 최대 길이 (문자 수 기준)\n",
    "        chunk_overlap (int): 청크 간 중첩되는 문자 수\n",
    "\n",
    "    Returns:\n",
    "        SimpleVectorStore: 문서 청크와 임베딩이 포함된 벡터 저장소 객체\n",
    "    \"\"\"\n",
    "    # 1단계: PDF 파일에서 텍스트 추출\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 2단계: 텍스트를 중첩된 청크로 분할\n",
    "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
    "\n",
    "    # 3단계: 각 청크에 대한 임베딩 생성\n",
    "    print(\"청크 임베딩 생성 중...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "\n",
    "    # 4단계: 벡터 저장소 초기화\n",
    "    vector_store = SimpleVectorStore()\n",
    "\n",
    "    # 5단계: 청크와 임베딩을 저장소에 추가\n",
    "    vector_store.add_items(chunks, chunk_embeddings)\n",
    "\n",
    "    print(f\"{len(chunks)}개의 청크로 구성된 벡터 저장소가 생성되었습니다.\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_document_relevance(query, document):\n",
    "    \"\"\"\n",
    "    문서가 쿼리와 얼마나 관련 있는지를 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        document (str): 평가할 문서 텍스트\n",
    "\n",
    "    Returns:\n",
    "        float: 관련도 점수 (0~1 사이의 값)\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 모델에게 작업 방식 지시\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 문서의 관련성을 평가하는 전문가입니다.\n",
    "    아래의 쿼리와 문서를 기준으로 관련성을 0에서 1 사이의 점수로 평가하세요.\n",
    "    0은 완전히 무관함을, 1은 완전히 관련됨을 의미합니다.\n",
    "    결과는 숫자(float) 하나만 출력하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 구성\n",
    "    user_prompt = f\"질문: {query}\\n\\n문서:\\n{document}\"\n",
    "\n",
    "    try:\n",
    "        # OpenAI API 호출로 관련성 평가 요청\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # 사용할 모델\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0,     # 변동 없는 일관된 응답 유도\n",
    "            max_tokens=5       # 숫자 하나만 받으면 되므로 최소 토큰\n",
    "        )\n",
    "\n",
    "        # 응답에서 점수 텍스트 추출\n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "\n",
    "        # 정규표현식으로 float 값 추출\n",
    "        score_match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "        if score_match:\n",
    "            return float(score_match.group(1))  # 점수를 float으로 반환\n",
    "        return 0.5  # 파싱 실패 시 중간값 반환\n",
    "\n",
    "    except Exception as e:\n",
    "        # 에러 발생 시 기본값 반환\n",
    "        print(f\"문서 관련성 평가 오류: {e}\")\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def duck_duck_go_search(query, num_results=3):\n",
    "    \"\"\"\n",
    "    DuckDuckGo를 사용하여 웹 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 검색할 질의 문자열\n",
    "        num_results (int): 반환할 최대 결과 수\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[Dict]]: \n",
    "            - results_text: 요약된 검색 결과 텍스트\n",
    "            - sources: 각 결과의 출처 메타데이터 (제목 및 URL 포함)\n",
    "    \"\"\"\n",
    "    # 쿼리를 URL에 사용할 수 있도록 인코딩\n",
    "    encoded_query = quote_plus(query)\n",
    "\n",
    "    # DuckDuckGo API 엔드포인트 (비공식 JSON API)\n",
    "    url = f\"https://api.duckduckgo.com/?q={encoded_query}&format=json\"\n",
    "\n",
    "    try:\n",
    "        # API 요청 수행\n",
    "        response = requests.get(url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        })\n",
    "        data = response.json()\n",
    "\n",
    "        results_text = \"\"\n",
    "        sources = []\n",
    "\n",
    "        # 요약(abstract)이 있을 경우 추가\n",
    "        if data.get(\"AbstractText\"):\n",
    "            results_text += f\"{data['AbstractText']}\\n\\n\"\n",
    "            sources.append({\n",
    "                \"title\": data.get(\"AbstractSource\", \"Wikipedia\"),\n",
    "                \"url\": data.get(\"AbstractURL\", \"\")\n",
    "            })\n",
    "\n",
    "        # 관련 주제(RelatedTopics)에서 추가 결과 수집\n",
    "        for topic in data.get(\"RelatedTopics\", [])[:num_results]:\n",
    "            if \"Text\" in topic and \"FirstURL\" in topic:\n",
    "                results_text += f\"{topic['Text']}\\n\\n\"\n",
    "                sources.append({\n",
    "                    \"title\": topic.get(\"Text\", \"\").split(\" - \")[0],\n",
    "                    \"url\": topic.get(\"FirstURL\", \"\")\n",
    "                })\n",
    "\n",
    "        return results_text, sources\n",
    "\n",
    "    except Exception as e:\n",
    "        # 메인 검색 실패 시 예외 출력\n",
    "        print(f\"웹 검색 중 오류 발생: {e}\")\n",
    "\n",
    "        # 백업 API 시도 (SerpAPI 사용)\n",
    "        try:\n",
    "            backup_url = f\"https://serpapi.com/search.json?q={encoded_query}&engine=duckduckgo\"\n",
    "            response = requests.get(backup_url)\n",
    "            data = response.json()\n",
    "\n",
    "            results_text = \"\"\n",
    "            sources = []\n",
    "\n",
    "            # 백업 API에서 유기적 검색 결과 수집\n",
    "            for result in data.get(\"organic_results\", [])[:num_results]:\n",
    "                results_text += f\"{result.get('title', '')}: {result.get('snippet', '')}\\n\\n\"\n",
    "                sources.append({\n",
    "                    \"title\": result.get(\"title\", \"\"),\n",
    "                    \"url\": result.get(\"link\", \"\")\n",
    "                })\n",
    "\n",
    "            return results_text, sources\n",
    "\n",
    "        except Exception as backup_error:\n",
    "            # 백업 API도 실패할 경우 기본 메시지 반환\n",
    "            print(f\"백업 검색도 실패했습니다: {backup_error}\")\n",
    "            return \"검색 결과를 가져오지 못했습니다.\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rewrite_search_query(query):\n",
    "    \"\"\"\n",
    "    사용자의 원래 질문을 웹 검색에 더 적합하도록 재작성합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 원본 사용자 질문\n",
    "\n",
    "    Returns:\n",
    "        str: 웹 검색 최적화된 재작성 쿼리\n",
    "    \"\"\"\n",
    "    # LLM에게 검색 친화적 쿼리를 생성하도록 지시하는 시스템 프롬프트\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 효과적인 검색 쿼리를 만드는 전문가입니다.\n",
    "    주어진 질문을 웹 검색 엔진에 적합하도록 재작성하세요.\n",
    "    핵심 키워드와 사실 중심으로 표현하고, 불필요한 단어는 제거하며, 간결하게 만드세요.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # OpenAI API를 호출하여 쿼리를 재작성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # 사용할 모델 지정\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # 시스템 지시\n",
    "                {\"role\": \"user\", \"content\": f\"원래 질문: {query}\\n\\n재작성된 검색 쿼리:\"}\n",
    "            ],\n",
    "            temperature=0.3,  # 적당한 창의성 유지\n",
    "            max_tokens=50  # 너무 길지 않게 제한\n",
    "        )\n",
    "\n",
    "        # 재작성된 쿼리 텍스트 반환\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # 예외 발생 시 원래 쿼리 반환\n",
    "        print(f\"검색 쿼리 재작성 중 오류 발생: {e}\")\n",
    "        return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_web_search(query):\n",
    "    \"\"\"\n",
    "    쿼리를 웹 검색에 적합하게 재작성한 뒤, 웹 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자의 원래 질문\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, List[Dict]]: \n",
    "            - 검색 결과 텍스트 (요약된 결과들)\n",
    "            - 출처 메타데이터 리스트 (각 항목에 title과 url 포함)\n",
    "    \"\"\"\n",
    "    # 1단계: 검색 성능 향상을 위해 쿼리를 재작성\n",
    "    rewritten_query = rewrite_search_query(query)\n",
    "    print(f\"재작성된 검색 쿼리: {rewritten_query}\")\n",
    "\n",
    "    # 2단계: DuckDuckGo를 사용하여 웹 검색 실행\n",
    "    results_text, sources = duck_duck_go_search(rewritten_query)\n",
    "\n",
    "    # 3단계: 결과 텍스트와 출처 정보 반환\n",
    "    return results_text, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Refinement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def refine_knowledge(text):\n",
    "    \"\"\"\n",
    "    텍스트에서 핵심 정보를 추출하고 정제된 요약 형태로 반환합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 요약 및 정제할 원본 텍스트\n",
    "\n",
    "    Returns:\n",
    "        str: 정제된 핵심 정보 (• 로 시작하는 글머리표 리스트)\n",
    "    \"\"\"\n",
    "    # LLM에게 명확한 글머리표 형식으로 핵심 정보를 추출하도록 지시\n",
    "    system_prompt = \"\"\"\n",
    "    아래 텍스트에서 가장 핵심적인 정보를 간결하고 명확한 글머리표 형태로 정리하세요.\n",
    "    가장 중요한 사실과 관련된 내용을 중심으로 요약하되,\n",
    "    각 항목은 새 줄에서 \"• \"로 시작하는 글머리표 형식으로 작성하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # OpenAI API 호출하여 텍스트 정제 요청\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"정제할 텍스트:\\n\\n{text}\"}\n",
    "            ],\n",
    "            temperature=0.3  # 핵심 유지에 적절한 온도 설정\n",
    "        )\n",
    "\n",
    "        # 정제된 요약 내용 반환\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # 오류 발생 시 원본 텍스트 반환\n",
    "        print(f\"지식 정제 중 오류 발생: {e}\")\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core CRAG Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crag_process(query, vector_store, k=3):\n",
    "    \"\"\"\n",
    "    CRAG(Corrective RAG) 프로세스를 실행합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        vector_store (SimpleVectorStore): 문서 청크를 포함한 벡터 저장소\n",
    "        k (int): 초기 검색할 문서 수\n",
    "\n",
    "    Returns:\n",
    "        Dict: 최종 응답과 디버깅 정보를 포함한 처리 결과\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== CRAG 방식으로 쿼리 처리 중: {query} ===\\n\")\n",
    "\n",
    "    # 1단계: 쿼리 임베딩 생성 및 유사 문서 검색\n",
    "    print(\"초기 문서 검색 중...\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "    retrieved_docs = vector_store.similarity_search(query_embedding, k=k)\n",
    "\n",
    "    # 2단계: 각 문서에 대해 관련도 평가\n",
    "    print(\"문서 관련도 평가 중...\")\n",
    "    relevance_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        score = evaluate_document_relevance(query, doc[\"text\"])\n",
    "        relevance_scores.append(score)\n",
    "        doc[\"relevance\"] = score\n",
    "        print(f\"문서 관련도 점수: {score:.2f}\")\n",
    "\n",
    "    # 3단계: 최고 관련도 점수를 기준으로 처리 전략 결정\n",
    "    max_score = max(relevance_scores) if relevance_scores else 0\n",
    "    best_doc_idx = relevance_scores.index(max_score) if relevance_scores else -1\n",
    "\n",
    "    sources = []          # 출처 정보 리스트\n",
    "    final_knowledge = \"\"  # 응답 생성을 위한 최종 정보 컨텐츠\n",
    "\n",
    "    # 4단계: 관련도에 따라 지식 수집 전략 분기 처리\n",
    "    if max_score > 0.7:\n",
    "        # Case 1: 높은 관련도 - 문서만 사용\n",
    "        print(f\"높은 관련도 ({max_score:.2f}) - 문서 직접 활용\")\n",
    "        best_doc = retrieved_docs[best_doc_idx][\"text\"]\n",
    "        final_knowledge = best_doc\n",
    "        sources.append({\n",
    "            \"title\": \"내부 문서\",\n",
    "            \"url\": \"\"\n",
    "        })\n",
    "\n",
    "    elif max_score < 0.3:\n",
    "        # Case 2: 낮은 관련도 - 웹 검색 수행\n",
    "        print(f\"낮은 관련도 ({max_score:.2f}) - 웹 검색 수행\")\n",
    "        web_results, web_sources = perform_web_search(query)\n",
    "        final_knowledge = refine_knowledge(web_results)\n",
    "        sources.extend(web_sources)\n",
    "\n",
    "    else:\n",
    "        # Case 3: 중간 관련도 - 내부 문서 + 웹 검색 결합\n",
    "        print(f\"중간 관련도 ({max_score:.2f}) - 문서 + 웹 검색 병합\")\n",
    "        best_doc = retrieved_docs[best_doc_idx][\"text\"]\n",
    "        refined_doc = refine_knowledge(best_doc)\n",
    "\n",
    "        web_results, web_sources = perform_web_search(query)\n",
    "        refined_web = refine_knowledge(web_results)\n",
    "\n",
    "        final_knowledge = f\"내부 문서 발췌:\\n{refined_doc}\\n\\n웹 검색 요약:\\n{refined_web}\"\n",
    "        sources.append({\n",
    "            \"title\": \"내부 문서\",\n",
    "            \"url\": \"\"\n",
    "        })\n",
    "        sources.extend(web_sources)\n",
    "\n",
    "    # 5단계: 최종 응답 생성\n",
    "    print(\"최종 응답 생성 중...\")\n",
    "    response = generate_response(query, final_knowledge, sources)\n",
    "\n",
    "    # 최종 결과 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"relevance_scores\": relevance_scores,\n",
    "        \"max_relevance\": max_score,\n",
    "        \"final_knowledge\": final_knowledge,\n",
    "        \"sources\": sources\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(query, knowledge, sources):\n",
    "    \"\"\"\n",
    "    주어진 질문과 지식을 바탕으로 AI 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        knowledge (str): 응답의 근거가 될 지식 내용\n",
    "        sources (List[Dict]): 출처 정보 리스트 (각 항목은 title 및 url 포함)\n",
    "\n",
    "    Returns:\n",
    "        str: 생성된 AI 응답\n",
    "    \"\"\"\n",
    "    # 출처 정보를 프롬프트에 맞게 포맷팅\n",
    "    sources_text = \"\"\n",
    "    for source in sources:\n",
    "        title = source.get(\"title\", \"출처 없음\")\n",
    "        url = source.get(\"url\", \"\")\n",
    "        if url:\n",
    "            sources_text += f\"- {title}: {url}\\n\"\n",
    "        else:\n",
    "            sources_text += f\"- {title}\\n\"\n",
    "\n",
    "    # 시스템 프롬프트: 모델에게 응답 작성 기준 지시\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 유용하고 신뢰할 수 있는 AI 어시스턴트입니다.\n",
    "    아래 지식을 기반으로 명확하고 포괄적인 답변을 작성하세요.\n",
    "    질문에 완전히 답할 수 없다면 그 한계를 명확히 언급하세요.\n",
    "    응답 마지막에 출처 목록을 포함하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 프롬프트: 질문 + 지식 + 출처를 포함\n",
    "    user_prompt = f\"\"\"\n",
    "    질문: {query}\n",
    "\n",
    "    지식:\n",
    "    {knowledge}\n",
    "\n",
    "    출처:\n",
    "    {sources_text}\n",
    "\n",
    "    위 정보를 바탕으로 질문에 대해 유익한 응답을 작성해 주세요.\n",
    "    응답 마지막에 위 출처들을 함께 포함해 주세요.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # OpenAI API 호출을 통해 응답 생성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # 고품질 응답 생성을 위한 GPT-4 사용\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2  # 일관된 톤을 위한 낮은 창의성\n",
    "        )\n",
    "\n",
    "        # 생성된 응답 반환\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # 오류 발생 시 예외 메시지와 함께 사과 응답 반환\n",
    "        print(f\"응답 생성 중 오류 발생: {e}\")\n",
    "        return f\"죄송합니다. 질문 '{query}'에 대한 응답 생성 중 오류가 발생했습니다. 오류 내용: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_crag_response(query, response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    CRAG 응답의 품질을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        response (str): 생성된 AI 응답\n",
    "        reference_answer (str, optional): 기준이 되는 정답 (비교용, 선택)\n",
    "\n",
    "    Returns:\n",
    "        Dict: 평가 기준별 점수와 설명, 종합 점수를 포함한 평가 결과\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 평가 기준 안내\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 AI 응답 품질 평가 전문가입니다.\n",
    "    아래의 기준에 따라 주어진 응답을 평가하세요:\n",
    "\n",
    "    1. 관련성 (0~10): 응답이 질문에 얼마나 직접적으로 답하고 있는가?\n",
    "    2. 정확성 (0~10): 정보가 사실에 얼마나 부합하는가?\n",
    "    3. 완전성 (0~10): 질문의 모든 측면을 얼마나 충실히 다루는가?\n",
    "    4. 명확성 (0~10): 응답이 얼마나 명확하고 이해하기 쉬운가?\n",
    "    5. 출처 품질 (0~10): 응답이 얼마나 적절하게 출처를 인용하고 있는가?\n",
    "\n",
    "    결과는 다음 항목을 포함하는 JSON 형식으로 반환하세요:\n",
    "    - 각 기준별 점수 및 간단한 평가 설명\n",
    "    - 종합 점수(overall_score, 0~10)\n",
    "    - 전체 평가 요약(summary)\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 구성\n",
    "    user_prompt = f\"\"\"\n",
    "    질문: {query}\n",
    "\n",
    "    평가 대상 응답:\n",
    "    {response}\n",
    "    \"\"\"\n",
    "\n",
    "    # 기준 정답이 제공된 경우 포함\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "    기준 정답 (비교용):\n",
    "    {reference_answer}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # GPT-4 모델을 사용해 평가 요청\n",
    "        evaluation_response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        # 평가 결과를 JSON으로 파싱\n",
    "        evaluation = json.loads(evaluation_response.choices[0].message.content)\n",
    "        return evaluation\n",
    "\n",
    "    except Exception as e:\n",
    "        # 예외 발생 시 기본값 반환\n",
    "        print(f\"응답 평가 중 오류 발생: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"overall_score\": 0,\n",
    "            \"summary\": \"오류로 인해 평가에 실패했습니다.\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_crag_vs_standard_rag(query, vector_store, reference_answer=None):\n",
    "    \"\"\"\n",
    "    동일한 쿼리에 대해 CRAG 방식과 일반 RAG 방식을 비교합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        vector_store (SimpleVectorStore): 문서 청크가 저장된 벡터 저장소\n",
    "        reference_answer (str, optional): 비교용 기준 정답 (선택)\n",
    "\n",
    "    Returns:\n",
    "        Dict: 두 방식의 응답, 평가 점수, 비교 결과를 포함한 정보\n",
    "    \"\"\"\n",
    "    # 1단계: CRAG 방식 실행\n",
    "    print(\"\\n***CRAG 실행 중***\")\n",
    "    crag_result = crag_process(query, vector_store)\n",
    "    crag_response = crag_result[\"response\"]\n",
    "\n",
    "    # 2단계: 일반 RAG 방식 실행 (단순 유사 문서 검색 후 응답 생성)\n",
    "    print(\"\\n***일반 RAG 실행 중***\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "    retrieved_docs = vector_store.similarity_search(query_embedding, k=3)\n",
    "    combined_text = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    standard_sources = [{\"title\": \"문서\", \"url\": \"\"}]\n",
    "    standard_response = generate_response(query, combined_text, standard_sources)\n",
    "\n",
    "    # 3단계: CRAG 응답 평가\n",
    "    print(\"\\n***CRAG 응답 평가 중***\")\n",
    "    crag_eval = evaluate_crag_response(query, crag_response, reference_answer)\n",
    "\n",
    "    # 4단계: 일반 RAG 응답 평가\n",
    "    print(\"\\n***일반 RAG 응답 평가 중***\")\n",
    "    standard_eval = evaluate_crag_response(query, standard_response, reference_answer)\n",
    "\n",
    "    # 5단계: 두 응답 비교\n",
    "    print(\"\\n***두 방식 응답 비교 중***\")\n",
    "    comparison = compare_responses(query, crag_response, standard_response, reference_answer)\n",
    "\n",
    "    # 결과 딕셔너리 반환\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"crag_response\": crag_response,\n",
    "        \"standard_response\": standard_response,\n",
    "        \"reference_answer\": reference_answer,\n",
    "        \"crag_evaluation\": crag_eval,\n",
    "        \"standard_evaluation\": standard_eval,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_responses(query, crag_response, standard_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    CRAG 응답과 일반 RAG 응답을 비교 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문\n",
    "        crag_response (str): CRAG 방식의 응답\n",
    "        standard_response (str): 일반 RAG 방식의 응답\n",
    "        reference_answer (str, optional): 기준 정답 (비교용, 선택)\n",
    "\n",
    "    Returns:\n",
    "        str: 비교 분석 결과 (자연어 분석)\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 두 방식 비교 기준 안내\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 응답 생성 시스템을 비교 평가하는 전문가입니다.\n",
    "    \n",
    "    두 시스템을 비교하세요:\n",
    "\n",
    "    1. CRAG (Corrective RAG): 문서의 관련도를 평가하고 필요 시 웹 검색을 병행하는 방식\n",
    "    2. Standard RAG: 임베딩 유사도 기반으로 문서를 직접 검색하여 응답을 생성하는 방식\n",
    "\n",
    "    아래 기준을 중심으로 두 응답을 비교 평가하세요:\n",
    "    - 정보의 정확성 및 사실 기반 여부\n",
    "    - 질문과의 관련성\n",
    "    - 답변의 완전성 (질문의 모든 측면을 다루는지)\n",
    "    - 명확성과 구성의 논리성\n",
    "    - 출처 표기의 신뢰성과 구체성\n",
    "\n",
    "    이 쿼리에 대해 어느 방식이 더 나은 응답을 제공했는지, 그 이유와 함께 설명해 주세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 구성\n",
    "    user_prompt = f\"\"\"\n",
    "    질문: {query}\n",
    "\n",
    "    CRAG 응답:\n",
    "    {crag_response}\n",
    "\n",
    "    Standard RAG 응답:\n",
    "    {standard_response}\n",
    "    \"\"\"\n",
    "\n",
    "    # 기준 정답이 있을 경우 포함\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "    기준 정답:\n",
    "    {reference_answer}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # GPT-4 모델을 사용해 비교 분석 요청\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        # 분석 결과 반환\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        # 오류 발생 시 메시지 반환\n",
    "        print(f\"응답 비교 중 오류 발생: {e}\")\n",
    "        return f\"응답 비교 중 오류 발생: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_crag_evaluation(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    여러 개의 테스트 쿼리에 대해 CRAG 평가를 실행합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): 평가할 PDF 문서의 경로\n",
    "        test_queries (List[str]): 평가에 사용할 쿼리 목록\n",
    "        reference_answers (List[str], optional): 기준 정답 목록 (선택)\n",
    "\n",
    "    Returns:\n",
    "        Dict: 전체 평가 결과 (개별 결과 + 종합 분석 포함)\n",
    "    \"\"\"\n",
    "    # 문서를 처리하고 벡터 스토어 생성\n",
    "    vector_store = process_document(pdf_path)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # 쿼리별로 CRAG vs Standard RAG 평가 수행\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n***쿼리 {i+1}/{len(test_queries)} 평가 중***\")\n",
    "        print(f\"질문: {query}\")\n",
    "\n",
    "        # 기준 정답이 있는 경우\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "\n",
    "        # CRAG vs 일반 RAG 비교 실행\n",
    "        result = compare_crag_vs_standard_rag(query, vector_store, reference)\n",
    "        results.append(result)\n",
    "\n",
    "        # 비교 결과 출력\n",
    "        print(\"\\n***비교 결과***\")\n",
    "        print(result[\"comparison\"])\n",
    "\n",
    "    # 전체 평가 결과 요약 분석 생성\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "\n",
    "    return {\n",
    "        \"results\": results,  # 개별 쿼리별 평가 결과\n",
    "        \"overall_analysis\": overall_analysis  # 전체 요약 분석\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    여러 쿼리 평가 결과를 종합하여 CRAG vs Standard RAG의 전체 성능을 분석합니다.\n",
    "\n",
    "    Args:\n",
    "        results (List[Dict]): 각 쿼리별 평가 결과 리스트\n",
    "\n",
    "    Returns:\n",
    "        str: 두 접근 방식의 전반적인 성능 분석 결과 (자연어 설명)\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 분석 방향과 항목 정의\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 정보 검색 및 응답 생성 시스템의 성능을 분석하는 전문가입니다.\n",
    "    다수의 테스트 쿼리 결과를 바탕으로 CRAG(Corrective RAG)와 Standard RAG의 성능을 비교 분석하세요.\n",
    "\n",
    "    다음 항목에 초점을 맞추어 분석하세요:\n",
    "    1. CRAG가 더 잘 작동한 상황과 그 이유\n",
    "    2. Standard RAG가 더 나았던 경우와 그 이유\n",
    "    3. 각 방식의 전반적인 강점과 약점\n",
    "    4. 각 방식을 사용하기 적합한 상황에 대한 실용적인 권장 사항\n",
    "    \"\"\"\n",
    "\n",
    "    # 각 쿼리의 요약 결과 텍스트로 정리\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"쿼리 {i+1}: {result['query']}\\n\"\n",
    "        if 'crag_evaluation' in result and 'overall_score' in result['crag_evaluation']:\n",
    "            crag_score = result['crag_evaluation'].get('overall_score', 'N/A')\n",
    "            evaluations_summary += f\"- CRAG 점수: {crag_score}\\n\"\n",
    "        if 'standard_evaluation' in result and 'overall_score' in result['standard_evaluation']:\n",
    "            std_score = result['standard_evaluation'].get('overall_score', 'N/A')\n",
    "            evaluations_summary += f\"- Standard RAG 점수: {std_score}\\n\"\n",
    "        evaluations_summary += f\"- 비교 요약: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # 사용자 프롬프트: 실제 평가 데이터 기반 분석 요청\n",
    "    user_prompt = f\"\"\"\n",
    "    다음은 총 {len(results)}개의 쿼리에 대해 CRAG vs Standard RAG 방식을 비교한 평가 요약입니다.\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    이 정보를 바탕으로 두 방식의 상대적인 강점과 약점을 중심으로 종합적인 분석을 작성해 주세요.\n",
    "    특히 어떤 조건에서 어느 방식이 더 효과적인지를 명확히 설명해 주세요.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # GPT-4를 이용해 전체 분석 생성\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0  # 객관적인 분석을 위한 낮은 창의성 설정\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"전체 분석 생성 중 오류 발생: {e}\")\n",
    "        return f\"전체 분석 생성 중 오류 발생: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of CRAG with Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/AI_Understanding.pdf에서 텍스트 추출 중...\n",
      "총 21개의 텍스트 청크가 생성되었습니다.\n",
      "청크 임베딩 생성 중...\n",
      "21개의 청크로 구성된 벡터 저장소가 생성되었습니다.\n",
      "\n",
      "\n",
      "***쿼리 1/1 평가 중***\n",
      "질문: 머신러닝은 전통적인 프로그래밍과 어떻게 다른가요?\n",
      "\n",
      "***CRAG 실행 중***\n",
      "\n",
      "=== CRAG 방식으로 쿼리 처리 중: 머신러닝은 전통적인 프로그래밍과 어떻게 다른가요? ===\n",
      "\n",
      "초기 문서 검색 중...\n",
      "문서 관련도 평가 중...\n",
      "문서 관련도 점수: 0.90\n",
      "문서 관련도 점수: 0.70\n",
      "문서 관련도 점수: 0.20\n",
      "높은 관련도 (0.90) - 문서 직접 활용\n",
      "최종 응답 생성 중...\n",
      "\n",
      "***일반 RAG 실행 중***\n",
      "\n",
      "***CRAG 응답 평가 중***\n",
      "\n",
      "***일반 RAG 응답 평가 중***\n",
      "\n",
      "***두 방식 응답 비교 중***\n",
      "\n",
      "***비교 결과***\n",
      "두 시스템의 응답을 비교 평가해보겠습니다.\n",
      "\n",
      "### 1. 정보의 정확성 및 사실 기반 여부\n",
      "- **CRAG 응답**: 머신러닝과 전통적인 프로그래밍의 차이를 잘 설명하고 있으며, 각 개념에 대한 예시도 적절합니다. 그러나 \"내부 문서\"라는 출처는 신뢰성이 떨어질 수 있습니다.\n",
      "- **Standard RAG 응답**: 정보의 정확성이 높고, 머신러닝의 세 가지 유형(지도 학습, 비지도 학습, 강화 학습)을 명확히 구분하여 설명합니다. \"문서\"라는 출처도 다소 모호하지만, CRAG보다 더 일반적인 출처로 보입니다.\n",
      "\n",
      "### 2. 질문과의 관련성\n",
      "- 두 응답 모두 질문에 대한 관련성을 잘 유지하고 있습니다. 머신러닝과 전통적인 프로그래밍의 차이를 명확히 설명하고 있습니다.\n",
      "\n",
      "### 3. 답변의 완전성\n",
      "- **CRAG 응답**: 머신러닝의 다양한 방법론(지도 학습, 비지도 학습, 강화 학습)을 언급하였으나, 각 방법론에 대한 구체적인 예시가 부족합니다.\n",
      "- **Standard RAG 응답**: 머신러닝의 세 가지 유형을 명확히 나누어 설명하고, 각 유형에 대한 예시를 제공하여 답변의 완전성이 높습니다.\n",
      "\n",
      "### 4. 명확성과 구성의 논리성\n",
      "- **CRAG 응답**: 전통적인 프로그래밍과 머신러닝의 차이를 잘 설명하고 있으나, 문장이 다소 길고 복잡하여 읽기 어려울 수 있습니다.\n",
      "- **Standard RAG 응답**: 구조가 명확하고, 각 개념을 단계적으로 설명하여 이해하기 쉽습니다. \n",
      "\n",
      "### 5. 출처 표기의 신뢰성과 구체성\n",
      "- **CRAG 응답**: \"내부 문서\"라는 출처는 신뢰성이 떨어질 수 있습니다.\n",
      "- **Standard RAG 응답**: \"문서\"라는 출처도 구체적이지 않지만, 일반적인 문서라는 점에서 다소 신뢰성을 가질 수 있습니다.\n",
      "\n",
      "### 결론\n",
      "**Standard RAG** 방식이 더 나은 응답을 제공했다고 평가할 수 있습니다. 그 이유는 정보의 정확성과 완전성이 높고, 명확한 구조로 질문에 대한 답변을 잘 구성했기 때문입니다. CRAG 응답은 유용한 정보를 제공하지만, 출처의 신뢰성과 답변의 완전성에서 다소 부족함이 있었습니다.\n",
      "\n",
      "***CRAG vs Standard RAG 전체 성능 분석***\n",
      "### CRAG vs Standard RAG 성능 비교 분석\n",
      "\n",
      "#### 1. CRAG가 더 잘 작동한 상황과 그 이유\n",
      "CRAG는 특정한 정보의 정확성과 사실 기반 여부를 중시하는 상황에서 더 잘 작동하는 경향이 있습니다. 예를 들어, 머신러닝과 전통적인 프로그래밍의 차이를 설명할 때, CRAG는 각 개념에 대한 예시를 통해 사용자가 이해하기 쉽게 정보를 제공했습니다. 그러나 CRAG의 응답은 출처의 신뢰성 문제로 인해 점수가 다소 낮아졌습니다. 이는 CRAG가 정보의 출처를 평가하는 데 있어 더 많은 주의를 기울여야 함을 시사합니다.\n",
      "\n",
      "#### 2. Standard RAG가 더 나았던 경우와 그 이유\n",
      "Standard RAG는 정보의 정확성과 신뢰성을 중시하는 쿼리에서 더 나은 성능을 보였습니다. 예를 들어, 머신러닝의 세 가지 주요 특징을 명확하게 설명하며, 신뢰할 수 있는 출처를 기반으로 정보를 제공했습니다. 이는 사용자가 보다 신뢰할 수 있는 정보를 얻을 수 있도록 도와주며, 특히 학술적이거나 전문적인 질문에 대해 더 효과적입니다.\n",
      "\n",
      "#### 3. 각 방식의 전반적인 강점과 약점\n",
      "- **CRAG의 강점**:\n",
      "  - 사용자 친화적인 설명과 예시 제공\n",
      "  - 특정 주제에 대한 깊이 있는 정보 제공 가능\n",
      "- **CRAG의 약점**:\n",
      "  - 출처의 신뢰성 문제로 인해 정보의 정확성이 떨어질 수 있음\n",
      "  - 정보의 검증 과정이 부족할 수 있음\n",
      "\n",
      "- **Standard RAG의 강점**:\n",
      "  - 높은 정보 정확성과 신뢰성\n",
      "  - 검증된 출처를 기반으로 한 정보 제공\n",
      "- **Standard RAG의 약점**:\n",
      "  - 사용자 친화적인 설명이 부족할 수 있음\n",
      "  - 특정 주제에 대한 깊이 있는 정보 제공이 제한적일 수 있음\n",
      "\n",
      "#### 4. 각 방식을 사용하기 적합한 상황에 대한 실용적인 권장 사항\n",
      "- **CRAG 사용 권장 상황**:\n",
      "  - 비전문가를 대상으로 하는 교육적 자료나 설명이 필요한 경우\n",
      "  - 특정 주제에 대한 예시나 비유를 통해 이해를 돕고자 할 때\n",
      "  - 정보의 깊이보다는 사용자 친화성을 중시할 때\n",
      "\n",
      "- **Standard RAG 사용 권장 상황**:\n",
      "  - 학술적이거나 전문적인 질문에 대한 정확한 정보가 필요한 경우\n",
      "  - 신뢰할 수 있는 출처를 기반으로 한 정보가 중요한 상황\n",
      "  - 정보의 정확성과 사실 기반 여부가 중요한 비즈니스 결정에 영향을 미칠 때\n",
      "\n",
      "결론적으로, CRAG와 Standard RAG는 각각의 강점과 약점이 있으며, 사용자의 필요와 상황에 따라 적절한 방식을 선택하는 것이 중요합니다.\n"
     ]
    }
   ],
   "source": [
    "# AI 정보가 담긴 PDF 문서 경로\n",
    "pdf_path = \"dataset/AI_Understanding.pdf\"\n",
    "\n",
    "# AI 관련 쿼리를 포함한 전체 평가 실행\n",
    "test_queries = [\n",
    "    \"머신러닝은 전통적인 프로그래밍과 어떻게 다른가요?\",\n",
    "]\n",
    "\n",
    "# 평가 품질 향상을 위한 (선택적) 기준 정답\n",
    "reference_answers = [\n",
    "    \"머신러닝은 컴퓨터가 명시적인 규칙 없이 데이터를 통해 패턴을 학습한다는 점에서 전통적인 프로그래밍과 다릅니다. 전통적인 프로그래밍에서는 개발자가 일일이 규칙을 작성하지만, 머신러닝에서는 알고리즘이 데이터에서 규칙을 스스로 학습합니다.\",\n",
    "]\n",
    "\n",
    "# CRAG vs Standard RAG 전체 비교 평가 실행\n",
    "evaluation_results = run_crag_evaluation(pdf_path, test_queries, reference_answers)\n",
    "\n",
    "# 전체 평가 분석 결과 출력\n",
    "print(\"\\n***CRAG vs Standard RAG 전체 성능 분석***\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
