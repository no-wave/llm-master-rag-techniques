{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Query Transformations for Enhanced RAG Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Query Transformation Techniques\n",
    "### 1. Query Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(original_query, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    검색 정확도를 높이기 위해 쿼리를 더 구체적이고 명확하게 재작성합니다.\n",
    "\n",
    "    Args:\n",
    "        original_query (str): 원본 사용자 쿼리.\n",
    "        model (str): 쿼리 재작성에 사용할 언어 모델.\n",
    "\n",
    "    Returns:\n",
    "        str: 재작성된 구체적인 쿼리.\n",
    "    \"\"\"\n",
    "    # AI 어시스턴트의 동작을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = (\n",
    "        \"당신은 검색 쿼리를 개선하는 데 특화된 AI 어시스턴트입니다. \"\n",
    "        \"사용자의 원본 쿼리를 더 구체적이고 상세하게 다시 작성하여, \"\n",
    "        \"정확한 정보 검색이 가능하도록 돕는 것이 목적입니다.\"\n",
    "    )\n",
    "\n",
    "    # 사용자 프롬프트: 개선이 필요한 원본 쿼리를 포함\n",
    "    user_prompt = f\"\"\"\n",
    "    다음 쿼리를 더 구체적이고 상세하게 다시 작성하세요. \n",
    "    관련된 키워드나 개념을 포함하여 보다 정확한 검색이 가능하도록 만드세요.\n",
    "\n",
    "    원본 쿼리: {original_query}\n",
    "\n",
    "    재작성된 쿼리:\n",
    "    \"\"\"\n",
    "\n",
    "    # 지정된 모델을 사용하여 쿼리 재작성 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.0,  # 결과의 일관성을 위한 낮은 온도 설정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 결과에서 텍스트를 정제하여 반환\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Step-back Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_step_back_query(original_query, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    더 넓은 문맥을 검색할 수 있도록 일반화된 '스텝백(step-back)' 쿼리를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        original_query (str): 원래의 사용자 질문.\n",
    "        model (str): 스텝백 쿼리 생성을 위한 언어 모델.\n",
    "\n",
    "    Returns:\n",
    "        str: 일반화된 스텝백 쿼리.\n",
    "    \"\"\"\n",
    "    # AI 어시스턴트의 동작을 안내하는 시스템 프롬프트\n",
    "    system_prompt = (\n",
    "        \"당신은 검색 전략에 특화된 AI 어시스턴트입니다. \"\n",
    "        \"사용자의 구체적인 질문을 더 일반적이고 포괄적인 질문으로 바꿔, \"\n",
    "        \"배경 지식이나 문맥을 넓게 검색할 수 있도록 도와주는 것이 목표입니다.\"\n",
    "    )\n",
    "\n",
    "    # 사용자 프롬프트: 일반화할 원본 쿼리를 포함\n",
    "    user_prompt = f\"\"\"\n",
    "    다음 질문을 더 넓고 일반적인 형태로 바꾸어,\n",
    "    관련된 배경 지식이나 문맥 정보를 검색할 수 있도록 하세요.\n",
    "\n",
    "    원본 쿼리: {original_query}\n",
    "\n",
    "    스텝백 쿼리:\n",
    "    \"\"\"\n",
    "\n",
    "    # 언어 모델을 사용하여 스텝백 쿼리 생성 요청\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.1,  # 약간의 다양성을 위한 온도 설정\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 결과 반환 (양 끝 공백 제거)\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Sub-query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(original_query, num_subqueries=4, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    복잡한 쿼리를 더 단순한 하위 쿼리로 분해합니다.\n",
    "\n",
    "    Args:\n",
    "        original_query (str): 복잡한 원본 질문.\n",
    "        num_subqueries (int): 생성할 하위 질문 수.\n",
    "        model (str): 쿼리 분해에 사용할 언어 모델.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 단순한 하위 질문 리스트.\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트: 복잡한 질문을 분해하는 역할\n",
    "    system_prompt = (\n",
    "        \"당신은 복잡한 질문을 분해하는 데 특화된 AI 어시스턴트입니다. \"\n",
    "        \"주어진 질문을 더 단순한 하위 질문들로 나누고, \"\n",
    "        \"이 하위 질문들이 함께 원래 질문에 대한 답변을 구성할 수 있도록 하세요.\"\n",
    "    )\n",
    "\n",
    "    # 사용자 프롬프트 정의\n",
    "    user_prompt = f\"\"\"\n",
    "    다음 복잡한 질문을 {num_subqueries}개의 더 단순한 하위 질문으로 나누세요.\n",
    "    각 하위 질문은 원래 질문의 서로 다른 측면에 초점을 맞추어야 합니다.\n",
    "\n",
    "    원본 질문: {original_query}\n",
    "\n",
    "    다음 형식으로 {num_subqueries}개의 하위 질문을 생성하세요:\n",
    "    1. [첫 번째 하위 질문]\n",
    "    2. [두 번째 하위 질문]\n",
    "    ...\n",
    "    \"\"\"\n",
    "\n",
    "    # 모델을 호출하여 하위 질문 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0.2,  # 약간의 다양성을 허용\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 결과에서 질문만 추출\n",
    "    content = response.choices[0].message.content.strip()\n",
    "    lines = content.split(\"\\n\")\n",
    "    sub_queries = []\n",
    "\n",
    "    for line in lines:\n",
    "        if line.strip() and any(line.strip().startswith(f\"{i}.\") for i in range(1, 10)):\n",
    "            # 번호 제거 및 공백 정리\n",
    "            query = line.strip()\n",
    "            query = query[query.find(\".\")+1:].strip()\n",
    "            sub_queries.append(query)\n",
    "\n",
    "    return sub_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating Query Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query:\n",
      "AI가 업무 자동화와 고용에 미치는 영향은 무엇인가요?\n",
      "\n",
      "Rewritten Query:\n",
      "AI 기술이 업무 자동화에 미치는 영향과 이로 인해 고용 시장에서 발생하는 변화, 특히 특정 산업 분야(예: 제조업, 서비스업, IT업계)에서의 고용 감소 또는 증가 추세, 직무 변화, 새로운 직업의 창출 가능성에 대한 연구 및 사례를 포함하여 설명해 주세요.\n",
      "\n",
      "Step-back Query:\n",
      "AI 기술의 발전이 산업 및 노동 시장에 미치는 전반적인 영향은 무엇인가요?\n",
      "\n",
      "Sub-queries:\n",
      "   1. AI가 업무 자동화에 어떻게 기여하고 있는가?\n",
      "   2. AI의 도입이 특정 산업의 고용 구조에 어떤 변화를 가져오는가?\n",
      "   3. AI로 인해 사라지는 직업과 새롭게 생겨나는 직업은 무엇인가?\n",
      "   4. AI가 업무 자동화로 인해 직원들의 업무 만족도에 미치는 영향은 무엇인가?\n"
     ]
    }
   ],
   "source": [
    "# 예시 쿼리\n",
    "original_query = \"AI가 업무 자동화와 고용에 미치는 영향은 무엇인가요?\"\n",
    "\n",
    "# 쿼리 변환 적용\n",
    "print(\"Original Query:\")\n",
    "print(original_query)\n",
    "\n",
    "# 1. 쿼리 재작성 (더 구체적으로)\n",
    "rewritten_query = rewrite_query(original_query)\n",
    "print(\"\\nRewritten Query:\")\n",
    "print(rewritten_query)\n",
    "\n",
    "# 2. 스텝백 쿼리 생성 (더 일반화된 문맥 요청)\n",
    "step_back_query = generate_step_back_query(original_query)\n",
    "print(\"\\nStep-back Query:\")\n",
    "print(step_back_query)\n",
    "\n",
    "# 3. 하위 쿼리 분해 (다양한 측면으로 나눔)\n",
    "sub_queries = decompose_query(original_query, num_subqueries=4)\n",
    "print(\"\\nSub-queries:\")\n",
    "for i, query in enumerate(sub_queries, 1):\n",
    "    print(f\"   {i}. {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 활용한 간단한 벡터 저장소 클래스입니다.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화.\n",
    "        \"\"\"\n",
    "        self.vectors = []    # 임베딩 벡터를 저장할 리스트\n",
    "        self.texts = []      # 원본 텍스트를 저장할 리스트\n",
    "        self.metadata = []   # 각 텍스트에 대한 메타데이터를 저장할 리스트\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목을 추가합니다.\n",
    "\n",
    "        Args:\n",
    "            text (str): 원본 텍스트.\n",
    "            embedding (List[float]): 임베딩 벡터.\n",
    "            metadata (dict, optional): 추가 메타데이터 (기본값: None).\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))     # 임베딩 벡터를 NumPy 배열로 변환하여 저장\n",
    "        self.texts.append(text)                      # 원본 텍스트 저장\n",
    "        self.metadata.append(metadata or {})         # 메타데이터 저장 (없으면 빈 딕셔너리)\n",
    "\n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        쿼리 임베딩과 가장 유사한 항목들을 검색합니다.\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "            k (int): 반환할 결과 수 (기본값: 5).\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 가장 유사한 상위 k개 항목 (텍스트, 메타데이터, 유사도 포함).\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 저장된 벡터가 없으면 빈 리스트 반환\n",
    "\n",
    "        # 쿼리 벡터를 NumPy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "\n",
    "        # 코사인 유사도를 계산하여 (인덱스, 유사도) 튜플 저장\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (\n",
    "                np.linalg.norm(query_vector) * np.linalg.norm(vector)\n",
    "            )\n",
    "            similarities.append((i, similarity))\n",
    "\n",
    "        # 유사도를 기준으로 내림차순 정렬\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # 상위 k개의 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    주어진 텍스트에 대해 지정된 모델을 사용하여 임베딩 벡터를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str or List[str]): 임베딩을 생성할 입력 텍스트 또는 텍스트 리스트.\n",
    "        model (str): 사용할 임베딩 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "        List[float] or List[List[float]]: 단일 텍스트의 경우 임베딩 벡터 하나,\n",
    "                                          여러 텍스트의 경우 임베딩 벡터 리스트.\n",
    "    \"\"\"\n",
    "    # 입력이 문자열인 경우 리스트로 변환하여 처리\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # 지정된 모델로 임베딩 생성 요청\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # 입력이 문자열이면 첫 번째 임베딩만 반환\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # 리스트 입력일 경우 전체 임베딩 벡터 리스트 반환\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    PDF 파일에서 텍스트를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일의 경로.\n",
    "\n",
    "    Returns:\n",
    "        str: 추출된 전체 텍스트 문자열.\n",
    "    \"\"\"\n",
    "    # PDF 파일 열기\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 추출된 텍스트를 저장할 문자열 초기화\n",
    "\n",
    "    # 각 페이지를 순회하며 텍스트 추출\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]                 # 페이지 객체 가져오기\n",
    "        text = page.get_text(\"text\")           # 페이지에서 텍스트 추출\n",
    "        all_text += text                       # 누적하여 전체 텍스트 구성\n",
    "\n",
    "    return all_text  # 최종 텍스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 n자 단위로 중첩(overlap)을 포함하여 분할합니다.\n",
    "\n",
    "    Args:\n",
    "        text (str): 분할할 원본 텍스트.\n",
    "        n (int): 각 청크의 문자 수 (기본값: 1000).\n",
    "        overlap (int): 청크 간 중첩되는 문자 수 (기본값: 200).\n",
    "\n",
    "    Returns:\n",
    "        List[str]: 분할된 텍스트 청크 리스트.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크를 저장할 리스트 초기화\n",
    "\n",
    "    # (n - overlap)씩 이동하며 텍스트를 분할\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        chunks.append(text[i:i + n])  # 현재 위치부터 n자까지 잘라 청크로 추가\n",
    "\n",
    "    return chunks  # 생성된 청크 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    RAG(Retrieval-Augmented Generation)을 위한 문서 전처리 작업을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 파일 경로.\n",
    "        chunk_size (int): 각 텍스트 청크의 문자 수.\n",
    "        chunk_overlap (int): 청크 간 중첩되는 문자 수.\n",
    "\n",
    "    Returns:\n",
    "        SimpleVectorStore: 청크와 해당 임베딩이 저장된 벡터 저장소 객체.\n",
    "    \"\"\"\n",
    "    print(\"PDF에서 텍스트 추출 중...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    print(\"텍스트를 청크 단위로 분할 중...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"총 {len(chunks)}개의 텍스트 청크가 생성되었습니다.\")\n",
    "\n",
    "    print(\"청크에 대한 임베딩 생성 중...\")\n",
    "    # 효율성을 위해 모든 청크에 대한 임베딩을 한 번에 생성\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "\n",
    "    # 벡터 저장소 생성\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    # 각 청크와 임베딩을 저장소에 추가\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "\n",
    "    print(f\"벡터 저장소에 {len(chunks)}개의 청크가 추가되었습니다.\")\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformed_search(query, vector_store, transformation_type, top_k=3):\n",
    "    \"\"\"\n",
    "    변환된 쿼리를 사용하여 벡터 저장소에서 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 원본 사용자 쿼리.\n",
    "        vector_store (SimpleVectorStore): 검색 대상 벡터 저장소.\n",
    "        transformation_type (str): 쿼리 변환 방식 ('rewrite', 'step_back', 'decompose').\n",
    "        top_k (int): 반환할 상위 결과 수.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: 검색된 결과 리스트.\n",
    "    \"\"\"\n",
    "    print(f\"쿼리 변환 방식: {transformation_type}\")\n",
    "    print(f\"원본 쿼리: {query}\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if transformation_type == \"rewrite\":\n",
    "        # 쿼리 재작성\n",
    "        transformed_query = rewrite_query(query)\n",
    "        print(f\"재작성된 쿼리: {transformed_query}\")\n",
    "\n",
    "        # 임베딩 생성 및 검색 수행\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "\n",
    "    elif transformation_type == \"step_back\":\n",
    "        # 스텝백 쿼리 생성\n",
    "        transformed_query = generate_step_back_query(query)\n",
    "        print(f\"스텝백 쿼리: {transformed_query}\")\n",
    "\n",
    "        # 임베딩 생성 및 검색 수행\n",
    "        query_embedding = create_embeddings(transformed_query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "\n",
    "    elif transformation_type == \"decompose\":\n",
    "        # 복잡한 쿼리를 하위 쿼리로 분해\n",
    "        sub_queries = decompose_query(query)\n",
    "        print(\"하위 쿼리로 분해:\")\n",
    "        for i, sub_q in enumerate(sub_queries, 1):\n",
    "            print(f\"{i}. {sub_q}\")\n",
    "\n",
    "        # 하위 쿼리 각각에 대한 임베딩 생성 및 검색 수행\n",
    "        sub_query_embeddings = create_embeddings(sub_queries)\n",
    "        all_results = []\n",
    "\n",
    "        for i, embedding in enumerate(sub_query_embeddings):\n",
    "            sub_results = vector_store.similarity_search(embedding, k=2)  # 각 하위 쿼리당 적은 수 반환\n",
    "            all_results.extend(sub_results)\n",
    "\n",
    "        # 중복 제거 (동일한 텍스트가 여러 번 등장할 경우, 가장 높은 유사도 결과만 유지)\n",
    "        seen_texts = {}\n",
    "        for result in all_results:\n",
    "            text = result[\"text\"]\n",
    "            if text not in seen_texts or result[\"similarity\"] > seen_texts[text][\"similarity\"]:\n",
    "                seen_texts[text] = result\n",
    "\n",
    "        # 유사도 기준 내림차순 정렬 후 top_k 개 추출\n",
    "        results = sorted(seen_texts.values(), key=lambda x: x[\"similarity\"], reverse=True)[:top_k]\n",
    "\n",
    "    else:\n",
    "        # 변환 없이 일반 쿼리로 검색\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=top_k)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response with Transformed Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    쿼리와 검색된 문맥을 기반으로 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 질문.\n",
    "        context (str): 검색된 문맥 정보.\n",
    "        model (str): 응답 생성을 위한 언어 모델 이름.\n",
    "\n",
    "    Returns:\n",
    "        str: 생성된 응답 문자열.\n",
    "    \"\"\"\n",
    "    # AI 어시스턴트의 동작을 안내하는 시스템 프롬프트 정의\n",
    "    system_prompt = (\n",
    "        \"당신은 도움이 되는 AI 어시스턴트입니다. 사용자 질문에 대해 \"\n",
    "        \"오직 제공된 문맥(Context)만을 기반으로 답변하세요. \"\n",
    "        \"만약 문맥에서 답을 찾을 수 없다면, 정보가 부족하다고 솔직하게 말하세요.\"\n",
    "    )\n",
    "\n",
    "    # 사용자 프롬프트 구성: 문맥과 질문 포함\n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        위 문맥에만 근거하여 포괄적이고 명확한 답변을 작성해주세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # 모델을 호출하여 응답 생성\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,  # 일관된 출력 생성을 위한 낮은 온도\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 응답 텍스트 반환\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Complete RAG Pipeline with Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_query_transformation(pdf_path, query, transformation_type=None):\n",
    "    \"\"\"\n",
    "    쿼리 변환을 포함한 RAG 파이프라인 전체를 실행합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로.\n",
    "        query (str): 사용자 질문.\n",
    "        transformation_type (str): 쿼리 변환 방식 (None, 'rewrite', 'step_back', 'decompose').\n",
    "\n",
    "    Returns:\n",
    "        Dict: 쿼리, 변환 방식, 검색된 문맥, 생성된 응답을 포함한 결과 딕셔너리.\n",
    "    \"\"\"\n",
    "    # PDF 문서를 처리하여 벡터 저장소 생성\n",
    "    vector_store = process_document(pdf_path)\n",
    "\n",
    "    # 쿼리 변환 적용 및 검색\n",
    "    if transformation_type:\n",
    "        # 변환된 쿼리를 사용한 검색\n",
    "        results = transformed_search(query, vector_store, transformation_type)\n",
    "    else:\n",
    "        # 변환 없이 일반 쿼리로 검색\n",
    "        query_embedding = create_embeddings(query)\n",
    "        results = vector_store.similarity_search(query_embedding, k=3)\n",
    "\n",
    "    # 검색 결과에서 문맥 추출\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"PASSAGE {i+1}:\\n{result['text']}\" for i, result in enumerate(results)\n",
    "    ])\n",
    "\n",
    "    # 문맥을 기반으로 응답 생성\n",
    "    response = generate_response(query, context)\n",
    "\n",
    "    # 결과 딕셔너리 반환\n",
    "    return {\n",
    "        \"original_query\": query,\n",
    "        \"transformation_type\": transformation_type,\n",
    "        \"context\": context,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Transformation Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(results, reference_answer, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    다양한 쿼리 변환 기법을 통해 생성된 응답들을 기준 정답과 비교하여 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        results (Dict): 각 쿼리 변환 기법에 대한 결과 딕셔너리 (original, rewrite, step_back, decompose).\n",
    "        reference_answer (str): 비교 대상이 되는 기준 정답.\n",
    "        model (str): 평가에 사용할 언어 모델 이름.\n",
    "    \"\"\"\n",
    "    # 평가 시스템용 시스템 프롬프트 정의\n",
    "    system_prompt = (\n",
    "        \"당신은 RAG 시스템 평가에 특화된 전문가입니다. \"\n",
    "        \"다양한 쿼리 변환 기법을 통해 생성된 응답을 기준 정답과 비교하여, \"\n",
    "        \"어떤 기법이 가장 정확하고 관련성 있으며 완전한 응답을 생성했는지를 평가하세요.\"\n",
    "    )\n",
    "\n",
    "    # 평가용 텍스트 구성\n",
    "    comparison_text = f\"기준 정답:\\n{reference_answer}\\n\\n\"\n",
    "    for technique, result in results.items():\n",
    "        comparison_text += f\"{technique.capitalize()} 쿼리 응답:\\n{result['response']}\\n\\n\"\n",
    "\n",
    "    # 사용자 프롬프트 구성\n",
    "    user_prompt = f\"\"\"\n",
    "    {comparison_text}\n",
    "\n",
    "    각 쿼리 방식 (original, rewrite, step_back, decompose)에 대해 다음을 수행하세요:\n",
    "    1. 정확성, 완전성, 관련성을 기준으로 1~10 점수 부여\n",
    "    2. 각 기법의 장점과 단점 기술\n",
    "\n",
    "    마지막으로 전체 기법을 가장 잘한 순서대로 순위를 매기고,\n",
    "    어떤 기법이 전반적으로 가장 효과적이었는지 그 이유를 설명하세요.\n",
    "    \"\"\"\n",
    "\n",
    "    # 평가 모델 호출\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # 평가 결과 출력\n",
    "    print(\"\\n***EVALUATION RESULTS***\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_transformations(pdf_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    동일한 쿼리에 대해 다양한 쿼리 변환 기법의 성능을 평가합니다.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): PDF 문서 경로.\n",
    "        query (str): 평가할 원본 쿼리.\n",
    "        reference_answer (str, optional): 기준 정답. 제공되면 비교 평가를 수행합니다.\n",
    "\n",
    "    Returns:\n",
    "        Dict: 각 기법별 RAG 결과를 포함한 딕셔너리.\n",
    "    \"\"\"\n",
    "    # 평가할 쿼리 변환 기법 리스트 (None = 원본 쿼리 사용)\n",
    "    transformation_types = [None, \"rewrite\", \"step_back\", \"decompose\"]\n",
    "    results = {}\n",
    "\n",
    "    # 각 기법에 대해 RAG 파이프라인 실행\n",
    "    for transformation_type in transformation_types:\n",
    "        type_name = transformation_type if transformation_type else \"original\"\n",
    "        print(f\"\\n***{type_name.upper()} 쿼리로 RAG 실행 중***\")\n",
    "\n",
    "        # 변환 기법에 따라 문서 처리 및 응답 생성\n",
    "        result = rag_with_query_transformation(pdf_path, query, transformation_type)\n",
    "        results[type_name] = result\n",
    "\n",
    "        # 생성된 응답 출력\n",
    "        print(f\"응답 ({type_name} 쿼리):\")\n",
    "        print(result[\"response\"])\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "    # 기준 정답이 주어진 경우, 모든 응답을 비교 평가\n",
    "    if reference_answer:\n",
    "        compare_responses(results, reference_answer)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Query Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***ORIGINAL 쿼리로 RAG 실행 중***\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트를 청크 단위로 분할 중...\n",
      "총 42개의 텍스트 청크가 생성되었습니다.\n",
      "청크에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 42개의 청크가 추가되었습니다.\n",
      "응답 (original 쿼리):\n",
      "설명 가능한 AI(Explainable AI, XAI)란 AI 시스템의 결정 과정을 더 투명하고 이해하기 쉽게 만드는 기술을 의미합니다. XAI는 사용자가 AI의 결정이 공정하고 정확한지를 평가할 수 있도록 돕는 데 중점을 두고 있습니다. 이는 AI 시스템이 \"블랙 박스\"처럼 작동하여 결정 과정이 불투명한 문제를 해결하는 데 중요한 역할을 합니다.\n",
      "\n",
      "설명 가능한 AI가 중요한 이유는 다음과 같습니다:\n",
      "\n",
      "1. **신뢰 구축**: AI 시스템의 투명성과 설명 가능성을 높임으로써 사용자들이 AI의 신뢰성과 공정성을 평가할 수 있게 됩니다.\n",
      "2. **책임과 책임성**: AI 시스템의 결정 과정이 명확해지면, 개발자, 배포자, 사용자 간의 역할과 책임을 정의하는 데 도움이 됩니다.\n",
      "3. **윤리적 행동**: AI의 결정이 이해 가능해지면, 잠재적인 해악을 다루고 윤리적인 행동을 보장하는 데 기여할 수 있습니다.\n",
      "\n",
      "결론적으로, 설명 가능한 AI는 AI 시스템의 신뢰성과 책임성을 높이는 데 필수적인 요소입니다.\n",
      "========================================\n",
      "\n",
      "***REWRITE 쿼리로 RAG 실행 중***\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트를 청크 단위로 분할 중...\n",
      "총 42개의 텍스트 청크가 생성되었습니다.\n",
      "청크에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 42개의 청크가 추가되었습니다.\n",
      "쿼리 변환 방식: rewrite\n",
      "원본 쿼리: '설명 가능한 AI(Explainable AI)'란 무엇이며, 왜 중요한가?\n",
      "재작성된 쿼리: '설명 가능한 AI(Explainable AI, XAI)'의 정의와 주요 개념, 그리고 이 기술이 인공지능 시스템의 투명성, 신뢰성 및 윤리적 사용에 미치는 중요성에 대해 설명해 주세요. 또한, 설명 가능한 AI의 실제 적용 사례와 이점, 그리고 현재의 연구 동향에 대해서도 알려주세요.\n",
      "응답 (rewrite 쿼리):\n",
      "설명 가능한 AI(Explainable AI, XAI)란 AI 시스템의 결정 과정을 더 투명하고 이해하기 쉽게 만드는 기술을 의미합니다. XAI는 사용자가 AI의 결정이 어떻게 이루어지는지를 이해할 수 있도록 돕고, 이를 통해 AI의 신뢰성과 책임성을 높이는 데 기여합니다. \n",
      "\n",
      "XAI가 중요한 이유는 다음과 같습니다:\n",
      "\n",
      "1. **신뢰 구축**: AI 시스템의 결정 과정을 이해할 수 있게 함으로써 사용자들이 AI에 대한 신뢰를 가질 수 있도록 합니다.\n",
      "2. **공정성 및 정확성 평가**: 사용자가 AI의 결정이 공정하고 정확한지를 평가할 수 있게 해줍니다.\n",
      "3. **책임성**: AI 시스템의 투명성을 높임으로써 개발자와 사용자 간의 책임을 명확히 할 수 있습니다.\n",
      "\n",
      "결국, 설명 가능한 AI는 AI 시스템의 신뢰성을 높이고, 윤리적 행동을 보장하는 데 필수적인 요소입니다.\n",
      "========================================\n",
      "\n",
      "***STEP_BACK 쿼리로 RAG 실행 중***\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트를 청크 단위로 분할 중...\n",
      "총 42개의 텍스트 청크가 생성되었습니다.\n",
      "청크에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 42개의 청크가 추가되었습니다.\n",
      "쿼리 변환 방식: step_back\n",
      "원본 쿼리: '설명 가능한 AI(Explainable AI)'란 무엇이며, 왜 중요한가?\n",
      "스텝백 쿼리: '설명 가능한 AI(Explainable AI)'의 개념과 그 중요성에 대한 일반적인 이해를 돕기 위해, 다음과 같은 질문으로 바꿀 수 있습니다:\n",
      "\n",
      "'인공지능의 투명성과 이해 가능성에 대한 개념은 무엇이며, 이러한 특성이 왜 현대 사회와 기술 발전에서 중요한가?'\n",
      "응답 (step_back 쿼리):\n",
      "설명 가능한 AI(Explainable AI, XAI)란 AI 시스템의 결정 과정을 더 투명하고 이해하기 쉽게 만드는 기술을 의미합니다. XAI는 사용자가 AI의 결정이 공정하고 정확한지를 평가할 수 있도록 돕는 데 중점을 두고 있습니다. 이는 AI 시스템이 종종 \"블랙 박스\"처럼 작동하여 그 결정 과정이 불투명한 경우가 많기 때문에 중요합니다. \n",
      "\n",
      "설명 가능한 AI는 신뢰와 책임을 구축하는 데 필수적이며, AI 시스템의 신뢰성과 공정성을 평가할 수 있는 통찰력을 제공함으로써 사용자와의 신뢰를 증진시키는 역할을 합니다.\n",
      "========================================\n",
      "\n",
      "***DECOMPOSE 쿼리로 RAG 실행 중***\n",
      "PDF에서 텍스트 추출 중...\n",
      "텍스트를 청크 단위로 분할 중...\n",
      "총 42개의 텍스트 청크가 생성되었습니다.\n",
      "청크에 대한 임베딩 생성 중...\n",
      "벡터 저장소에 42개의 청크가 추가되었습니다.\n",
      "쿼리 변환 방식: decompose\n",
      "원본 쿼리: '설명 가능한 AI(Explainable AI)'란 무엇이며, 왜 중요한가?\n",
      "하위 쿼리로 분해:\n",
      "1. '설명 가능한 AI(Explainable AI)'의 정의는 무엇인가?\n",
      "2. 설명 가능한 AI의 주요 특징이나 원칙은 무엇인가?\n",
      "3. 설명 가능한 AI가 필요한 이유는 무엇인가?\n",
      "4. 설명 가능한 AI가 실제로 어떻게 활용되고 있는 사례는 무엇인가?\n",
      "응답 (decompose 쿼리):\n",
      "설명 가능한 AI(Explainable AI, XAI)란 AI의 결정 과정을 이해할 수 있도록 만들어주는 기술을 의미합니다. 이러한 기술은 사용자가 AI의 결정이 공정하고 정확한지를 평가할 수 있도록 돕습니다. 설명 가능한 AI는 투명성과 설명 가능성을 통해 AI 시스템에 대한 신뢰를 구축하는 데 중요한 역할을 합니다. 사용자가 AI의 의사결정 과정을 이해할 수 있을 때, AI 시스템의 신뢰성과 공정성을 평가할 수 있으며, 이는 AI의 책임성과 윤리적 행동을 보장하는 데 기여합니다.\n",
      "========================================\n",
      "\n",
      "***EVALUATION RESULTS***\n",
      "### 1. 평가 점수 및 장단점\n",
      "\n",
      "#### Original 쿼리 응답\n",
      "- **정확성**: 9\n",
      "- **완전성**: 9\n",
      "- **관련성**: 9\n",
      "- **장점**: \n",
      "  - 원래의 정의와 중요성을 잘 설명하고 있으며, 신뢰, 책임성, 윤리적 행동 등 주요 요소를 포함하고 있습니다.\n",
      "  - 명확하고 구체적인 예시를 통해 이해를 돕습니다.\n",
      "- **단점**: \n",
      "  - \"블랙 박스\" 문제에 대한 언급이 있지만, 그에 대한 해결책이나 구체적인 방법론이 부족합니다.\n",
      "\n",
      "#### Rewrite 쿼리 응답\n",
      "- **정확성**: 9\n",
      "- **완전성**: 8\n",
      "- **관련성**: 9\n",
      "- **장점**: \n",
      "  - 원래의 내용을 잘 재구성하여 명확하게 전달하고 있습니다.\n",
      "  - 신뢰성과 책임성에 대한 강조가 잘 드러납니다.\n",
      "- **단점**: \n",
      "  - 원본보다 약간의 정보가 축약되어 있어 완전성이 다소 떨어질 수 있습니다.\n",
      "\n",
      "#### Step_back 쿼리 응답\n",
      "- **정확성**: 8\n",
      "- **완전성**: 8\n",
      "- **관련성**: 8\n",
      "- **장점**: \n",
      "  - AI의 \"블랙 박스\" 문제를 강조하여 독자의 이해를 돕습니다.\n",
      "  - 신뢰와 책임의 중요성을 잘 설명합니다.\n",
      "- **단점**: \n",
      "  - 구체적인 예시나 세부 사항이 부족하여 다소 일반적인 설명으로 느껴질 수 있습니다.\n",
      "\n",
      "#### Decompose 쿼리 응답\n",
      "- **정확성**: 8\n",
      "- **완전성**: 8\n",
      "- **관련성**: 8\n",
      "- **장점**: \n",
      "  - AI의 결정 과정을 이해하는 데 필요한 요소들을 잘 나누어 설명합니다.\n",
      "  - 신뢰성과 공정성의 중요성을 강조합니다.\n",
      "- **단점**: \n",
      "  - 각 요소에 대한 깊이 있는 설명이 부족하여 다소 피상적으로 느껴질 수 있습니다.\n",
      "\n",
      "### 2. 전체 기법 순위\n",
      "1. **Original 쿼리 응답**\n",
      "2. **Rewrite 쿼리 응답**\n",
      "3. **Step_back 쿼리 응답**\n",
      "4. **Decompose 쿼리 응답**\n",
      "\n",
      "### 3. 전반적으로 가장 효과적인 기법\n",
      "**Original 쿼리 응답**이 가장 효과적이었습니다. 그 이유는 다음과 같습니다:\n",
      "- 원래의 정의와 중요성을 명확하게 전달하며, 신뢰, 책임성, 윤리적 행동 등 핵심 요소를 포괄적으로 설명하고 있습니다.\n",
      "- 독자가 AI의 설명 가능성의 중요성을 이해하는 데 필요한 모든 정보를 제공하고 있어, 정확성, 완전성, 관련성 모두에서 높은 점수를 받았습니다.\n",
      "- 다른 기법들은 원본의 내용을 재구성하거나 축약하는 과정에서 일부 정보가 손실되거나 깊이가 부족해지는 경향이 있었으나, Original 응답은 이러한 문제를 피했습니다.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터를 JSON 파일에서 불러옵니다.\n",
    "with open('dataset/validation.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 첫 번째 쿼리와 기준 정답을 추출합니다.\n",
    "query = data[0]['question']\n",
    "reference_answer = data[0]['ideal_answer']\n",
    "\n",
    "# PDF 파일 경로 설정\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 다양한 쿼리 변환 기법에 대한 RAG 평가 실행\n",
    "evaluation_results = evaluate_transformations(pdf_path, query, reference_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
